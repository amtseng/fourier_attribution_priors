### `raw/`
Links to `/mnt/lab_data2/amtseng/att_priors/data/raw/`
- `ENCODE_TFChIP/`
	- Contains TF-ChIPseq data fetched from ENCODE
	- `encode_tf_chip_experiments.tsv`
		- A metadata file listing various experiments from ENCODE, filtered for those that are TF-ChIPseq experiments, aligned to hg38, and status released (and not retracted, for example)
		- Downloaded with the following command, on 4 Oct 2019:
			```
			wget -O encode_tf_chip_experiments.tsv "https://www.encodeproject.org/report.tsv?type=Experiment&status=released&assay_slims=DNA+binding&assay_title=TF+ChIP-seq&assembly=GRCh38"
			```
	- `encode_control_chip_experiments.tsv`
		- A metadata file listing various control experiments from ENCODE (i.e. for a particular cell-line, a ChIP-seq experiment with no immunoprecipitation), filtered for those that are aligned to hg38 and have a status of released
		- Downloaded with the following command, on 4 Oct 2019:
			```
			wget -O encode_control_chip_experiments.tsv "https://www.encodeproject.org/report.tsv?type=Experiment&status=released&assay_slims=DNA+binding&assay_title=Control+ChIP-seq&assembly=GRCh38"
			```
	- `encode_tf_chip_files.tsv`
		- A metadata file listing various ENCODE files, filtered for those that are aligned to hg38, status released, and of the relevant output types (i.e. unfiltered alignments, called peaks, and optimal IDR-filtered peaks)
		- Downloaded with the following commands:
			```
			FIRST=1; for otype in "unfiltered alignments" "peaks and background as input for IDR" "optimal IDR thresholded peaks"; do wget -O - "https://www.encodeproject.org/report.tsv?type=File&status=released&assembly=GRCh38&output_type=$otype" | awk -v first=$FIRST '(first == 1) || NR > 2' >> encode_tf_chip_files.tsv; FIRST=2; done
			```
		- The API for the download of the experiment and files metadata is described [here](https://app.swaggerhub.com/apis-docs/encodeproject/api/basic_search/)
	- The files for each TF are downloaded by `download_ENCODE_TFChIP_data.py`

- `ENCODE_DNase/`
	- Contains metadata on DNase-seq data that has already been processed by Anna
	- Unlike TF-ChIPseq runs, most DNase-seq runs have been processed by Anna using the [ENCODE ATAC-seq pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline/)
	- The set of processed outputs can be found [here](http://mitra.stanford.edu/kundaje/projects/atlas/dnase_processed/)
		- Specifically, under `aggregate_outputs/`, we download the files specified in `count_bigwig_minus_5p`, `count_bigwig_plus_5p`, and `idr.optimal.narrowPeak`
		- These files are directly downloaded from [here](http://mitra.stanford.edu/kundaje/projects/atlas/dnase_processed/aggregate_outputs/), on 26 Jan 2020
	- The set of experiment IDs in `to_download.tsv` are cell lines with released DNase-seq data (as of 24 Jan 2020), aligned to hg38, prioritized for cell types with as many TF-ChIPseq runs available as possible
		- This is because the TF-ChIPseq runs will be valuable for verifying identified motifs using their footprints. This file was created manually
	- The download was performed by `download_ENCODE_DNase_data.py`
		- Since these files have already been processed by a pipeline, the data is downloaded into `interim/`, not `raw/`
	

### `interim/`
Links to `/mnt/lab_data2/amtseng/att_priors/data/interim/`
- Raw data that has been processed to an intermediate form
- `ENCODE_TFChIP/`
	- `binary/`
		- Processed data that is almost ready to use to train binary models
		- `labels/`
			- Labels generated by the peaks from `raw/`, using `generate_ENCODE_binary_labels.sh`
			- Ambiguous regions are given a value of -1
		- `config/`
			- Config files for training specific TF datasets
	- `profile/`
		- Processed data that is almost ready to use to train profile models
		- There is a subdirectory for each TF; each such directory contains a set of BigWigs and peak files for that TF
			- Labels are generated by the alignments and peaks from `raw/`, using `generate_ENCODE_TFChIP_profile_labels.sh`
			- In addition to bedTools, this script also requires bedGraphToBigWig from UCSC Tools, as well as the [hg38 chromosome sizes with chrEBV included](https://github.com/ENCODE-DCC/encValData/blob/master/GRCh38/GRCh38_EBV.chrom.sizes)
			- Note that we start with the _unfiltered_ alignments from ENCODE and filter them ourselves
				- The filtering process we do is identical, but we keep duplicate reads, because those can be useful for profile prediction

- `ENCODE_DNase/`
	- Data downloaded by `download_ENCODE_DNase_data.py`, which has already been processed by a pipeline
	- There is a subdirectory for each cell line, and this contains BigWigs and peak BEDs

### `processed/`
Links to `/mnt/lab_data2/amtseng/att_priors/data/processed/`
- Processed data ready to train models
- `chrom_splits.json`
	- This defines the chromosome splits for hg38, based on chromosome size, for appropriate training, validation, and test sets

- `ENCODE_TFChIP/`
	- `binary/`
		- Processed data ready for binary models
		- `labels/`
			- Binarized labels for each TF
		- `config/`
			- Training configuration files
	- `profile/`
		- Processed data ready for profile models
		- `labels/`
			- HDF5 containing BigWig tracks for training, and BED files in NarrowPeak format for the peaks
			- Made from BigWigs in `interim/` using `create_ENCODE_TFChIP_profile_hdf5.py/`
		- `config/`
			- Training configuration files like paths to training data in `labels/`, and parameter configurations like number of tasks

- `ENCODE_DNase/`
	- `profile/`
		- Processed data ready for profile models
		- `labels/`
			- HDF5 containing BigWig tracks for training, and BED files in NarrowPeak format for the peaks
			- Made from BigWigs in `interim/` using `create_ENCODE_DNase_profile_hdf5.py/`
		- `config/`
			- Training configuration files like paths to training data in `labels/`, and parameter configurations like number of tasks
