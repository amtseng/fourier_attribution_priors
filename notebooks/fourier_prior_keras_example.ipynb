{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A standalone example of the Fourier-based prior\n",
    "In this notebook, we will be running a simple example of the Fourier-based prior, in order to show how it can be applied. We will train a very simple model with and without the Fourier-based prior, and at the end we will show the importance scores on a random sample of input sequences using three different methods of computing attributions.\n",
    "\n",
    "The goal of this notebook is to present a very simple, standalone example of models trained with and without the Fourier-based prior. None of the code in this notebook will rely on libraries/repositories other than very standard and ubiquitous ones (e.g. Keras, NumPy, etc.).\n",
    "\n",
    "We'll be training a simple binary model to predict binding of the SPI1 transcription factor. For the sake of simplicity and efficiency, we'll be training with only one output task (i.e. single-task models), with a slightly simpler data processing. Thus, these results won't fully match those presented in the paper (the results in the paper can be reproduced from the other code/notebooks in this repository).\n",
    "\n",
    "For more results, see the corresponding pre-print [here](https://www.biorxiv.org/content/10.1101/2020.06.11.147272v1).\n",
    "\n",
    "Some of these cells can take awhile to run, and the entire notebook can take on the order of an 30 minutes to complete. Several intermediates are saved along the way, so feel free to run the notebook in pieces, commenting out certain cells that have already been run before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/amtseng/miniconda3/envs/att-priors-keras/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee433a71e59441a2aa38f55bd79932cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "|<bar/>| 0/? [00:00<?, ?it/s]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "import keras.optimizers as ko\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "import scipy.stats\n",
    "import scipy.ndimage\n",
    "import sklearn.metrics\n",
    "import pyfaidx\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "We'll need the following files to train our model:\n",
    "1. Locations of SPI1 binding in the human genome (using the hg38 annotation). We will download the called peaks from the [ENCODE project](https://www.encodeproject.org/). Specifically, we'll be fetching the IDR-thresholded optimal peaks (i.e. peaks that the replicates agreed upon the most), using these regions as our positive (binding) set. We will be using the experiment [ENCSR000BGQ](https://www.encodeproject.org/experiments/ENCSR000BGQ/), which measures SPI1 binding in the GM12878 cell line.\n",
    "\n",
    "2. The hg38 human reference genome. We will be downloading this from the UCSC genome portal.\n",
    "\n",
    "3. The hg38 chromosome sizes. We will also be downloading this from the UCSC genome portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a directory to store everything\n",
    "# !mkdir -p prior_example/data\n",
    "# !mkdir -p prior_example/models\n",
    "# !mkdir -p prior_example/aux_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Download the peaks from ENCODE\n",
    "# !wget https://www.encodeproject.org/files/ENCFF071ZMW/@@download/ENCFF071ZMW.bed.gz -O prior_example/data/peaks.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the hg38 reference genome, and unzip it\n",
    "# !wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz -O prior_example/data/hg38.fasta.gz\n",
    "# !gunzip prior_example/data/hg38.fasta.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the hg38 chromosome sizes\n",
    "# !wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.chrom.sizes -O prior_example/data/hg38.chrom.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"prior_example/aux_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_bed_path = \"prior_example/data/peaks.bed.gz\"\n",
    "reference_fasta_path = \"prior_example/data/hg38.fasta\"\n",
    "chrom_sizes_path = \"prior_example/data/hg38.chrom.sizes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training data\n",
    "We're going to create a rather simple data loader for our binary dataset. We will split each chromosome into windows of length 1000 bp (i.e. the input sequence length for our models), strided across the chromosome with a stride of 50 bp. A 1000 bp window will be considered a \"positive\" binding example if the central 200 bp region overlaps a peak summit, and will be considered a \"negative\" otherwise.\n",
    "\n",
    "We'll save these labels as big BED files: one for training and one for testing. Our test set will consist of chr1. Our training set will consist of all other canonical (non-scaffold) chromosomes other than chrY and chrM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the chromosome sizes, ignoring the small scaffolds\n",
    "chrom_sizes = {}\n",
    "with open(chrom_sizes_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        chrom, size = line.strip().split()\n",
    "        if len(chrom) > 5 or chrom in (\"chrY\", \"chrM\"):\n",
    "            continue\n",
    "        chrom_sizes[chrom] = int(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chroms = [\"chr1\"]\n",
    "train_chroms = [chrom for chrom in chrom_sizes.keys() if chrom not in test_chroms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the peaks BED\n",
    "peaks_bed = pd.read_csv(\n",
    "    peaks_bed_path, sep=\"\\t\", header=None,  # Infer compression\n",
    "    names=[\n",
    "        \"chrom\", \"peak_start\", \"peak_end\", \"name\", \"score\",\n",
    "        \"strand\", \"signal\", \"pval\", \"qval\", \"summit_offset\"\n",
    "    ]\n",
    ")\n",
    "peaks_bed[\"summit\"] = peaks_bed[\"peak_start\"] + peaks_bed[\"summit_offset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 1000\n",
    "window_stride = 50\n",
    "center_overlap_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_bed_path = \"prior_example/data/train_labels.bed\"\n",
    "test_labels_bed_path = \"prior_example/data/test_labels.bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chrom_labels(\n",
    "    chrom, chrom_size, summit_locs, window_length, window_stride,\n",
    "    window_center_length, labels_bed_fp\n",
    "):\n",
    "    \"\"\"\n",
    "    For a single chromosome, write its labels a BED file.\n",
    "    Arguments:\n",
    "        `chrom`: a single chromosome (e.g. \"chr1\")\n",
    "        `chrom_size`: size of the chromosome (e.g. 100000)\n",
    "        `summit_locs`: an iterable of locations of peak summits\n",
    "            in this chromosome\n",
    "        `window_length`: length of windows \n",
    "        `labels_bed_fp`: open file pointer of the file to write\n",
    "            the coordinates and labels\n",
    "    \"\"\"\n",
    "    coord_starts = np.arange(0, chrom_size - window_length, window_stride)\n",
    "    coord_ends = coord_starts + window_length\n",
    "    centers = coord_starts + (window_length // 2)\n",
    "    values = np.zeros_like(coord_starts)\n",
    "    for summit_loc in summit_locs:\n",
    "        delta = summit_loc - (window_length // 2)\n",
    "        values[np.abs(centers - summit_loc) < (window_center_length // 2)] = 1\n",
    "    for i in tqdm.notebook.trange(len(coord_starts), desc=(\"Writing \" + chrom)):\n",
    "        labels_bed_fp.write(\"%s\\t%d\\t%d\\t%d\\n\" % (chrom, coord_starts[i], coord_ends[i], values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create the label BEDs for each chromosome set\n",
    "\n",
    "# # Clear the files first, if they already exist\n",
    "# with open(train_labels_bed_path, \"w\") as f:\n",
    "#     pass\n",
    "# with open(test_labels_bed_path, \"w\") as f:\n",
    "#     pass\n",
    "\n",
    "# # Create the label BEDs\n",
    "# with open(train_labels_bed_path, \"a\") as f:\n",
    "#     for chrom in sorted(train_chroms):\n",
    "#         summit_locs = peaks_bed[peaks_bed[\"chrom\"] == chrom][\"summit\"].values\n",
    "#         write_chrom_labels(\n",
    "#             chrom, chrom_sizes[chrom], summit_locs, input_length,\n",
    "#             window_stride, center_overlap_length, f\n",
    "#         )\n",
    "# with open(test_labels_bed_path, \"a\") as f:\n",
    "#     for chrom in sorted(test_chroms):\n",
    "#         summit_locs = peaks_bed[peaks_bed[\"chrom\"] == chrom][\"summit\"].values\n",
    "#         write_chrom_labels(\n",
    "#             chrom, chrom_sizes[chrom], summit_locs, input_length,\n",
    "#             window_stride, center_overlap_length, f\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dna_to_one_hot(seqs):\n",
    "    \"\"\"\n",
    "    Converts a list of DNA (\"ACGT\") sequences to one-hot encodings, where the\n",
    "    position of 1s is ordered alphabetically by \"ACGT\". `seqs` must be a list\n",
    "    of N strings, where every string is the same length L. Returns an N x L x 4\n",
    "    NumPy array of one-hot encodings, in the same order as the input sequences.\n",
    "    All bases will be converted to upper-case prior to performing the encoding.\n",
    "    Any bases that are not \"ACGT\" will be given an encoding of all 0s.\n",
    "    \"\"\"\n",
    "    seq_len = len(seqs[0])\n",
    "    assert np.all(np.array([len(s) for s in seqs]) == seq_len)\n",
    "\n",
    "    # Join all sequences together into one long string, all uppercase\n",
    "    seq_concat = \"\".join(seqs).upper()\n",
    "\n",
    "    one_hot_map = np.identity(5)[:, :-1]\n",
    "\n",
    "    # Convert string into array of ASCII character codes;\n",
    "    base_vals = np.frombuffer(bytearray(seq_concat, \"utf8\"), dtype=np.int8)\n",
    "\n",
    "    # Anything that's not an A, C, G, or T gets assigned a higher code\n",
    "    base_vals[~np.isin(base_vals, np.array([65, 67, 71, 84]))] = 85\n",
    "\n",
    "    # Convert the codes into indices in [0, 4], in ascending order by code\n",
    "    _, base_inds = np.unique(base_vals, return_inverse=True)\n",
    "\n",
    "    # Get the one-hot encoding for those indices, and reshape back to separate\n",
    "    return one_hot_map[base_inds].reshape((len(seqs), seq_len, 4))\n",
    "\n",
    "def one_hot_to_dna(one_hot):\n",
    "    \"\"\"\n",
    "    Converts a one-hot encoding into a list of DNA (\"ACGT\") sequences, where the\n",
    "    position of 1s is ordered alphabetically by \"ACGT\". `one_hot` must be an\n",
    "    N x L x 4 array of one-hot encodings. Returns a lits of N \"ACGT\" strings,\n",
    "    each of length L, in the same order as the input array. The returned\n",
    "    sequences will only consist of letters \"A\", \"C\", \"G\", \"T\", or \"N\" (all\n",
    "    upper-case). Any encodings that are all 0s will be translated to \"N\".\n",
    "    \"\"\"\n",
    "    bases = np.array([\"A\", \"C\", \"G\", \"T\", \"N\"])\n",
    "    # Create N x L array of all 5s\n",
    "    one_hot_inds = np.tile(one_hot.shape[2], one_hot.shape[:2])\n",
    "\n",
    "    # Get indices of where the 1s are\n",
    "    batch_inds, seq_inds, base_inds = np.where(one_hot)\n",
    "\n",
    "    # In each of the locations in the N x L array, fill in the location of the 1\n",
    "    one_hot_inds[batch_inds, seq_inds] = base_inds\n",
    "\n",
    "    # Fetch the corresponding base for each position using indexing\n",
    "    seq_array = bases[one_hot_inds]\n",
    "    return [\"\".join(seq) for seq in seq_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader which returns one-hot encoded sequences\n",
    "# and labels\n",
    "class BinaryDataLoader:\n",
    "    def __init__(\n",
    "        self, labels_npy_path, reference_genome_path, batch_size,\n",
    "        reverse_complement=True, seed=20200930\n",
    "    ):\n",
    "        labels_table = pd.read_csv(\n",
    "            labels_npy_path, header=None, sep=\"\\t\",\n",
    "            names=[\"chrom\", \"start\", \"end\", \"value\"]\n",
    "        )\n",
    "        \n",
    "        self.coords = labels_table[[\"chrom\", \"start\", \"end\"]].values\n",
    "        labels = labels_table[\"value\"].values\n",
    "        self.pos_inds = np.where(labels)[0]\n",
    "        self.neg_inds = np.where(~labels)[0]\n",
    "        print(\"Positive coordinates: %d\" % len(self.pos_inds))\n",
    "        print(\"Negative coordinates: %d\" % len(self.neg_inds))\n",
    "        print(\"Total: %d\" % len(labels))\n",
    "        \n",
    "        self.reference_genome_path = reference_genome_path\n",
    "        \n",
    "        self.shuffle_rng = np.random.RandomState(seed)\n",
    "        self.reverse_complement = reverse_complement\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        self.shuffle_rng.shuffle(self.pos_inds)\n",
    "        self.shuffle_rng.shuffle(self.neg_inds)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.pos_inds) / batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns batch of data: a B x L x 4 array of one-hot encoded\n",
    "        sequences, and a B-array of binary labels.\n",
    "        \"\"\"\n",
    "        batch_slice = slice(index * batch_size, (index + 1) * batch_size)\n",
    "        pos_coords = self.coords[self.pos_inds[batch_slice]]\n",
    "        neg_coords = self.coords[self.neg_inds[batch_slice]]\n",
    "        all_coords = np.concatenate([pos_coords, neg_coords])\n",
    "        \n",
    "        labels = np.ones(len(pos_coords) + len(neg_coords))\n",
    "        labels[len(pos_coords):] = 0\n",
    "        \n",
    "        genome_reader = pyfaidx.Fasta(self.reference_genome_path)\n",
    "        seqs = [\n",
    "            genome_reader[chrom][start:end].seq for\n",
    "            chrom, start, end in all_coords\n",
    "        ]\n",
    "        one_hot = dna_to_one_hot(seqs)\n",
    "        \n",
    "        if not self.reverse_complement:\n",
    "            return one_hot, labels\n",
    "        else:\n",
    "            return np.concatenate([one_hot, np.flip(one_hot, axis=(1, 2))]), \\\n",
    "                np.concatenate([labels, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive coordinates: 158549\n",
      "Negative coordinates: 55641291\n",
      "Total: 55641291\n",
      "Positive coordinates: 16296\n",
      "Negative coordinates: 4979109\n",
      "Total: 4979109\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "reverse_complement = True\n",
    "train_data_loader = BinaryDataLoader(\n",
    "    train_labels_bed_path, reference_fasta_path,\n",
    "    batch_size, reverse_complement\n",
    ")\n",
    "test_data_loader = BinaryDataLoader(\n",
    "    test_labels_bed_path, reference_fasta_path,\n",
    "    batch_size, reverse_complement\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "We'll be using the same binary model architecture defined in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_tensor_1d(input_tensor, smooth_sigma):\n",
    "    \"\"\"\n",
    "    Smooths an input tensor along a dimension using a Gaussian filter.\n",
    "    Arguments:\n",
    "        `input_tensor`: a A x B tensor to smooth along the second dimension\n",
    "        `smooth_sigma`: width of the Gaussian to use for smoothing; this is the\n",
    "            standard deviation of the Gaussian to use, and the Gaussian will be\n",
    "            truncated after 1 sigma (i.e. the smoothing window is\n",
    "            1 + (2 * sigma); sigma of 0 means no smoothing\n",
    "    Returns an array the same shape as the input tensor, with the dimension of\n",
    "    `B` smoothed.\n",
    "    \"\"\"\n",
    "    # Generate the kernel\n",
    "    if smooth_sigma == 0:\n",
    "        sigma, truncate = 1, 0\n",
    "    else:\n",
    "        sigma, truncate = smooth_sigma, 1\n",
    "    base = np.zeros(1 + (2 * sigma))\n",
    "    base[sigma] = 1  # Center of window is 1 everywhere else is 0\n",
    "    kernel = scipy.ndimage.gaussian_filter(base, sigma=sigma, truncate=truncate)\n",
    "    kernel = kb.constant(kernel)\n",
    "\n",
    "    # Expand the input and kernel to 3D, with channels of 1\n",
    "    input_tensor = kb.expand_dims(input_tensor, axis=2)  # Shape: A x B x 1\n",
    "    kernel = kb.expand_dims(kb.expand_dims(kernel, axis=1), axis=2)  # Shape: (1 + 2s) x 1 x 1\n",
    "\n",
    "    smoothed = tf.nn.conv1d(\n",
    "        input_tensor, kernel, stride=1, padding=\"SAME\", data_format=\"NWC\"\n",
    "    )\n",
    "\n",
    "    return kb.squeeze(smoothed, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_logits_to_probs(logit_pred_vals):\n",
    "    \"\"\"\n",
    "    Converts the model's predicted binary logits into probabilities via a\n",
    "    sigmoid on all values.\n",
    "    Arguments:\n",
    "        `logit_pred_vals`: a tensor/array containing the predicted logits\n",
    "    Returns a tensor/array of the same shape, containing the predictions as\n",
    "    raw probabilities by doing a sigmoid. If the input is a tensor, the output\n",
    "    will be a tensor. If the input is a NumPy array, the output will be a NumPy\n",
    "    array.\n",
    "    \"\"\"\n",
    "    if type(logit_pred_vals) is np.ndarray:\n",
    "        return scipy.special.expit(logit_pred_vals)\n",
    "    else:\n",
    "        return kb.sigmoid(logit_pred_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_predictor():\n",
    "    \"\"\"\n",
    "    Creates an binary TF binding site predictor from a DNA sequence.\n",
    "    \n",
    "    The model input is a B x L x 4 tensor, where B is the batch size,\n",
    "    and L is the sequence length. The output is the LOGITS of each input\n",
    "    as a B-tensor. Note that the logits are returned in the order\n",
    "    according to the input sequences.\n",
    "    \"\"\"\n",
    "    input_seq = kl.Input(shape=(input_length, 4), name=\"input_seq\")\n",
    "    \n",
    "    # Define the convolutional layers\n",
    "    depths = [64, 64, 64]\n",
    "    conv_filter_sizes = [15, 15, 13]\n",
    "    last_conv_out = input_seq\n",
    "    for i in range(3):\n",
    "        last_conv_out = kl.Conv1D(\n",
    "            filters=conv_filter_sizes[i], kernel_size=conv_filter_sizes[i],\n",
    "            padding=\"valid\", activation=\"relu\", name=(\"conv_%d\" % (i + 1))\n",
    "        )(last_conv_out)\n",
    "        last_conv_out = kl.BatchNormalization(\n",
    "             momentum=0.1, epsilon=1e-05\n",
    "        )(last_conv_out)\n",
    "\n",
    "    # Define the max pooling layer\n",
    "    pool_out = kl.MaxPooling1D(\n",
    "        pool_size=40, strides=40, padding=\"valid\"\n",
    "    )(last_conv_out)\n",
    "    \n",
    "    # Flatten the pooling output\n",
    "    pool_out_flat = kl.Flatten(name=\"flatten\")(pool_out)\n",
    "\n",
    "    # Define the fully connected layers\n",
    "    dims = [50, 15]\n",
    "    last_fc_out = pool_out_flat\n",
    "    for i in range(2):\n",
    "        last_fc_out = kl.Dense(\n",
    "            units=dims[i], activation=\"relu\", name=(\"dense_%d\" % (i + 1))\n",
    "        )(last_fc_out)\n",
    "        last_fc_out = kl.BatchNormalization(\n",
    "             momentum=0.1, epsilon=1e-05\n",
    "        )(last_fc_out)\n",
    "\n",
    "    # Map last fully connected layer to final outputs\n",
    "    output = kl.Dense(\n",
    "        units=1, name=\"dense_3\"\n",
    "    )(last_fc_out)\n",
    "    \n",
    "    model = km.Model(inputs=input_seq, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def correctness_loss(true_vals, logit_pred_vals):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss.\n",
    "    Arguments:\n",
    "        `true_vals`: a B-tensor of true binary values\n",
    "        `logit_pred_vals`: a B-tensor containing the predicted LOGITS\n",
    "    Returns a tensor scalar that is the loss for the batch.\n",
    "    \"\"\"\n",
    "    return kb.mean(kb.binary_crossentropy(\n",
    "        true_vals, logit_pred_vals, from_logits=True\n",
    "    ))\n",
    "\n",
    "\n",
    "def fourier_att_prior_loss(\n",
    "    status, input_grads, freq_limit, limit_softness,\n",
    "    att_prior_grad_smooth_sigma\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes an attribution prior loss for some given training examples,\n",
    "    using a Fourier transform form.\n",
    "    Arguments:\n",
    "        `status`: a B-tensor, where B is the batch size; each entry is 1 if\n",
    "            that example is to be treated as a positive example, and 0\n",
    "            otherwise\n",
    "        `input_grads`: a B x L x 4 tensor, where B is the batch size, L is\n",
    "            the length of the input; this needs to be the gradients of the\n",
    "            input with respect to the output; this should be\n",
    "            *gradient times input*\n",
    "        `freq_limit`: the maximum integer frequency index, k, to consider for\n",
    "            the loss; this corresponds to a frequency cut-off of pi * k / L;\n",
    "            k should be less than L / 2\n",
    "        `limit_softness`: amount to soften the limit by, using a hill\n",
    "            function; None means no softness\n",
    "        `att_prior_grad_smooth_sigma`: amount to smooth the gradient before\n",
    "            computing the loss\n",
    "    Returns a single scalar Tensor consisting of the attribution loss for\n",
    "    the batch.\n",
    "    \"\"\"\n",
    "    abs_grads = kb.sum(kb.abs(input_grads), axis=2)\n",
    "\n",
    "    # Smooth the gradients\n",
    "    grads_smooth = smooth_tensor_1d(\n",
    "        abs_grads, att_prior_grad_smooth_sigma\n",
    "    )\n",
    "\n",
    "    # Only do the positives\n",
    "    pos_grads = grads_smooth[status == 1]\n",
    "\n",
    "    if pos_grads.numpy().size:\n",
    "        pos_fft = tf.signal.rfft(pos_grads)\n",
    "        pos_mags = tf.abs(pos_fft)\n",
    "        pos_mag_sum = kb.sum(pos_mags, axis=1, keepdims=True)\n",
    "        zero_mask = tf.cast(pos_mag_sum == 0, tf.float32)\n",
    "        pos_mag_sum = pos_mag_sum + zero_mask  # Keep 0s when the sum is 0  \n",
    "        pos_mags = pos_mags / pos_mag_sum\n",
    "\n",
    "        # Cut off DC\n",
    "        pos_mags = pos_mags[:, 1:]\n",
    "\n",
    "        # Construct weight vector\n",
    "        if limit_softness is None:\n",
    "            weights = tf.sequence_mask(\n",
    "                [freq_limit], maxlen=tf.shape(pos_mags)[1], dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            weights = tf.sequence_mask(\n",
    "                [freq_limit], maxlen=tf.shape(pos_mags)[1], dtype=tf.float32\n",
    "            )\n",
    "            x = tf.abs(tf.range(\n",
    "                -freq_limit + 1, tf.shape(pos_mags)[1] - freq_limit + 1, dtype=tf.float32\n",
    "            ))  # Take absolute value of negatives just to avoid NaN; they'll be removed\n",
    "            decay = 1 / (1 + tf.pow(x, limit_softness))\n",
    "            weights = weights + ((1.0 - weights) * decay)\n",
    "\n",
    "        # Multiply frequency magnitudes by weights\n",
    "        pos_weighted_mags = pos_mags * weights\n",
    "\n",
    "        # Add up along frequency axis to get score\n",
    "        pos_score = tf.reduce_sum(pos_weighted_mags, axis=1)\n",
    "        pos_loss = 1 - pos_score\n",
    "        return kb.mean(pos_loss)\n",
    "    else:\n",
    "        return kb.constant(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, save_path):\n",
    "    \"\"\"\n",
    "    Saves the given model at the given path. This saves the state of the model\n",
    "    (i.e. trained layers and parameters).\n",
    "    \"\"\"\n",
    "    model.save(save_path)\n",
    "\n",
    "\n",
    "def restore_model(load_path):\n",
    "    \"\"\"\n",
    "    Restores a model from the given path. It will restore the learned\n",
    "    parameters to the model.\n",
    "    \"\"\"\n",
    "    custom_objects = {\"kb\": kb, \"tf\": tf}\n",
    "    return km.load_model(load_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models\n",
    "We'll train two models, one with the Fourier-based prior, and the other without. Note that the first batch might take some time to load, while the reference Fasta is being indexed.\n",
    "\n",
    "While the model trained without the prior can converge in just 1 - 2 epochs, the model trained with the prior often requires a few more epochs (due to optimizing over multiple objectives). To keep it simple, we'll just train both models for 1 epoch each. This puts the Fourier-based prior at a slight disadvantage in the comparisons, but we will see that the interpretability of the model trained with the prior is still cleaner. We will also verify that the predictive performances between the two models are reasonably close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "freq_limit = 150\n",
    "limit_softness = 0.2\n",
    "att_prior_grad_smooth_sigma = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    data_loader, model, num_epochs, learning_rate, use_prior=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model for the given number of epochs.\n",
    "    \"\"\"\n",
    "    optimizer = ko.Adam(lr=learning_rate)\n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        losses = []\n",
    "        t_iter = tqdm.notebook.trange(\n",
    "            len(data_loader),\n",
    "            desc=(\"Epoch %d/%d: Loss: ---\" % (epoch_i + 1, num_epochs))\n",
    "        )\n",
    "        \n",
    "        data_loader.shuffle_data()\n",
    "        for i in t_iter:\n",
    "            input_seqs, output_vals = data_loader[i]\n",
    "            \n",
    "            input_seqs = tf.convert_to_tensor(input_seqs, dtype=tf.float32)\n",
    "            output_vals = tf.convert_to_tensor(output_vals, dtype=tf.float32)\n",
    "            \n",
    "            with tf.GradientTape() as opt_tape:\n",
    "                with tf.GradientTape() as input_grad_tape:\n",
    "                    input_grad_tape.watch(input_seqs)\n",
    "                    logit_pred_vals = model(input_seqs, training=True)\n",
    "\n",
    "                loss = correctness_loss(\n",
    "                    output_vals, kb.squeeze(logit_pred_vals, axis=1)\n",
    "                )\n",
    "                \n",
    "                # Compute gradients of the output with respect to the input\n",
    "                if use_prior:\n",
    "                    input_grads = input_grad_tape.gradient(\n",
    "                        logit_pred_vals, input_seqs\n",
    "                    )\n",
    "                    input_grads = input_grads * input_seqs  # Gradient * input\n",
    "                    loss = loss + fourier_att_prior_loss(\n",
    "                        output_vals, input_grads, freq_limit, limit_softness,\n",
    "                        att_prior_grad_smooth_sigma\n",
    "                    )\n",
    "            \n",
    "            # Compute gradient and update weights through backprop\n",
    "            opt_grads = opt_tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(opt_grads, model.trainable_weights))\n",
    "            \n",
    "            losses.append(float(loss))\n",
    "            t_iter.set_description(\n",
    "                \"Epoch %d/%d: Loss: %6.4f\" % (epoch_i + 1, num_epochs, float(loss))\n",
    "            )\n",
    "        print(\"Average loss: %6.4f\" % np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(data_loader, model):\n",
    "    \"\"\"\n",
    "    Predicts data from the model, and returns the true values\n",
    "    and the predicted probabilities.\n",
    "    \"\"\"\n",
    "    true_vals, pred_vals = [], []\n",
    "    \n",
    "    t_iter = tqdm.notebook.trange(\n",
    "        len(data_loader), desc=\"Loss: ---\"\n",
    "    )\n",
    "    for i in t_iter:\n",
    "        input_seqs, output_vals = data_loader[i]\n",
    "        true_vals.append(output_vals)\n",
    "\n",
    "        input_seqs = tf.convert_to_tensor(input_seqs, dtype=tf.float32)\n",
    "        output_vals = tf.convert_to_tensor(output_vals, dtype=tf.float32)\n",
    "\n",
    "        logit_pred_vals = model(input_seqs)\n",
    "        loss = correctness_loss(\n",
    "            output_vals, kb.squeeze(logit_pred_vals, axis=1)\n",
    "        )\n",
    "\n",
    "        t_iter.set_description(\"Loss: %6.4f\" % float(loss))\n",
    "        pred_vals.append(\n",
    "            binary_logits_to_probs(logit_pred_vals.numpy())\n",
    "        )\n",
    "    return np.concatenate(true_vals), np.concatenate(pred_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_performance(true_vals, pred_vals, acc_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Shows accuracy, auROC, and auPRC.\n",
    "    \"\"\"\n",
    "    pos_mask = true_vals == 1\n",
    "    neg_mask = true_vals == 0\n",
    "    \n",
    "    pos_right = np.sum(pred_vals[pos_mask] > acc_thresh)\n",
    "    neg_right = np.sum(pred_vals[neg_mask] <= acc_thresh)\n",
    "    pos_acc = pos_right / np.sum(pos_mask)\n",
    "    neg_acc = neg_right / np.sum(neg_mask)\n",
    "    acc = (pos_right + neg_right) / len(true_vals)\n",
    "    \n",
    "    auroc = sklearn.metrics.roc_auc_score(true_vals, pred_vals)\n",
    "    \n",
    "    precis, recall, thresh = \\\n",
    "        sklearn.metrics.precision_recall_curve(true_vals, pred_vals)\n",
    "    auprc = sklearn.metrics.auc(recall, precis)\n",
    "    \n",
    "    print(\"Accuracy: %.2f%%\" % (acc * 100))\n",
    "    print(\"Positive accuracy: %.2f%%\" % (pos_acc * 100))\n",
    "    print(\"Negative accuracy: %.2f%%\" % (neg_acc * 100))\n",
    "    print(\"auROC: %.3f\" % auroc)\n",
    "    print(\"auPRC: %.3f\" % auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model_path = \"prior_example/models/prior_model.h5\"\n",
    "noprior_model_path = \"prior_example/models/noprior_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate new models\n",
    "# prior_model = create_binary_predictor()\n",
    "# noprior_model = create_binary_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(\n",
    "#     train_data_loader, prior_model, 1, learning_rate, use_prior=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(prior_model, prior_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(\n",
    "#     train_data_loader, noprior_model, 1, learning_rate, use_prior=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(noprior_model, noprior_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load in saved models\n",
    "prior_model = restore_model(prior_model_path)\n",
    "noprior_model = restore_model(noprior_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc898ff465a409dac87eb76e22b90b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loss: ---', max=255.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784932cc7c8844da8e499ade0a1ae3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loss: ---', max=255.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prior_true_vals, prior_pred_vals = predict_model(test_data_loader, prior_model)\n",
    "noprior_true_vals, noprior_pred_vals = predict_model(test_data_loader, noprior_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with prior\n",
      "Accuracy: 90.79%\n",
      "Positive accuracy: 97.10%\n",
      "Negative accuracy: 84.49%\n",
      "auROC: 0.975\n",
      "auPRC: 0.972\n",
      "\n",
      "Performance without prior\n",
      "Accuracy: 90.89%\n",
      "Positive accuracy: 98.74%\n",
      "Negative accuracy: 83.06%\n",
      "auROC: 0.980\n",
      "auPRC: 0.973\n"
     ]
    }
   ],
   "source": [
    "# Compare predictive performance\n",
    "print(\"Performance with prior\")\n",
    "show_performance(prior_true_vals, prior_pred_vals)\n",
    "print(\"\")\n",
    "print(\"Performance without prior\")\n",
    "show_performance(noprior_true_vals, noprior_pred_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare interpretability\n",
    "We'll show the importance score tracks for a random sample of the test-set input sequences, to visually confirm the improved signal-to-noise ratio. More sophisticated methods of quantifying the improved interpretability can be found in the other notebooks of this repository (see paper for details).\n",
    "\n",
    "We will show the importance scores using input gradients, DeepSHAP, and _in silico_ mutagenesis.\n",
    "\n",
    "Note that SPI1 binding is a relatively simple task, because of the straightforward motif and binding mode. Although we will see an improvement in the interpretability of models trained with the Fourier-based prior, these improvements are still small compared to the improvements we get when we train on more complex tasks (see the paper for examples of these complex tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use DeepSHAP, we'll need to install the library. If you don't want to install this, then comment out all DeepSHAP-related cells (including the next four)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The code below will clone and install the DeepSHAP repository\n",
    "# !git clone https://github.com/amtseng/shap.git prior_example/aux_code/shap\n",
    "# !pip install prior_example/aux_code/shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-16 19:08:58--  https://raw.githubusercontent.com/amtseng/fourier_attribution_priors/master/src/extract/dinuc_shuffle.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.196.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.196.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8137 (7.9K) [text/plain]\n",
      "Saving to: ‘prior_example/aux_code/dinuc_shuffle.py’\n",
      "\n",
      "prior_example/aux_c 100%[===================>]   7.95K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-11-16 19:08:58 (50.9 MB/s) - ‘prior_example/aux_code/dinuc_shuffle.py’ saved [8137/8137]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the code for performing dinucleotide shuffles\n",
    "!wget https://raw.githubusercontent.com/amtseng/fourier_attribution_priors/master/src/extract/dinuc_shuffle.py -O prior_example/aux_code/dinuc_shuffle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dinuc_shuffle import dinuc_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need some code to visualize the importance score tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-16 19:09:44--  https://raw.githubusercontent.com/amtseng/fourier_attribution_priors/master/src/plot/viz_sequence.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.196.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.196.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7081 (6.9K) [text/plain]\n",
      "Saving to: ‘prior_example/aux_code/viz_sequence.py’\n",
      "\n",
      "prior_example/aux_c 100%[===================>]   6.92K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-11-16 19:09:44 (43.6 MB/s) - ‘prior_example/aux_code/viz_sequence.py’ saved [7081/7081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download code for visualizing importance scores\n",
    "!wget https://raw.githubusercontent.com/amtseng/fourier_attribution_priors/master/src/plot/viz_sequence.py -O prior_example/aux_code/viz_sequence.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import viz_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_input_grad(model, input_seq):\n",
    "    \"\"\"\n",
    "    Computes input gradient x input for an L x 4 one-hot\n",
    "    encoded sequence.\n",
    "    \"\"\"\n",
    "    assert input_seq.shape == (input_length, 4)\n",
    "    input_seq_np = input_seq\n",
    "    input_seqs = tf.convert_to_tensor([input_seq], dtype=tf.float32)\n",
    "    with tf.GradientTape() as input_grad_tape:\n",
    "        input_grad_tape.watch(input_seqs)\n",
    "        logit_pred_vals = model(input_seqs)\n",
    "\n",
    "    input_grads = input_grad_tape.gradient(\n",
    "        logit_pred_vals, input_seqs\n",
    "    )\n",
    "    return input_grads.numpy()[0] * input_seq_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_deepshap(model, input_seq):\n",
    "    \"\"\"\n",
    "    Computes DeepSHAP scores for an L x 4 one-hot\n",
    "    encoded sequence.\n",
    "    \"\"\"\n",
    "    assert input_seq.shape == (input_length, 4)\n",
    "    \n",
    "    def bg_func(input_seq):\n",
    "        if not input_seq:\n",
    "            return tf.zeros((10, input_length, 4), dtype=tf.float32)\n",
    "        else:\n",
    "            return tf.convert_to_tensor(\n",
    "                dinuc_shuffle(input_seq[0].numpy(), 10), dtype=tf.float32\n",
    "            )\n",
    "\n",
    "    explainer = shap.DeepExplainer(\n",
    "        model=(model.input, kb.squeeze(model.output, axis=1)),\n",
    "        data=bg_func\n",
    "    )\n",
    "\n",
    "    input_seqs = tf.convert_to_tensor([input_seq], dtype=tf.float32)\n",
    "\n",
    "    # We'll hide some of the internal DeepSHAP messages just for aesthetic purposes\n",
    "    scores = explainer.shap_values(input_seqs)[0]\n",
    "    return scores * input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'tensorflow_backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-8eafb1d1fd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mviz_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpret_deepshap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mviz_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpret_deepshap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoprior_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-02614d2f3a26>\u001b[0m in \u001b[0;36minterpret_deepshap\u001b[0;34m(model, input_seq)\u001b[0m\n\u001b[1;32m     16\u001b[0m     explainer = shap.DeepExplainer(\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbg_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/shap/shap/explainers/deep/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags, combine_mult_and_diffref)\u001b[0m\n\u001b[1;32m     95\u001b[0m             self.explainer = TFDeepExplainer(model=model, data=data,\n\u001b[1;32m     96\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_phase_flags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_phase_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 combine_mult_and_diffref=combine_mult_and_diffref)\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             self.explainer = PyTorchDeepExplainer(model=model, data=data,\n",
      "\u001b[0;32m~/lib/shap/shap/explainers/deep/deep_tf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags, combine_mult_and_diffref)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# if keras is installed and already has a session then use it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SESSION\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'tensorflow_backend'"
     ]
    }
   ],
   "source": [
    "viz_sequence.plot_weights(interpret_deepshap(prior_model, input_seq)[400:600])\n",
    "viz_sequence.plot_weights(interpret_deepshap(noprior_model, input_seq)[400:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_ism(model, input_seq):\n",
    "    \"\"\"\n",
    "    Computes in silico mutagenesis for an L x 4 one-hot\n",
    "    encoded sequence.\n",
    "    \"\"\"\n",
    "    assert input_seq.shape == (input_length, 4)\n",
    "    mutations = np.tile(input_seq, (len(input_seq) + 1, 1, 1))\n",
    "    inds = np.arange(len(input_seq))\n",
    "    mutations[(inds + 1, inds)] = 0  # First one is the original\n",
    "\n",
    "    mutations = tf.convert_to_tensor(mutations, dtype=tf.float32)\n",
    "    pred_logits = np.squeeze(model(mutations).numpy(), axis=1)\n",
    "    return np.expand_dims(pred_logits[0] - pred_logits[1:], axis=1) * input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a few random positive sequences from the test set\n",
    "rng = np.random.RandomState(20200930)\n",
    "rand_inds = rng.choice(test_data_loader.pos_inds, size=5, replace=False)\n",
    "pos_coords = test_data_loader.coords[rand_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1:208342950-208343950\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show the importance scores\n",
    "genome_reader = pyfaidx.Fasta(reference_fasta_path)\n",
    "center_slice = slice(300, 700)\n",
    "for chrom, start, end in pos_coords:\n",
    "    print(\"%s:%d-%d\" % (chrom, start, end))\n",
    "    print(\"-------------------------------\")\n",
    "    \n",
    "    input_seq = dna_to_one_hot([genome_reader[chrom][start:end].seq])[0]\n",
    "    \n",
    "    print(\"With prior:\")\n",
    "    print(\"Input gradients\")\n",
    "    viz_sequence.plot_weights(\n",
    "        interpret_input_grad(prior_model, input_seq)[center_slice],\n",
    "        subticks_frequency=100\n",
    "    )\n",
    "    print(\"DeepSHAP\")\n",
    "    viz_sequence.plot_weights(\n",
    "        interpret_deepshap(prior_model, input_seq)[center_slice],\n",
    "        subticks_frequency=100\n",
    "    )\n",
    "    print(\"ISM\")\n",
    "    viz_sequence.plot_weights(\n",
    "        interpret_ism(prior_model, input_seq)[center_slice],\n",
    "        subticks_frequency=100\n",
    "    )\n",
    "    print(\"No prior:\")\n",
    "    print(\"Input gradients\")\n",
    "    viz_sequence.plot_weights(\n",
    "        interpret_input_grad(noprior_model, input_seq)[center_slice],\n",
    "        subticks_frequency=100\n",
    "    )\n",
    "    print(\"DeepSHAP\")\n",
    "    viz_sequence.plot_weights(\n",
    "        interpret_deepshap(noprior_model, input_seq)[center_slice],\n",
    "        subticks_frequency=100\n",
    "    )\n",
    "    print(\"ISM\")\n",
    "    viz_sequence.plot_weights(\n",
    "        interpret_ism(noprior_model, input_seq)[center_slice],\n",
    "        subticks_frequency=100\n",
    "    )\n",
    "    print(\"===============================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
