{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/amtseng/miniconda3/envs/att-priors/lib/python3.7/site-packages/ipykernel_launcher.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3953ca33e36746659a48265f9904405c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src/\"))\n",
    "import model.util as model_util\n",
    "import model.profile_models as profile_models\n",
    "import model.train_profile_model as train_profile_model\n",
    "import model.binary_models as binary_models\n",
    "import model.train_binary_model as train_binary_model\n",
    "import feature.util as feature_util\n",
    "import feature.make_profile_dataset as make_profile_dataset\n",
    "import feature.make_binary_dataset as make_binary_dataset\n",
    "import plot.viz_sequence as viz_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()  # It is necessary to call this before the tqdm.notebook submodule is available\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for the model and data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared paths/constants\n",
    "reference_fasta = \"/users/amtseng/genomes/hg38.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/hg38.canon.chrom.sizes\"\n",
    "input_length = 1000 if model_type == \"binary\" else 1346\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_spec_path = \"/users/amtseng/att_priors/data/processed/ENCODE_TFChIP/%s/config/SPI1/SPI1_training_paths.json\" % model_type\n",
    "# num_tasks = 4\n",
    "# model_class = binary_models.BinaryPredictor if model_type == \"binary\" else profile_models.ProfilePredictorWithControls\n",
    "# use_controls = True\n",
    "# noprior_model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s_models/SPI1\" % model_type\n",
    "# prior_model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s_models/SPI1_prior\" % model_type\n",
    "# chrom_set = [\"chr1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_spec_path = \"/users/amtseng/att_priors/data/processed/ENCODE_TFChIP/%s/config/E2F6/E2F6_training_paths.json\" % model_type\n",
    "# num_tasks = 2\n",
    "# model_class = binary_models.BinaryPredictor if model_type == \"binary\" else profile_models.ProfilePredictorWithControls\n",
    "# use_controls = True\n",
    "# noprior_model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s_models/E2F6\" % model_type\n",
    "# prior_model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s_models/E2F6_prior\" % model_type\n",
    "# chrom_set = [\"chr1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_spec_path = \"/users/amtseng/att_priors/data/processed/ENCODE_DNase/%s/config/HepG2/HepG2_training_paths.json\" % model_type\n",
    "num_tasks = 2\n",
    "model_class = binary_models.BinaryPredictor if model_type == \"binary\" else profile_models.ProfilePredictorWithoutControls\n",
    "use_controls = False\n",
    "noprior_model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s_models/HepG2\" % model_type\n",
    "prior_model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s_models/HepG2_prior\" % model_type\n",
    "chrom_set = [\"chr1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the file specs\n",
    "with open(files_spec_path, \"r\") as f:\n",
    "    files_spec = json.load(f)\n",
    "if model_type == \"binary\":\n",
    "    labels_hdf5 = files_spec[\"labels_hdf5\"]\n",
    "    bin_labels_npy = files_spec[\"bin_labels_npy\"]\n",
    "else:\n",
    "    peaks_bed = files_spec[\"peak_beds\"][0]  # First peaks BED, arbitrarily\n",
    "    profile_hdf5 = files_spec[\"profile_hdf5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(model_base_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {model_base_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(model_base_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_paths(\n",
    "    model_base_path, metric_name=\"val_prof_corr_losses\",\n",
    "    reduce_func=(lambda values: np.mean(values)), compare_func=(lambda x, y: x < y),\n",
    "    print_found_values=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Looks in `model_base_path` and for each run, returns the full path to\n",
    "    the best epoch. By default, the best epoch in a run is determined by\n",
    "    the lowest validation profile loss.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(model_base_path, run_num) for run_num in os.listdir(model_base_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    model_paths = []\n",
    "    for run_num in sorted(metrics.keys(), key=lambda x: int(x)):\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            model_path = os.path.join(model_base_path, run_num, \"model_ckpt_epoch_%d.pt\" % best_epoch_in_run)\n",
    "            model_paths.append(model_path)\n",
    "            if print_found_values:\n",
    "                print(\"\\tRun %s, epoch %d: %6.2f\" % (run_num, best_epoch_in_run, best_val_in_run))\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return model_paths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"val_prof_corr_losses\" if model_type == \"profile\" else \"val_corr_losses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRun 1, epoch 1:   0.26\n",
      "\tRun 2, epoch 1:   0.26\n",
      "\tRun 3, epoch 1:   0.24\n",
      "\tRun 4, epoch 1:   0.25\n",
      "\tRun 5, epoch 1:   0.24\n",
      "\tRun 6, epoch 1:   0.24\n",
      "\tRun 7, epoch 1:   0.24\n",
      "\tRun 8, epoch 1:   0.24\n",
      "\tRun 9, epoch 1:   0.24\n",
      "\tRun 10, epoch 1:   0.24\n",
      "\tRun 11, epoch 1:   0.25\n",
      "\tRun 12, epoch 1:   0.25\n",
      "\tRun 13, epoch 1:   0.24\n",
      "\tRun 14, epoch 1:   0.24\n",
      "\tRun 15, epoch 1:   0.24\n",
      "\tRun 16, epoch 1:   0.24\n",
      "\tRun 17, epoch 1:   0.25\n",
      "\tRun 18, epoch 1:   0.24\n",
      "\tRun 19, epoch 1:   0.24\n",
      "\tRun 20, epoch 1:   0.24\n",
      "\tRun 21, epoch 1:   0.26\n",
      "\tRun 22, epoch 1:   0.25\n",
      "\tRun 23, epoch 1:   0.24\n",
      "\tRun 24, epoch 1:   0.24\n",
      "\tRun 25, epoch 1:   0.25\n",
      "\tRun 26, epoch 1:   0.26\n",
      "\tRun 27, epoch 1:   0.26\n",
      "\tRun 28, epoch 1:   0.25\n"
     ]
    }
   ],
   "source": [
    "noprior_model_paths = get_model_paths(noprior_model_base_path, metric_name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fddadabfbe8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprior_model_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_model_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d99f645ded76>\u001b[0m in \u001b[0;36mget_model_paths\u001b[0;34m(model_base_path, metric_name, reduce_func, compare_func, print_found_values)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get the metrics, ignoring empty or nonexistent metrics.json files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mrun_num\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mimport_metrics_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_base_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# Remove empties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d99f645ded76>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get the metrics, ignoring empty or nonexistent metrics.json files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mrun_num\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mimport_metrics_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_base_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# Remove empties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-35d65e1bdee5>\u001b[0m in \u001b[0;36mimport_metrics_json\u001b[0;34m(model_base_path, run_num)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "prior_model_paths = get_model_paths(prior_model_base_path, metric_name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(model_path):\n",
    "    model = model_util.restore_model(model_class, model_path)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Use classes from `make_profile_dataset` to prepare positive and negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps coordinates to 1-hot encoded sequence\n",
    "coords_to_seq = feature_util.CoordsToSeq(reference_fasta, center_size_to_use=input_length)\n",
    "\n",
    "if model_type == \"binary\":\n",
    "    # Maps bin index to profiles\n",
    "    bins_to_vals = make_binary_dataset.BinsToVals(labels_hdf5)\n",
    "\n",
    "    # Maps many bin indices to coordinates, inputs sequences, and output values for the network\n",
    "    def bin_inds_to_network_inputs(bin_inds):\n",
    "        coords, outout_vals = bins_to_vals(bin_inds) \n",
    "        input_seqs = coords_to_seq(coords)\n",
    "        return coords, input_seqs, outout_vals\n",
    "    \n",
    "    # Import set of all labels\n",
    "    labels_array = np.load(bin_labels_npy, allow_pickle=True)\n",
    "\n",
    "    # Get set of positive and negative bin indices\n",
    "    chrom_mask = np.isin(labels_array[:, 0], chrom_set)\n",
    "    pos_examples = np.where(chrom_mask & (labels_array[:, 1] == 1))[0]\n",
    "else:\n",
    "    # Maps coordinates to profiles\n",
    "    coords_to_vals = make_profile_dataset.CoordsToVals(profile_hdf5, profile_length)\n",
    "\n",
    "    # Maps many coordinates to inputs sequences and profiles for the network\n",
    "    def coords_to_network_inputs(coords):\n",
    "        input_seq = coords_to_seq(coords)\n",
    "        profs = coords_to_vals(coords)\n",
    "        return input_seq, np.swapaxes(profs, 1, 2)\n",
    "    # Import set of positive peaks\n",
    "    pos_coords_table = pd.read_csv(peaks_bed, sep=\"\\t\", header=None, compression=\"gzip\")\n",
    "    # Filter for only the desired chromosomes\n",
    "    pos_coords_table = pos_coords_table[pos_coords_table[0].isin(chrom_set)]\n",
    "    pos_examples = pos_coords_table.values[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_grads(model, model_type, coords_or_bin_inds):\n",
    "    \"\"\"\n",
    "    Fetches the necessary data from the given coordinates or bin indices\n",
    "    and runs it through a profile or binary model. Returns the input\n",
    "    gradients and the input sequences as NumPy arrays.\n",
    "    \"\"\"\n",
    "    if model_type == \"binary\":\n",
    "        coords, input_seqs_np, output_vals_np = bin_inds_to_network_inputs(coords_or_bin_inds)\n",
    "        output_vals = model_util.place_tensor(torch.tensor(output_vals_np)).float()\n",
    "    else:\n",
    "        input_seqs_np, profiles = coords_to_network_inputs(coords_or_bin_inds)\n",
    "        profiles = model_util.place_tensor(torch.tensor(profiles)).float()\n",
    "        if use_controls:\n",
    "            true_profs = profiles[:, :num_tasks, :, :]\n",
    "            cont_profs = profiles[:, num_tasks:, :, :]\n",
    "        else:\n",
    "            true_profs, cont_profs = profiles, None\n",
    "    input_seqs = model_util.place_tensor(torch.tensor(input_seqs_np)).float()\n",
    "\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Run through the model\n",
    "    input_seqs.requires_grad = True  # Set gradient required\n",
    "    if model_type == \"binary\":\n",
    "        output = model(input_seqs)\n",
    "    else:\n",
    "        output, _ = model(input_seqs, cont_profs)\n",
    "    \n",
    "    # Compute input gradients\n",
    "    input_grads, = torch.autograd.grad(\n",
    "        output, input_seqs,\n",
    "        grad_outputs=model_util.place_tensor(torch.ones(output.size())),\n",
    "        retain_graph=True, create_graph=True\n",
    "    )\n",
    "    input_grads_np = input_grads.detach().cpu().numpy()\n",
    "    \n",
    "    return input_grads_np, input_seqs_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample of 100 random coordinates/bins\n",
    "num_samples = 100\n",
    "sample = pos_examples[np.random.choice(len(pos_examples), size=num_samples, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model_paths, sample, batch_size=128):\n",
    "    \"\"\"\n",
    "    Given a list of paths to M models and a list of N coordinates, computes\n",
    "    the input gradients over all models, returning an M x N x I x 4 array of\n",
    "    gradient values and an N x I x 4 array of one-hot encoded sequence.\n",
    "    \"\"\"\n",
    "    num_models, num_samples = len(model_paths), len(sample)\n",
    "    \n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    \n",
    "    all_input_grads = np.empty((num_models, num_samples, input_length, 4))\n",
    "    all_one_hot_seqs = np.empty((num_samples, input_length, 4))\n",
    "    \n",
    "    for i in tqdm.notebook.trange(num_models):\n",
    "        model = restore_model(model_paths[i])\n",
    "        for j in range(num_batches):\n",
    "            batch_slice = slice(j * batch_size, (j + 1) * batch_size)\n",
    "            batch = sample[batch_slice]\n",
    "            input_grads, one_hot_seqs = get_input_grads(model, model_type, batch)\n",
    "            all_input_grads[i, batch_slice] = input_grads\n",
    "            if i == 0:\n",
    "                all_one_hot_seqs[batch_slice] = one_hot_seqs\n",
    "    return all_input_grads, all_one_hot_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradients and 1-hot seqs\n",
    "prior_grads, one_hot_seqs = compute_gradients(prior_model_paths, sample)\n",
    "noprior_grads, _ = compute_gradients(noprior_model_paths, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Plot out a few examples over some different models\n",
    "# center_slice = slice(600, 700)\n",
    "# for i in np.random.choice(num_samples, size=3):\n",
    "#     print(sample[i])\n",
    "#     print(\"=========================\")\n",
    "#     print(\"Without priors:\")\n",
    "#     print(\"---------------\")\n",
    "#     for j in np.random.choice(len(noprior_model_paths), size=3):\n",
    "#         print(\"Model %d\" % (j + 1))\n",
    "#         viz_sequence.plot_weights(noprior_grads[j, i, center_slice] * one_hot_seqs[i, center_slice])\n",
    "#     print(\"With priors:\")\n",
    "#     print(\"------------\")\n",
    "#     for j in np.random.choice(len(prior_model_paths), size=3):\n",
    "#         print(\"Model %d\" % (j + 1))\n",
    "#         viz_sequence.plot_weights(prior_grads[j, i, center_slice] * one_hot_seqs[i, center_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_jaccard(seq_1, seq_2):\n",
    "    \"\"\"\n",
    "    Takes two gradient sequences (I x 4 arrays) and computes a similarity between\n",
    "    them, using a continuous Jaccard metric.\n",
    "    \"\"\"\n",
    "    ab_1, ab_2 = np.abs(seq_1), np.abs(seq_2)\n",
    "    inter = np.minimum(ab_1, ab_2) * np.sign(seq_1) * np.sign(seq_2)\n",
    "    union = np.maximum(ab_1, ab_2)\n",
    "    cont_jaccard = np.sum(inter, axis=1) / np.sum(union, axis=1)\n",
    "    return np.sum(cont_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(seq_1, seq_2):\n",
    "    \"\"\"\n",
    "    Takes two gradient sequences (I x 4 arrays) and computes a similarity between\n",
    "    them, using a cosine similarity.\n",
    "    \"\"\"\n",
    "    dot = np.sum(seq_1 * seq_2, axis=1)\n",
    "    mag_1, mag_2 = np.sqrt(np.sum(seq_1 * seq_1, axis=1)), np.sqrt(np.sum(seq_2 * seq_2, axis=1))\n",
    "    # Ensure that base with all 0 is given a similarity score of 0\n",
    "    zero_mask = (mag_1 == 0) | (mag_2 == 0)\n",
    "    mag_1[zero_mask] = 1\n",
    "    mag_2[zero_mask] = 1\n",
    "    return np.sum(dot / (mag_1 * mag_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(input_grads, sim_func=cont_jaccard):\n",
    "    \"\"\"\n",
    "    Given the M x N x I x 4 input gradients returned by `compute_gradients`,\n",
    "    computes an N x M x M similarity matrix of similarity across models (i.e.\n",
    "    each coordinate gets a similarity matrix across different models).\n",
    "    By defaults uses the continuous Jaccard similarity score\n",
    "    \"\"\"\n",
    "    num_models, num_coords = input_grads.shape[0], input_grads.shape[1]\n",
    "    \n",
    "    sim_mats = np.empty((num_coords, num_models, num_models))\n",
    "    for i in tqdm.notebook.trange(num_coords):\n",
    "        for j in range(num_models):\n",
    "            sim_mats[i, j, j] = 0\n",
    "            for k in range(j):\n",
    "                sim_score = sim_func(input_grads[j][i], input_grads[k][i])\n",
    "                sim_mats[i, j, k] = sim_score\n",
    "                sim_mats[i, k, j] = sim_score\n",
    "    return sim_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_sim_matrix = compute_similarity_matrix(prior_grads, sim_func=cosine_sim)\n",
    "noprior_sim_matrix = compute_similarity_matrix(noprior_grads, sim_func=cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprior_avg_sims, prior_avg_sims = [], []\n",
    "bin_num = 20\n",
    "for i in range(num_samples):\n",
    "    noprior_avg_sims.append(np.nanmean(noprior_sim_matrix[i][np.tril_indices(len(noprior_model_paths), k=-1)]))\n",
    "    prior_avg_sims.append(np.nanmean(prior_sim_matrix[i][np.tril_indices(len(prior_model_paths), k=-1)]))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(\n",
    "    [noprior_avg_sims, prior_avg_sims],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"]\n",
    ")\n",
    "plt.title(\n",
    "    \"Average pairwise similarities between different models\" +\n",
    "    \"\\nComputed over %d/%d models without/with priors and 100 randomly drawn test bins\" % (len(noprior_model_paths), len(prior_model_paths)) +\n",
    "    \"\\nTrained on all bins\"\n",
    ")\n",
    "plt.xlabel(\"Cosine similarity\")\n",
    "\n",
    "print(\"Average similarity without priors: %f\" % np.nanmean(noprior_avg_sims))\n",
    "print(\"Average similarity with priors: %f\" % np.nanmean(prior_avg_sims))\n",
    "t, p = scipy.stats.ttest_ind(prior_avg_sims, noprior_avg_sims)\n",
    "print(\"Test of difference of means: t = %f, p = %f\" % (t, p / 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
