{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src/\"))\n",
    "import extract.data_loading as data_loading\n",
    "import extract.compute_predictions as compute_predictions\n",
    "import extract.compute_shap as compute_shap\n",
    "import extract.compute_ism as compute_ism\n",
    "import model.util as model_util\n",
    "import model.profile_models as profile_models\n",
    "import model.binary_models as binary_models\n",
    "import plot.viz_sequence as viz_sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()  # It is necessary to call this before the tqdm.notebook submodule is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for the model and data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"profile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared paths/constants\n",
    "reference_fasta = \"/users/amtseng/genomes/hg38.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/hg38.canon.chrom.sizes\"\n",
    "data_base_path = \"/users/amtseng/att_priors/data/processed/\"\n",
    "model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s/\" % model_type\n",
    "chrom_set = [\"chr1\"]\n",
    "input_length = 1346 if model_type == \"profile\" else 1000\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPI1\n",
    "condition_name = \"SPI1\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_TFChIP/%s/config/SPI1/SPI1_training_paths.json\" % model_type)\n",
    "num_tasks = 4\n",
    "num_strands = 2\n",
    "task_index = None\n",
    "controls = \"matched\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithMatchedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_model_base_path = os.path.join(model_base_path, \"SPI1_keep1/\")\n",
    "prior_model_base_path = os.path.join(model_base_path, \"SPI1_prior_keep1/\")\n",
    "peak_retention = \"top 1%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GATA2\n",
    "condition_name = \"GATA2\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_TFChIP/%s/config/GATA2/GATA2_training_paths.json\" % model_type)\n",
    "num_tasks = 3\n",
    "num_strands = 2\n",
    "task_index = None\n",
    "controls = \"matched\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithMatchedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_model_base_path = os.path.join(model_base_path, \"GATA2_keep1/\")\n",
    "prior_model_base_path = os.path.join(model_base_path, \"GATA2_prior_keep1/\")\n",
    "peak_retention = \"top 1%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K562\n",
    "condition_name = \"K562\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_DNase/%s/config/K562/K562_training_paths.json\" % model_type)\n",
    "num_tasks = 1\n",
    "num_strands = 1\n",
    "task_index = None\n",
    "controls = \"shared\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithSharedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_model_base_path = os.path.join(model_base_path, \"K562_keep1/\")\n",
    "prior_model_base_path = os.path.join(model_base_path, \"K562_prior_keep1/\")\n",
    "peak_retention = \"top 1%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPNet\n",
    "condition_name = \"BPNet\"\n",
    "reference_fasta = \"/users/amtseng/genomes/mm10.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/mm10.canon.chrom.sizes\"\n",
    "files_spec_path = os.path.join(data_base_path, \"BPNet_ChIPseq/%s/config/BPNet_training_paths.json\" % model_type)\n",
    "num_tasks = 3\n",
    "num_strands = 2\n",
    "task_index = None\n",
    "controls = \"shared\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithSharedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_model_base_path = os.path.join(model_base_path, \"BPNet/\")\n",
    "prior_model_base_path = os.path.join(model_base_path, \"BPNet_prior/\")\n",
    "peak_retention = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all runs/epochs with random initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(model_base_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {model_base_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(model_base_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_paths(\n",
    "    model_base_path, metric_name=\"val_prof_corr_losses\",\n",
    "    reduce_func=(lambda values: np.mean(values)), compare_func=(lambda x, y: x < y),\n",
    "    print_found_values=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Looks in `model_base_path` and for each run, returns the full path to\n",
    "    the best epoch. By default, the best epoch in a run is determined by\n",
    "    the lowest validation profile loss.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(model_base_path, run_num) for run_num in os.listdir(model_base_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    model_paths, metric_vals = [], []\n",
    "    for run_num in sorted(metrics.keys(), key=lambda x: int(x)):\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            model_path = os.path.join(model_base_path, run_num, \"model_ckpt_epoch_%d.pt\" % best_epoch_in_run)\n",
    "            model_paths.append(model_path)\n",
    "            metric_vals.append(best_val_in_run)\n",
    "            if print_found_values:\n",
    "                print(\"\\tRun %s, epoch %d: %6.2f\" % (run_num, best_epoch_in_run, best_val_in_run))\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return model_paths, metric_vals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"val_prof_corr_losses\" if model_type == \"profile\" else \"val_corr_losses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noprior_model_paths, noprior_metric_vals = get_model_paths(noprior_model_base_path, metric_name=metric_name)\n",
    "prior_model_paths, prior_metric_vals = get_model_paths(prior_model_base_path, metric_name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(model_path):\n",
    "    model = model_util.restore_model(model_class, model_path)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior_model_paths = prior_model_paths[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Create an input data loader, that maps coordinates or bin indices to data needed for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"profile\":\n",
    "    input_func = data_loading.get_profile_input_func(\n",
    "        files_spec_path, input_length, profile_length, reference_fasta\n",
    "    )\n",
    "    pos_examples = data_loading.get_positive_profile_coords(\n",
    "        files_spec_path, chrom_set=chrom_set\n",
    "    )\n",
    "else:\n",
    "    input_func = data_loading.get_binary_input_func(\n",
    "        files_spec_path, input_length, reference_fasta\n",
    "    )\n",
    "    pos_examples = data_loading.get_positive_binary_bins(\n",
    "        files_spec_path, chrom_set=chrom_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample of 100 random coordinates/bins\n",
    "num_samples = 100\n",
    "sample = pos_examples[np.random.choice(len(pos_examples), size=num_samples, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For profile models, add a random jitter to avoid center-bias\n",
    "if model_type == \"profile\":\n",
    "    jitters = np.random.randint(-128, 128 + 1, size=len(sample))\n",
    "    sample[:, 1] = sample[:, 1] + jitters\n",
    "    sample[:, 2] = sample[:, 2] + jitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model_paths, sample):\n",
    "    \"\"\"\n",
    "    Given a list of paths to M models and a list of N coordinates or bins, computes\n",
    "    the input gradients over all models, returning an M x N x I x 4 array of\n",
    "    gradient values and an N x I x 4 array of one-hot encoded sequence.\n",
    "    \"\"\"\n",
    "    num_models, num_samples = len(model_paths), len(sample)\n",
    "    \n",
    "    all_input_grads = np.empty((num_models, num_samples, input_length, 4))\n",
    "    all_one_hot_seqs = np.empty((num_samples, input_length, 4))\n",
    "    \n",
    "    for i in tqdm.notebook.trange(num_models):\n",
    "        model = restore_model(model_paths[i])\n",
    "            \n",
    "        if model_type == \"profile\":\n",
    "            results = compute_predictions.get_profile_model_predictions(                                              \n",
    "                model, sample, num_tasks, input_func, controls=controls,                        \n",
    "                return_losses=False, return_gradients=True, show_progress=False                                         \n",
    "            )\n",
    "        else:\n",
    "            results = compute_predictions.get_binary_model_predictions(                                              \n",
    "                model, sample, input_func,                      \n",
    "                return_losses=False, return_gradients=True, show_progress=False                                         \n",
    "            )\n",
    "\n",
    "        all_input_grads[i] = results[\"input_grads\"]\n",
    "        if i == 0:\n",
    "            all_one_hot_seqs = results[\"input_seqs\"]\n",
    "    return all_input_grads, all_one_hot_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_scores(model_paths, sample, batch_size=128):\n",
    "    \"\"\"\n",
    "    Given a list of paths to M models and a list of N coordinates or bins, computes\n",
    "    the SHAP scores over all models, returning an M x N x I x 4 array of\n",
    "    SHAP scores and an N x I x 4 array of one-hot encoded sequence.\n",
    "    \"\"\"\n",
    "    num_models, num_samples = len(model_paths), len(sample)\n",
    "    \n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    \n",
    "    all_shap_scores = np.empty((num_models, num_samples, input_length, 4))\n",
    "    all_one_hot_seqs = np.empty((num_samples, input_length, 4))\n",
    "    \n",
    "    for i in tqdm.notebook.trange(num_models):\n",
    "        model = restore_model(model_paths[i])\n",
    "        \n",
    "        if model_type == \"profile\":\n",
    "            shap_explainer = compute_shap.create_profile_explainer(\n",
    "                model, input_length, profile_length, num_tasks, num_strands, controls,\n",
    "                task_index=task_index\n",
    "            )\n",
    "        else:\n",
    "            shap_explainer = compute_shap.create_binary_explainer(\n",
    "                model, input_length, task_index=task_index\n",
    "            )\n",
    "\n",
    "        for j in range(num_batches):\n",
    "            batch_slice = slice(j * batch_size, (j + 1) * batch_size)\n",
    "            batch = sample[batch_slice]\n",
    "            \n",
    "            if model_type == \"profile\":\n",
    "                input_seqs, profiles = input_func(sample)\n",
    "                shap_scores = shap_explainer(\n",
    "                    input_seqs, cont_profs=profiles[:, num_tasks:], hide_shap_output=True\n",
    "                )\n",
    "            else:\n",
    "                input_seqs, _, _ = input_func(sample)\n",
    "                shap_scores = shap_explainer(\n",
    "                    input_seqs, hide_shap_output=True\n",
    "                )\n",
    "            \n",
    "            all_shap_scores[i, batch_slice] = shap_scores\n",
    "            if i == 0:\n",
    "                all_one_hot_seqs[batch_slice] = input_seqs\n",
    "    return all_shap_scores, all_one_hot_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the importance scores and 1-hot seqs\n",
    "imp_type = (\"SHAP scores\", \"input gradients\")[0]\n",
    "imp_func = compute_shap_scores if imp_type == \"SHAP scores\" else compute_gradients\n",
    "noprior_scores, _ = imp_func(noprior_model_paths, sample)\n",
    "prior_scores, one_hot_seqs = imp_func(prior_model_paths, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot out a few examples over some different models\n",
    "center_slice = slice(None, None)\n",
    "for i in np.random.choice(num_samples, size=3, replace=False):\n",
    "    print(i, sample[i])\n",
    "    print(\"=========================\")\n",
    "    print(\"Without priors:\")\n",
    "    print(\"---------------\")\n",
    "    for j in np.random.choice(len(noprior_model_paths), size=3, replace=False):\n",
    "        print(\"Model %d\" % (j + 1))\n",
    "        # viz_sequence.plot_weights(noprior_scores[j, i, center_slice] * one_hot_seqs[i, center_slice])\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        plt.plot(np.sum(noprior_scores[j, i] * one_hot_seqs[i], axis=1))\n",
    "        plt.show()\n",
    "    print(\"With priors:\")\n",
    "    print(\"------------\")\n",
    "    for j in np.random.choice(len(prior_model_paths), size=3, replace=False):\n",
    "        print(\"Model %d\" % (j + 1))\n",
    "        # viz_sequence.plot_weights(prior_scores[j, i, center_slice] * one_hot_seqs[i, center_slice])\n",
    "        plt.figure(figsize=(20, 2))\n",
    "        plt.plot(np.sum(prior_scores[j, i] * one_hot_seqs[i], axis=1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_jaccard(seq_1, seq_2):\n",
    "    \"\"\"\n",
    "    Takes two gradient sequences (I x 4 arrays) and computes a similarity between\n",
    "    them, using a continuous Jaccard metric.\n",
    "    \"\"\"\n",
    "    ab_1, ab_2 = np.abs(seq_1), np.abs(seq_2)\n",
    "    inter = np.sum(np.minimum(ab_1, ab_2) * np.sign(seq_1) * np.sign(seq_2), axis=1)\n",
    "    union = np.sum(np.maximum(ab_1, ab_2), axis=1)\n",
    "    zero_mask = union == 0\n",
    "    inter[zero_mask] = 0\n",
    "    union[zero_mask] = 1\n",
    "    return np.sum(inter / union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(seq_1, seq_2):\n",
    "    \"\"\"\n",
    "    Takes two gradient sequences (I x 4 arrays) and computes a similarity between\n",
    "    them, using a cosine similarity.\n",
    "    \"\"\"\n",
    "    seq_1, seq_2 = np.ravel(seq_1), np.ravel(seq_2)\n",
    "    dot = np.sum(seq_1 * seq_2)\n",
    "    mag_1, mag_2 = np.sqrt(np.sum(seq_1 * seq_1)), np.sqrt(np.sum(seq_2 * seq_2))\n",
    "    return dot / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(imp_scores, sim_func=cosine_sim):\n",
    "    \"\"\"\n",
    "    Given the M x N x I x 4 importance scores returned by `compute_gradients`\n",
    "    or `compute_shap_scores`, computes an N x M x M similarity matrix of\n",
    "    similarity across models (i.e. each coordinate gets a similarity matrix\n",
    "    across different models). By default uses cosine similarity.\n",
    "    \"\"\"\n",
    "    num_models, num_coords = imp_scores.shape[0], imp_scores.shape[1]\n",
    "    \n",
    "    sim_mats = np.empty((num_coords, num_models, num_models))\n",
    "    for i in tqdm.notebook.trange(num_coords):\n",
    "        for j in range(num_models):\n",
    "            sim_mats[i, j, j] = 0\n",
    "            for k in range(j):\n",
    "                sim_score = sim_func(imp_scores[j][i], imp_scores[k][i])\n",
    "                sim_mats[i, j, k] = sim_score\n",
    "                sim_mats[i, k, j] = sim_score\n",
    "    return sim_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_type = (\"Cosine\", \"Continuous Jaccard\")[0]\n",
    "sim_func = cosine_sim if sim_type == \"Cosine\" else cont_jaccard\n",
    "noprior_sim_matrix = compute_similarity_matrix(noprior_scores, sim_func=sim_func)\n",
    "prior_sim_matrix = compute_similarity_matrix(prior_scores, sim_func=sim_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprior_avg_sims, prior_avg_sims = [], []\n",
    "bin_num = 20\n",
    "for i in range(num_samples):\n",
    "    noprior_avg_sims.append(np.mean(noprior_sim_matrix[i][np.tril_indices(len(noprior_model_paths), k=-1)]))\n",
    "    prior_avg_sims.append(np.mean(prior_sim_matrix[i][np.tril_indices(len(prior_model_paths), k=-1)]))\n",
    "noprior_avg_sims, prior_avg_sims = np.array(noprior_avg_sims), np.array(prior_avg_sims)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(\n",
    "    [noprior_avg_sims, prior_avg_sims],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With Fourier prior\"], color=[\"red\", \"blue\"]\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    (\"Mean pairwise similarities of %s between different random initializations\" % imp_type) +\n",
    "    (\"\\n%s %s models trained on %s peaks\" % (condition_name, model_type, peak_retention)) +\n",
    "    \"\\nComputed over %d/%d without/with priors and %d randomly drawn test peaks\" % (len(noprior_model_paths), len(prior_model_paths), num_samples)\n",
    ")\n",
    "plt.xlabel(\"%s similarity\" % sim_type)\n",
    "\n",
    "print(\"Average similarity without priors: %f\" % np.nanmean(noprior_avg_sims))\n",
    "print(\"Average similarity with priors: %f\" % np.nanmean(prior_avg_sims))\n",
    "t, p = scipy.stats.ttest_rel(prior_avg_sims, noprior_avg_sims)\n",
    "print(\"One-sided paired t-test p: %f\" % (p / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim_diffs = prior_avg_sims - noprior_avg_sims\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(avg_sim_diffs, bins=20)\n",
    "plt.title(\n",
    "    (\"Paired difference of %s similarity between different random initializations\" % imp_type) +\n",
    "    (\"\\n%s %s models trained on %s peaks\" % (condition_name, model_type, peak_retention)) +\n",
    "    \"\\nComputed over %d/%d without/with priors and %d randomly drawn test peaks\" % (len(noprior_model_paths), len(prior_model_paths), num_samples)\n",
    ")\n",
    "plt.xlabel(\"Average similarity difference: with prior - no prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(sim_matrix):\n",
    "    num_examples, num_models, _ = sim_matrix.shape\n",
    "    bias_vals = []\n",
    "    for i in range(num_models):\n",
    "        avg = np.sum(sim_matrix[:, i]) / (num_examples * (num_models - 1))\n",
    "        bias_vals.append(avg)\n",
    "        print(\"%d: %f\" % (i + 1, avg))\n",
    "    return bias_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Model-specific bias without priors\")\n",
    "noprior_bias_vals = get_bias(noprior_sim_matrix)\n",
    "print(\"Model-specific bias with priors\")\n",
    "prior_bias_vals = get_bias(prior_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle(\"Model-specific average Jaccard similarity vs model performance\")\n",
    "ax[0].scatter(noprior_bias_vals, np.array(noprior_metric_vals)[noprior_keep_mask])\n",
    "ax[0].set_title(\"No priors\")\n",
    "ax[1].scatter(prior_bias_vals, np.array(prior_metric_vals)[prior_keep_mask])\n",
    "ax[1].set_title(\"With priors\")\n",
    "plt.grid(False)\n",
    "fig.text(0.5, 0.04, \"Average Jaccard similarity with other models over all samples\", ha=\"center\", va=\"center\")\n",
    "fig.text(0.06, 0.5, \"Model profile validation loss\", ha=\"center\", va=\"center\", rotation=\"vertical\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
