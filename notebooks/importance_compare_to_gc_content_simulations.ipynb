{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src/\"))\n",
    "import extract.data_loading as data_loading\n",
    "import extract.compute_predictions as compute_predictions\n",
    "import extract.compute_shap as compute_shap\n",
    "import extract.compute_ism as compute_ism\n",
    "import model.util as model_util\n",
    "import model.profile_models as profile_models\n",
    "import model.binary_models as binary_models\n",
    "import plot.viz_sequence as viz_sequence\n",
    "import feature.util as feature_util\n",
    "import pyBigWig\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import json\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()  # It is necessary to call this before the tqdm.notebook submodule is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for the model and data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared paths/constants\n",
    "chrom_sizes = \"/users/amtseng/genomes/hg38.canon.chrom.sizes\"\n",
    "raw_data_base_path = \"/users/amtseng/att_priors/data/raw/\"\n",
    "proc_data_base_path = \"/users/amtseng/att_priors/data/processed/\"\n",
    "model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s/\" % model_type\n",
    "tfm_results_path = \"/users/amtseng/att_priors/results/tfmodisco/%s/\" % model_type\n",
    "chrom_set = [\"chr1\"]\n",
    "input_length = 1346 if model_type == \"profile\" else 1000\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPI1\n",
    "condition_name = \"SPI1-1task\"\n",
    "files_spec_path = os.path.join(proc_data_base_path, \"ENCODE_TFChIP/%s/config/SPI1-1task/SPI1-1task_training_paths.json\" % model_type)\n",
    "num_tasks = 1\n",
    "num_strands = 2\n",
    "controls = \"shared\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithSharedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "task_index = None\n",
    "motif_path = \"/users/amtseng/att_priors/results/SPI1_motifs/homer_motif1_trimmed.motif\"\n",
    "\n",
    "gc_probs = [0.50, 0.51, 0.52, 0.53, 0.54]  #, 0.55, 0.60]\n",
    "noprior_model_paths, prior_model_paths = [None] * len(gc_probs), [None] * len(gc_probs)\n",
    "\n",
    "noprior_model_paths[0] = os.path.join(model_base_path, \"SPI1-1task_simgc%0.2f/2/model_ckpt_epoch_3.pt\" % gc_probs[0])\n",
    "prior_model_paths[0] = os.path.join(model_base_path, \"SPI1-1task_prior_simgc%0.2f/2/model_ckpt_epoch_5.pt\" % gc_probs[0])\n",
    "\n",
    "noprior_model_paths[1] = os.path.join(model_base_path, \"SPI1-1task_simgc%0.2f/3/model_ckpt_epoch_4.pt\" % gc_probs[1])\n",
    "prior_model_paths[1] = os.path.join(model_base_path, \"SPI1-1task_prior_simgc%0.2f/3/model_ckpt_epoch_4.pt\" % gc_probs[1])\n",
    "\n",
    "noprior_model_paths[2] = os.path.join(model_base_path, \"SPI1-1task_simgc%0.2f/3/model_ckpt_epoch_5.pt\" % gc_probs[2])\n",
    "prior_model_paths[2] = os.path.join(model_base_path, \"SPI1-1task_prior_simgc%0.2f/3/model_ckpt_epoch_5.pt\" % gc_probs[2])\n",
    "\n",
    "noprior_model_paths[3] = os.path.join(model_base_path, \"SPI1-1task_simgc%0.2f/2/model_ckpt_epoch_5.pt\" % gc_probs[3])\n",
    "prior_model_paths[3] = os.path.join(model_base_path, \"SPI1-1task_prior_simgc%0.2f/1/model_ckpt_epoch_4.pt\" % gc_probs[3])\n",
    "\n",
    "noprior_model_paths[4] = os.path.join(model_base_path, \"SPI1-1task_simgc%0.2f/3/model_ckpt_epoch_3.pt\" % gc_probs[4])\n",
    "prior_model_paths[4] = os.path.join(model_base_path, \"SPI1-1task_prior_simgc%0.2f/2/model_ckpt_epoch_5.pt\" % gc_probs[4])\n",
    "\n",
    "# noprior_model_paths[5] = os.path.join(model_base_path, \"SPI1-1task_simgc%0.2f/2/model_ckpt_epoch_1.pt\" % gc_probs[5])\n",
    "# prior_model_paths[5] = os.path.join(model_base_path, \"SPI1-1task_prior_simgc%0.2f/2/model_ckpt_epoch_1.pt\" % gc_probs[5])\n",
    "\n",
    "# noprior_model_paths[6] = os.path.join(model_base_path, \"SPI1-1task_simgc%0.2f/2/model_ckpt_epoch_1.pt\" % gc_probs[6])\n",
    "# prior_model_paths[6] = os.path.join(model_base_path, \"SPI1-1task_prior_simgc%0.2f/2/model_ckpt_epoch_2.pt\" % gc_probs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "def restore_model(model_path):\n",
    "    model = model_util.restore_model(model_class, model_path)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model without priors\n",
    "noprior_models = [\n",
    "    restore_model(noprior_model_path) for noprior_model_path in noprior_model_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model with priors\n",
    "prior_models = [\n",
    "    restore_model(prior_model_path) for prior_model_path in prior_model_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Create an input data loader, that maps coordinates or bin indices to data needed for the model. We also create a loader for the GC content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20200420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_seq_generators = [\n",
    "    feature_util.StatusToSimulatedSeq(input_length, motif_path, 0, gc_prob)\n",
    "    for gc_prob in gc_probs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_freqs = [\n",
    "    np.array([1 - gc_prob, gc_prob, gc_prob, 1 - gc_prob]) / 2\n",
    "    for gc_prob in gc_probs\n",
    "]\n",
    "def pfm_to_pwm(pfm, background, pseudocount=0.001):\n",
    "    \"\"\"\n",
    "    Converts and L x 4 PFM into an L x 4 PWM.\n",
    "    \"\"\"\n",
    "    num_bases = pfm.shape[1]\n",
    "    # Incorporate pseudocount by adding it to every element and renormalizing\n",
    "    pfm_norm = (pfm + pseudocount) / (np.sum(pfm, axis=1, keepdims=True) + (num_bases * pseudocount))\n",
    "    return np.log2(pfm_norm / np.expand_dims(background, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_pfm = feature_util.import_homer_motif(motif_path)\n",
    "motif_pwms = [\n",
    "    pfm_to_pwm(motif_pfm, background) for background in background_freqs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_scores(model, input_seqs, batch_size=128):\n",
    "    \"\"\"\n",
    "    Given an array of N x I x 4 array of input sequences, computes the SHAP scores\n",
    "    for the model, returning an N x I x 4 array of SHAP scores.\n",
    "    \"\"\"\n",
    "    assert model_type == \"binary\", \"profile model types not supported here\"\n",
    "    num_samples = len(input_seqs)\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    \n",
    "    all_shap_scores = np.empty((num_samples, input_length, 4))\n",
    "        \n",
    "    shap_explainer = compute_shap.create_binary_explainer(\n",
    "        model, input_length, task_index=task_index\n",
    "    )\n",
    "\n",
    "    for i in tqdm.notebook.trange(num_batches):\n",
    "        batch_slice = slice(i * batch_size, (i + 1) * batch_size)\n",
    "        batch = input_seqs[batch_slice]\n",
    "\n",
    "        shap_scores = shap_explainer(\n",
    "            batch, hide_shap_output=True\n",
    "        )\n",
    "\n",
    "        all_shap_scores[batch_slice] = shap_scores\n",
    "    return all_shap_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "sample = np.arange(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the importance scores and 1-hot seqs\n",
    "imp_type = \"DeepSHAP scores\"\n",
    "imp_func = compute_shap_scores\n",
    "sample_input_seqs = [\n",
    "    sim_seq_generators[gc_index](np.ones(len(sample))) for gc_index in range(len(gc_probs))\n",
    "]\n",
    "noprior_imp_scores, prior_imp_scores = [], []\n",
    "for gc_index, noprior_model in enumerate(noprior_models):\n",
    "    noprior_imp_scores.append(imp_func(noprior_model, sample_input_seqs[gc_index]))\n",
    "for gc_index, prior_model in enumerate(prior_models):\n",
    "    prior_imp_scores.append(imp_func(prior_model, sample_input_seqs[gc_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_mask(one_hot_seqs, pwm, score_thresh=0.7):\n",
    "    rc_pwm = np.flip(pwm, axis=(0, 1))\n",
    "    mask = np.zeros(one_hot_seqs.shape[:2], dtype=bool)\n",
    "    for i, one_hot_seq in tqdm.notebook.tqdm(enumerate(one_hot_seqs), total=len(one_hot_seqs)):\n",
    "        for j in range(one_hot_seq.shape[0] - len(pwm) + 1):\n",
    "            match = np.sum(one_hot_seq[j : j + len(pwm)] * pwm) / len(pwm)\n",
    "            rc_match = np.sum(one_hot_seq[j : j + len(rc_pwm)] * rc_pwm) / len(rc_pwm)\n",
    "            if match >= score_thresh or rc_match >= score_thresh:\n",
    "                mask[i, j : j + len(pwm)] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_motif_gc(imp_scores, motif_mask):\n",
    "    keep_mask = ~motif_mask\n",
    "    gc_scores, at_scores, prod_scores = [], [], []\n",
    "    for i, score_track in enumerate(imp_scores):\n",
    "        gc_score = np.sum(score_track[keep_mask[i]][:, 1:3], axis=1) / np.max(score_track)\n",
    "        at_score = (score_track[keep_mask[i]][:, 0] + score_track[keep_mask[i]][:, 3]) / np.max(score_track)\n",
    "        gc_scores.append(np.nanmean(gc_score))\n",
    "        at_scores.append(np.nanmean(at_score))\n",
    "        prod_scores.append(np.nanmean(gc_score * at_score))\n",
    "    return np.array(gc_scores), np.array(at_scores), np.array(prod_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_importance_frac(imp_scores, input_seqs, motif_mask):\n",
    "    keep_mask = motif_mask\n",
    "    imp_fracs = []\n",
    "    for i, score_track in enumerate(imp_scores):\n",
    "        act_scores = np.abs(np.sum(score_track * input_seqs[i], axis=1))\n",
    "        imp_frac = np.sum(act_scores[keep_mask[i]]) / np.sum(act_scores)\n",
    "        imp_fracs.append(imp_frac)\n",
    "    return np.array(imp_fracs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [\n",
    "    get_motif_mask(sample_input_seqs[gc_index], motif_pwm, score_thresh=0.9)\n",
    "    for gc_index, motif_pwm in enumerate(motif_pwms)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprior_scores, prior_scores = [], []\n",
    "noprior_imp_fracs, prior_imp_fracs = [], []\n",
    "for gc_index in range(len(gc_probs)):\n",
    "    noprior_gc_scores, noprior_at_scores, noprior_prod_scores = get_non_motif_gc(\n",
    "        noprior_imp_scores[gc_index], masks[gc_index]\n",
    "    )\n",
    "    prior_gc_scores, prior_at_scores, prior_prod_scores = get_non_motif_gc(\n",
    "        prior_imp_scores[gc_index], masks[gc_index]\n",
    "    )\n",
    "    noprior_scores.append((noprior_gc_scores, noprior_at_scores, noprior_prod_scores))\n",
    "    prior_scores.append((prior_gc_scores, prior_at_scores, prior_prod_scores))\n",
    "    \n",
    "    noprior_imp_fracs.append(get_motif_importance_frac(\n",
    "        noprior_imp_scores[gc_index], sample_input_seqs[gc_index], masks[gc_index]\n",
    "    ))\n",
    "    prior_imp_fracs.append(get_motif_importance_frac(\n",
    "        prior_imp_scores[gc_index], sample_input_seqs[gc_index], masks[gc_index]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for gc_index in range(len(gc_probs)):\n",
    "    noprior_prod_scores, prior_prod_scores = noprior_scores[gc_index][2], prior_scores[gc_index][2]\n",
    "    bin_num = 50\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    title = \"Histogram of %s GC importance x AT importance outside motif instances\" % imp_type\n",
    "    title += \"\\nSingle-task SPI1 binary models, trained on %2.0f%% G/C bias\" % (gc_probs[gc_index] * 100)\n",
    "    title += \"\\nComputed on %d randomly simulated sequences\" % num_samples\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Signed importance of GC x importance of AT\")\n",
    "    all_vals = np.concatenate([noprior_prod_scores, prior_prod_scores])\n",
    "    bins = np.linspace(np.min(all_vals), np.max(all_vals), bin_num)\n",
    "    plt.hist(noprior_prod_scores, bins=bins, histtype=\"bar\", label=\"No prior\", color=\"coral\", alpha=0.7)\n",
    "    plt.hist(prior_prod_scores, bins=bins, histtype=\"bar\", label=\"With Fourier prior\", color=\"slateblue\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Average product without priors: %f\" % np.nanmean(noprior_prod_scores))\n",
    "    print(\"Average product with priors: %f\" % np.nanmean(prior_prod_scores))\n",
    "    w, p = scipy.stats.wilcoxon(noprior_prod_scores, prior_prod_scores, alternative=\"less\")\n",
    "    print(\"One-sided Wilcoxon test: W = %f, p = %f\" % (w, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of GC x AT importance product, on a shared x-axis\n",
    "bin_num = 40\n",
    "fig, ax = plt.subplots(1, len(gc_probs), figsize=(9 * len(gc_probs), 15), sharey=True)\n",
    "if len(gc_probs) == 1:\n",
    "    ax = [ax]\n",
    "title = \"Histogram of %s GC importance x AT importance outside motif instances\" % imp_type\n",
    "title += \"\\nSingle-task SPI1 binary models\"\n",
    "title += \"\\nComputed on %d randomly simulated sequences\" % num_samples\n",
    "plt.suptitle(title)\n",
    "fig.text(0.5, 0.05, \"Signed importance of GC x importance of AT\", ha=\"center\", fontsize=22)\n",
    "all_vals = np.ravel([\n",
    "    [noprior_scores[gc_index][2], prior_scores[gc_index][2]] for gc_index in range(len(gc_probs))\n",
    "])\n",
    "bins = np.linspace(np.min(all_vals), np.max(all_vals), bin_num)\n",
    "\n",
    "for gc_index in range(len(gc_probs)):\n",
    "    ax[gc_index].hist(noprior_scores[gc_index][2], bins=bins, histtype=\"bar\", label=\"No prior\", color=\"coral\", alpha=0.7)\n",
    "    ax[gc_index].hist(prior_scores[gc_index][2], bins=bins, histtype=\"bar\", label=\"With Fourier prior\", color=\"slateblue\", alpha=0.7)\n",
    "    ax[gc_index].set_title(\"%2.0f%% G/C bias\" % (gc_probs[gc_index] * 100))\n",
    "\n",
    "plt.subplots_adjust(top=0.85)\n",
    "ax[0].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for gc_index in range(len(gc_probs)):\n",
    "    noprior_prod_scores, prior_prod_scores = noprior_scores[gc_index][2], prior_scores[gc_index][2]\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.scatter(noprior_prod_scores, prior_prod_scores, color=\"mediumorchid\", alpha=0.5)\n",
    "    title = \"Pairwise comparison of %s GC importance x AT importance outside motif instances\" % imp_type\n",
    "    title += \"\\nSingle-task SPI1 binary models, trained on %2.0f%% G/C bias\" % (gc_probs[gc_index] * 100)\n",
    "    title += \"\\nComputed on %d randomly simulated sequences\" % num_samples\n",
    "    plt.title(title)\n",
    "    limits = [\n",
    "        np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "        np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "    ]\n",
    "    ax.plot(limits, limits, \"--\", alpha=0.5, color=\"black\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlim(limits)\n",
    "    ax.set_ylim(limits)\n",
    "    plt.xlabel(\"Importance of GC x AT without prior\")\n",
    "    plt.ylabel(\"Importance of GC x AT with Fourier prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for gc_index in range(len(gc_probs)):\n",
    "    noprior_gc_scores, noprior_at_scores = noprior_scores[gc_index][:2]\n",
    "    prior_gc_scores, prior_at_scores = prior_scores[gc_index][:2]\n",
    "    bin_num = 30\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    title = \"%s GC importance x AT importance outside motif instances\" % imp_type\n",
    "    title += \"\\nSingle-task SPI1 binary models, trained on %2.0f%% G/C bias\" % (gc_probs[gc_index] * 100)\n",
    "    title += \"\\nComputed on %d randomly simulated sequences\" % num_samples\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Signed importance of GC\")\n",
    "    plt.ylabel(\"Signed importance of AT\")\n",
    "    plt.scatter(noprior_gc_scores, noprior_at_scores, color=\"coral\", alpha=0.7, label=\"No prior\")\n",
    "    plt.scatter(prior_gc_scores, prior_at_scores, color=\"slateblue\", alpha=0.7, label=\"With Fourier prior\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for gc_index in range(len(gc_probs)):\n",
    "    noprior_frac, prior_fracs = noprior_imp_fracs[gc_index], prior_imp_fracs[gc_index]\n",
    "    bin_num = 30\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    title = \"Proportion of %s importance in motif instances\" % imp_type\n",
    "    title += \"\\nSingle-task SPI1 binary models, trained on %2.0f%% G/C bias\" % (gc_probs[gc_index] * 100)\n",
    "    title += \"\\nComputed on %d randomly simulated sequences\" % num_samples\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Proportion of importance in motif instances\")\n",
    "    all_vals = np.concatenate([noprior_frac, prior_fracs])\n",
    "    bins = np.linspace(np.min(all_vals), np.max(all_vals), bin_num)\n",
    "    plt.hist(noprior_frac, bins=bins, histtype=\"bar\", label=\"No prior\", color=\"coral\", alpha=0.7)\n",
    "    plt.hist(prior_fracs, bins=bins, histtype=\"bar\", label=\"With Fourier prior\", color=\"slateblue\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Average product without priors: %f\" % np.nanmean(noprior_frac))\n",
    "    print(\"Average product with priors: %f\" % np.nanmean(prior_fracs))\n",
    "    w, p = scipy.stats.wilcoxon(noprior_frac, prior_fracs, alternative=\"less\")\n",
    "    print(\"One-sided Wilcoxon test: W = %f, p = %f\" % (w, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(gc_index, i, center_slice=slice(450, 550)):\n",
    "    print(gc_probs[gc_index], i)\n",
    "    print(\"=========================\")\n",
    "    print(\"Without priors:\")\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(np.sum(noprior_imp_scores[gc_index][i] * sample_input_seqs[gc_index][i], axis=1), color=\"coral\")\n",
    "    plt.show()\n",
    "    viz_sequence.plot_weights((noprior_imp_scores[gc_index][i])[center_slice], subticks_frequency=1000)\n",
    "    viz_sequence.plot_weights((noprior_imp_scores[gc_index][i] * sample_input_seqs[gc_index][i])[center_slice], subticks_frequency=1000)\n",
    "    print(\"With priors:\")\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(np.sum(prior_imp_scores[gc_index][i] * sample_input_seqs[gc_index][i], axis=1), color=\"slateblue\")\n",
    "    plt.show()\n",
    "    viz_sequence.plot_weights((prior_imp_scores[gc_index][i])[center_slice], subticks_frequency=1000)\n",
    "    viz_sequence.plot_weights((prior_imp_scores[gc_index][i] * sample_input_seqs[gc_index][i])[center_slice], subticks_frequency=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot out a few examples\n",
    "for gc_index in range(len(gc_probs)):\n",
    "    for i in np.random.choice(num_samples, size=3, replace=False):\n",
    "        show_example(gc_index, i, center_slice=slice(400, 600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
