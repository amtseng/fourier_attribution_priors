{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src/\"))\n",
    "import extract.data_loading as data_loading\n",
    "import extract.compute_predictions as compute_predictions\n",
    "import extract.compute_shap as compute_shap\n",
    "import extract.compute_ism as compute_ism\n",
    "import model.util as model_util\n",
    "import model.profile_models as profile_models\n",
    "import model.binary_models as binary_models\n",
    "import plot.viz_sequence as viz_sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import json\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()  # It is necessary to call this before the tqdm.notebook submodule is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for the model and data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared paths/constants\n",
    "reference_fasta = \"/users/amtseng/genomes/hg38.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/hg38.canon.chrom.sizes\"\n",
    "data_base_path = \"/users/amtseng/att_priors/data/processed/\"\n",
    "model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s/\" % model_type\n",
    "chrom_set = [\"chr1\"]\n",
    "input_length = 1346 if model_type == \"profile\" else 1000\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPI1\n",
    "condition_name = \"SPI1\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_TFChIP/%s/config/SPI1/SPI1_training_paths.json\" % model_type)\n",
    "num_tasks = 4\n",
    "num_strands = 2\n",
    "task_index = None\n",
    "controls = \"matched\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithMatchedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_all_model_base_path = os.path.join(model_base_path, \"SPI1/\")\n",
    "noprior_less_model_base_path = os.path.join(model_base_path, \"SPI1_keep1/\")\n",
    "prior_all_model_base_path = os.path.join(model_base_path, \"SPI1_prior/\")\n",
    "prior_less_model_base_path = os.path.join(model_base_path, \"SPI1_prior_keep1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GATA2\n",
    "condition_name = \"GATA2\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_TFChIP/%s/config/GATA2/GATA2_training_paths.json\" % model_type)\n",
    "num_tasks = 3\n",
    "num_strands = 2\n",
    "task_index = None\n",
    "controls = \"matched\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithMatchedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_all_model_base_path = os.path.join(model_base_path, \"GATA2/\")\n",
    "noprior_less_model_base_path = os.path.join(model_base_path, \"GATA2_keep1/\")\n",
    "prior_all_model_base_path = os.path.join(model_base_path, \"GATA2_prior/\")\n",
    "prior_less_model_base_path = os.path.join(model_base_path, \"GATA2_prior_keep1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K562\n",
    "condition_name = \"K562\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_DNase/%s/config/K562/K562_training_paths.json\" % model_type)\n",
    "num_tasks = 1\n",
    "num_strands = 1\n",
    "task_index = None\n",
    "controls = \"shared\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithSharedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_all_model_base_path = os.path.join(model_base_path, \"K562/\")\n",
    "noprior_less_model_base_path = os.path.join(model_base_path, \"K562_keep1/\")\n",
    "prior_all_model_base_path = os.path.join(model_base_path, \"K562_prior/\")\n",
    "prior_less_model_base_path = os.path.join(model_base_path, \"K562_prior_keep1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPNet\n",
    "condition_name = \"BPNet\"\n",
    "reference_fasta = \"/users/amtseng/genomes/mm10.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/mm10.canon.chrom.sizes\"\n",
    "files_spec_path = os.path.join(data_base_path, \"BPNet_ChIPseq/%s/config/BPNet_training_paths.json\" % model_type)\n",
    "num_tasks = 3\n",
    "num_strands = 2\n",
    "task_index = None\n",
    "controls = \"shared\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithSharedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "noprior_all_model_base_path = os.path.join(model_base_path, \"BPNet/\")\n",
    "noprior_less_model_base_path = os.path.join(model_base_path, \"BPNet_keep1/\")\n",
    "prior_all_model_base_path = os.path.join(model_base_path, \"BPNet_prior/\")\n",
    "prior_less_model_base_path = os.path.join(model_base_path, \"BPNet_prior_keep1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all runs/epochs with random initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(model_base_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {model_base_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(model_base_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_paths(\n",
    "    model_base_path, metric_name=\"val_prof_corr_losses\",\n",
    "    reduce_func=(lambda values: np.mean(values)), compare_func=(lambda x, y: x < y),\n",
    "    print_found_values=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Looks in `model_base_path` and for each run, returns the full path to\n",
    "    the best epoch. By default, the best epoch in a run is determined by\n",
    "    the lowest validation profile loss.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(model_base_path, run_num) for run_num in os.listdir(model_base_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    model_paths, metric_vals = [], []\n",
    "    for run_num in sorted(metrics.keys(), key=lambda x: int(x)):\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            model_path = os.path.join(model_base_path, run_num, \"model_ckpt_epoch_%d.pt\" % best_epoch_in_run)\n",
    "            model_paths.append(model_path)\n",
    "            metric_vals.append(best_val_in_run)\n",
    "            if print_found_values:\n",
    "                print(\"\\tRun %s, epoch %d: %6.2f\" % (run_num, best_epoch_in_run, best_val_in_run))\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return np.array(model_paths), np.array(metric_vals)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"val_prof_corr_losses\" if model_type == \"profile\" else \"val_corr_losses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noprior_all_model_paths, noprior_all_metric_vals = get_model_paths(noprior_all_model_base_path, metric_name=metric_name)\n",
    "noprior_less_model_paths, noprior_less_metric_vals = get_model_paths(noprior_less_model_base_path, metric_name=metric_name)\n",
    "prior_all_model_paths, prior_all_metric_vals = get_model_paths(prior_all_model_base_path, metric_name=metric_name)\n",
    "prior_less_model_paths, prior_less_metric_vals = get_model_paths(prior_less_model_base_path, metric_name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 5  # Maximum of 5 models each\n",
    "noprior_all_model_paths = noprior_all_model_paths[np.argsort(noprior_all_metric_vals)[:num_models]]\n",
    "noprior_less_model_paths = noprior_less_model_paths[np.argsort(noprior_less_metric_vals)[:num_models]]\n",
    "prior_all_model_paths = prior_all_model_paths[np.argsort(prior_all_metric_vals)[:num_models]]\n",
    "prior_less_model_paths = prior_less_model_paths[np.argsort(prior_less_metric_vals)[:num_models]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(model_path):\n",
    "    model = model_util.restore_model(model_class, model_path)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Create an input data loader, that maps coordinates or bin indices to data needed for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"profile\":\n",
    "    input_func = data_loading.get_profile_input_func(\n",
    "        files_spec_path, input_length, profile_length, reference_fasta\n",
    "    )\n",
    "    pos_examples = data_loading.get_positive_profile_coords(\n",
    "        files_spec_path, chrom_set=chrom_set\n",
    "    )\n",
    "else:\n",
    "    input_func = data_loading.get_binary_input_func(\n",
    "        files_spec_path, input_length, reference_fasta\n",
    "    )\n",
    "    pos_examples = data_loading.get_positive_binary_bins(\n",
    "        files_spec_path, chrom_set=chrom_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample of 100 random coordinates/bins\n",
    "num_samples = 100\n",
    "rng = np.random.RandomState(20200318)\n",
    "sample = pos_examples[rng.choice(len(pos_examples), size=num_samples, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For profile models, add a random jitter to avoid center-bias\n",
    "if model_type == \"profile\":\n",
    "    jitters = np.random.randint(-128, 128 + 1, size=len(sample))\n",
    "    sample[:, 1] = sample[:, 1] + jitters\n",
    "    sample[:, 2] = sample[:, 2] + jitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model_paths, sample):\n",
    "    \"\"\"\n",
    "    Given a list of paths to M models and a list of N coordinates or bins, computes\n",
    "    the input gradients over all models, returning an M x N x I x 4 array of\n",
    "    gradient values and an N x I x 4 array of one-hot encoded sequence.\n",
    "    \"\"\"\n",
    "    num_models, num_samples = len(model_paths), len(sample)\n",
    "    \n",
    "    all_input_grads = np.empty((num_models, num_samples, input_length, 4))\n",
    "    all_one_hot_seqs = np.empty((num_samples, input_length, 4))\n",
    "    \n",
    "    for i in tqdm.notebook.trange(num_models):\n",
    "        model = restore_model(model_paths[i])\n",
    "            \n",
    "        if model_type == \"profile\":\n",
    "            results = compute_predictions.get_profile_model_predictions(                                              \n",
    "                model, sample, num_tasks, input_func, controls=controls,                        \n",
    "                return_losses=False, return_gradients=True, show_progress=False                                         \n",
    "            )\n",
    "        else:\n",
    "            results = compute_predictions.get_binary_model_predictions(                                              \n",
    "                model, sample, input_func,                      \n",
    "                return_losses=False, return_gradients=True, show_progress=False                                         \n",
    "            )\n",
    "\n",
    "        all_input_grads[i] = results[\"input_grads\"]\n",
    "        if i == 0:\n",
    "            all_one_hot_seqs = results[\"input_seqs\"]\n",
    "    return all_input_grads, all_one_hot_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_scores(model_paths, sample, batch_size=128):\n",
    "    \"\"\"\n",
    "    Given a list of paths to M models and a list of N coordinates or bins, computes\n",
    "    the SHAP scores over all models, returning an M x N x I x 4 array of\n",
    "    SHAP scores and an N x I x 4 array of one-hot encoded sequence.\n",
    "    \"\"\"\n",
    "    num_models, num_samples = len(model_paths), len(sample)\n",
    "    \n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    \n",
    "    all_shap_scores = np.empty((num_models, num_samples, input_length, 4))\n",
    "    all_one_hot_seqs = np.empty((num_samples, input_length, 4))\n",
    "    \n",
    "    for i in tqdm.notebook.trange(num_models):\n",
    "        model = restore_model(model_paths[i])\n",
    "        \n",
    "        if model_type == \"profile\":\n",
    "            shap_explainer = compute_shap.create_profile_explainer(\n",
    "                model, input_length, profile_length, num_tasks, num_strands, controls,\n",
    "                task_index=task_index\n",
    "            )\n",
    "        else:\n",
    "            shap_explainer = compute_shap.create_binary_explainer(\n",
    "                model, input_length, task_index=task_index\n",
    "            )\n",
    "\n",
    "        for j in range(num_batches):\n",
    "            batch_slice = slice(j * batch_size, (j + 1) * batch_size)\n",
    "            batch = sample[batch_slice]\n",
    "            \n",
    "            if model_type == \"profile\":\n",
    "                input_seqs, profiles = input_func(sample)\n",
    "                shap_scores = shap_explainer(\n",
    "                    input_seqs, cont_profs=profiles[:, num_tasks:], hide_shap_output=True\n",
    "                )\n",
    "            else:\n",
    "                input_seqs, _, _ = input_func(sample)\n",
    "                shap_scores = shap_explainer(\n",
    "                    input_seqs, hide_shap_output=True\n",
    "                )\n",
    "            \n",
    "            all_shap_scores[i, batch_slice] = shap_scores\n",
    "            if i == 0:\n",
    "                all_one_hot_seqs[batch_slice] = input_seqs\n",
    "    return all_shap_scores, all_one_hot_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the importance scores and 1-hot seqs\n",
    "imp_type = (\"DeepSHAP scores\", \"input gradients\")[0]\n",
    "imp_func = compute_shap_scores if imp_type == \"DeepSHAP scores\" else compute_gradients\n",
    "noprior_all_scores, one_hot_seqs = imp_func(noprior_all_model_paths, sample)\n",
    "noprior_less_scores, _ = imp_func(noprior_less_model_paths, sample)\n",
    "prior_all_scores, _ = imp_func(prior_all_model_paths, sample)\n",
    "prior_less_scores, _ = imp_func(prior_less_model_paths, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_jaccard(seq_1, seq_2):\n",
    "    \"\"\"\n",
    "    Takes two gradient sequences (I x 4 arrays) and computes a similarity between\n",
    "    them, using a continuous Jaccard metric.\n",
    "    \"\"\"\n",
    "    # L1-normalize\n",
    "    norm_1 = np.sum(np.abs(seq_1), axis=1, keepdims=True)\n",
    "    norm_2 = np.sum(np.abs(seq_2), axis=1, keepdims=True)\n",
    "    norm_1[norm_1 == 0] = 1\n",
    "    norm_2[norm_2 == 0] = 1\n",
    "    seq_1 = seq_1 / norm_1\n",
    "    seq_2 = seq_2 / norm_2\n",
    "    \n",
    "    ab_1, ab_2 = np.abs(seq_1), np.abs(seq_2)\n",
    "    inter = np.sum(np.minimum(ab_1, ab_2) * np.sign(seq_1) * np.sign(seq_2), axis=1)\n",
    "    union = np.sum(np.maximum(ab_1, ab_2), axis=1)\n",
    "    zero_mask = union == 0\n",
    "    inter[zero_mask] = 0\n",
    "    union[zero_mask] = 1\n",
    "    return np.sum(inter / union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(seq_1, seq_2):\n",
    "    \"\"\"\n",
    "    Takes two gradient sequences (I x 4 arrays) and computes a similarity between\n",
    "    them, using a cosine similarity.\n",
    "    \"\"\"\n",
    "    seq_1, seq_2 = np.ravel(seq_1), np.ravel(seq_2)\n",
    "    dot = np.sum(seq_1 * seq_2)\n",
    "    mag_1, mag_2 = np.sqrt(np.sum(seq_1 * seq_1)), np.sqrt(np.sum(seq_2 * seq_2))\n",
    "    return dot / (mag_1 * mag_2) if mag_1 * mag_2 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(imp_scores_1, imp_scores_2, sim_func=cosine_sim):\n",
    "    \"\"\"\n",
    "    Given the M1 x N x I x 4 and M2 x N x I x 4 importance scores returned\n",
    "    by `compute_gradients` or `compute_shap_scores`, computes an N x M1 x M2\n",
    "    similarity matrix of similarity across models (i.e. each coordinate gets\n",
    "    a similarity matrix between the two conditions). By default uses cosine\n",
    "    similarity.\n",
    "    \"\"\"\n",
    "    num_models_1, num_coords = imp_scores_1.shape[0], imp_scores_2.shape[1]\n",
    "    num_models_2, num_coords_2 = imp_scores_2.shape[0], imp_scores_2.shape[1]\n",
    "    assert num_coords == num_coords_2\n",
    "    sim_mats = np.empty((num_coords, num_models_1, num_models_2))\n",
    "    for i in tqdm.notebook.trange(num_coords):\n",
    "        for j in range(num_models_1):\n",
    "            for k in range(num_models_2):\n",
    "                sim_score = sim_func(imp_scores_1[j][i], imp_scores_2[k][i])\n",
    "                sim_mats[i, j, k] = sim_score\n",
    "    return sim_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_type = (\"Cosine\", \"Continuous Jaccard\")[1]\n",
    "sim_func = cosine_sim if sim_type == \"Cosine\" else cont_jaccard\n",
    "noprior_sim_matrix = compute_similarity_matrix(noprior_all_scores, noprior_less_scores, sim_func=sim_func)\n",
    "prior_sim_matrix = compute_similarity_matrix(prior_all_scores, prior_less_scores, sim_func=sim_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot some examples of poor consistency, particularly ones that showed an improvement\n",
    "num_to_show = 0  # 15\n",
    "center_view_length = 200\n",
    "plot_zoom = False\n",
    "midpoint = input_length // 2\n",
    "start = midpoint - (center_view_length // 2)\n",
    "end = start + center_view_length\n",
    "center_slice = slice(start, end)\n",
    "\n",
    "diffs = np.max(prior_sim_matrix, axis=(1, 2)) - np.min(noprior_sim_matrix, axis=(1, 2))\n",
    "best_example_inds = np.flip(np.argsort(diffs))[:num_to_show]\n",
    "\n",
    "for sample_index in best_example_inds:\n",
    "    noprior_model_ind_1, noprior_model_ind_2 = np.unravel_index(np.argmin(np.ravel(noprior_sim_matrix[sample_index])), noprior_sim_matrix[sample_index].shape)\n",
    "    prior_model_ind_1, prior_model_ind_2 = np.unravel_index(np.argmax(np.ravel(prior_sim_matrix[sample_index])), prior_sim_matrix[sample_index].shape)\n",
    "    print(\"Sample index: %d\" % sample_index)\n",
    "    if model_type == \"binary\":\n",
    "        bin_index = sample[sample_index]\n",
    "        coord = input_func(np.array([bin_index]))[2][0]\n",
    "        print(\"Coordinate: %s (bin %d)\" % (str(coord), bin_index))\n",
    "    else:\n",
    "        coord = sample[sample_index]\n",
    "        print(\"Coordinate: %s\" % str(coord))\n",
    "    print(\"Model indices without prior: %d vs %d\" % (noprior_model_ind_1, noprior_model_ind_2))\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(np.sum(noprior_all_scores[noprior_model_ind_1, sample_index] * one_hot_seqs[sample_index], axis=1), color=\"coral\")\n",
    "    plt.show()\n",
    "    if plot_zoom:\n",
    "        viz_sequence.plot_weights(noprior_all_scores[noprior_model_ind_1, sample_index, center_slice], subticks_frequency=1000)\n",
    "        viz_sequence.plot_weights(noprior_all_scores[noprior_model_ind_1, sample_index, center_slice] * one_hot_seqs[sample_index, center_slice], subticks_frequency=1000)\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(np.sum(noprior_less_scores[noprior_model_ind_2, sample_index] * one_hot_seqs[sample_index], axis=1), color=\"coral\")\n",
    "    plt.show()\n",
    "    if plot_zoom:\n",
    "        viz_sequence.plot_weights(noprior_less_scores[noprior_model_ind_2, sample_index, center_slice], subticks_frequency=1000)\n",
    "        viz_sequence.plot_weights(noprior_less_scores[noprior_model_ind_2, sample_index, center_slice] * one_hot_seqs[sample_index, center_slice], subticks_frequency=1000)\n",
    "    \n",
    "    print(\"Model indices with prior: %d vs %d\" % (prior_model_ind_1, prior_model_ind_2))\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(np.sum(prior_all_scores[prior_model_ind_1, sample_index] * one_hot_seqs[sample_index], axis=1), color=\"slateblue\")\n",
    "    plt.show()\n",
    "    if plot_zoom:\n",
    "        viz_sequence.plot_weights(prior_all_scores[prior_model_ind_1, sample_index, center_slice], subticks_frequency=1000)\n",
    "        viz_sequence.plot_weights(prior_all_scores[prior_model_ind_1, sample_index, center_slice] * one_hot_seqs[sample_index, center_slice], subticks_frequency=1000)\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(np.sum(prior_less_scores[prior_model_ind_2, sample_index] * one_hot_seqs[sample_index], axis=1), color=\"slateblue\")\n",
    "    plt.show()\n",
    "    if plot_zoom:\n",
    "        viz_sequence.plot_weights(prior_less_scores[prior_model_ind_2, sample_index, center_slice], subticks_frequency=1000)\n",
    "        viz_sequence.plot_weights(prior_less_scores[prior_model_ind_2, sample_index, center_slice] * one_hot_seqs[sample_index, center_slice], subticks_frequency=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprior_avg_sims = np.mean(noprior_sim_matrix, axis=(1, 2))\n",
    "prior_avg_sims = np.mean(prior_sim_matrix, axis=(1, 2))\n",
    "bin_num = 30\n",
    "all_vals = np.concatenate([noprior_avg_sims, prior_avg_sims])\n",
    "bins = np.linspace(np.min(all_vals), np.max(all_vals), bin_num)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.hist(noprior_avg_sims, bins=bins, color=\"coral\", label=\"No prior\", alpha=0.7)\n",
    "ax.hist(prior_avg_sims, bins=bins, color=\"slateblue\", label=\"With Fourier prior\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    (\"Mean pairwise similarities of %s between training on all vs 1%% of the peaks\" % imp_type) +\n",
    "    (\"\\n%s %s models\" % (condition_name, model_type)) +\n",
    "    (\"\\nComputed over top 5 models without/with Fourier prior on %d randomly drawn test peaks\" % num_samples)\n",
    ")\n",
    "plt.xlabel(\"%s similarity\" % sim_type)\n",
    "\n",
    "print(\"Average similarity without priors: %f\" % np.nanmean(noprior_avg_sims))\n",
    "print(\"Average similarity with priors: %f\" % np.nanmean(prior_avg_sims))\n",
    "print(\"Standard error without priors: %f\" % scipy.stats.sem(noprior_avg_sims, nan_policy=\"omit\"))\n",
    "print(\"Standard error with priors: %f\" % scipy.stats.sem(prior_avg_sims, nan_policy=\"omit\"))\n",
    "w, p = scipy.stats.wilcoxon(noprior_avg_sims, prior_avg_sims, alternative=\"less\")\n",
    "print(\"One-sided Wilcoxon test: w = %f, p = %f\" % (w, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim_diffs = prior_avg_sims - noprior_avg_sims\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.hist(avg_sim_diffs, bins=30, color=\"mediumorchid\")\n",
    "plt.title(\n",
    "    (\"Paired difference of %s similarity between training on all vs 1%% of the peaks\" % imp_type) +\n",
    "    (\"\\n%s %s models\" % (condition_name, model_type)) +\n",
    "    (\"\\nComputed over top 5 models without/with Fourier prior on %d randomly drawn test peaks\" % num_samples)\n",
    ")\n",
    "plt.xlabel(\"Average similarity difference: with Fourier prior - no prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(sim_matrix):\n",
    "    num_examples, num_models, _ = sim_matrix.shape\n",
    "    bias_vals = []\n",
    "    for i in range(num_models):\n",
    "        avg = np.sum(sim_matrix[:, i]) / (num_examples * (num_models - 1))\n",
    "        bias_vals.append(avg)\n",
    "        print(\"%d: %f\" % (i + 1, avg))\n",
    "    return bias_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Model-specific bias without priors\")\n",
    "noprior_bias_vals = get_bias(noprior_sim_matrix)\n",
    "print(\"Model-specific bias with priors\")\n",
    "prior_bias_vals = get_bias(prior_sim_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
