{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_extract_func, metric_compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    where the metric value is fetched by `metric_extract_func`. This function must\n",
    "    take the imported metrics JSON and return the (scalar) value to use for\n",
    "    comparison. The best metric value is determiend by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a list of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, []\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = metric_extract_func(metrics[run_num])\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to extract metric for run %s\" % run_num)\n",
    "            continue\n",
    "        all_vals.append(val)\n",
    "        if best_val is None or metric_compare_func(val, best_val):\n",
    "            best_val, best_run = val, run_num\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Was not able to extract metric for run 15\n",
      "Warning: Was not able to extract metric for run 42\n",
      "Warning: Was not able to extract metric for run 8\n",
      "Best run: 2\n",
      "Associated value: 171.80329483709107\n"
     ]
    }
   ],
   "source": [
    "models_path = \"/users/amtseng/att_priors/models/trained_profile_models/SPI1/\"\n",
    "best_run, best_val, all_vals = get_best_metric(\n",
    "    models_path,\n",
    "    lambda metrics: metrics[\"summit_prof_nll\"][\"values\"][0][0],  # First task, arbitrarily\n",
    "    lambda x, y: x < y\n",
    ")\n",
    "print(\"Best run: %s\" % best_run)\n",
    "print(\"Associated value: %s\" % best_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
