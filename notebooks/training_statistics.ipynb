{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the (one-indexed) number of the epch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_validation_profile_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "Warning: Was not able to compute values for run 8\n",
      "\tBest run: 4\n",
      "\tBest epoch in run: 10\n",
      "\tAssociated value: 179.99440738677978\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 10: 180.97\n",
      "\tRun 2, epoch 8: 180.73\n",
      "\tRun 3, epoch 7: 180.46\n",
      "\tRun 4, epoch 10: 179.99\n",
      "\tRun 5, epoch 10: 180.05\n",
      "\tRun 6, epoch 10: 180.81\n",
      "\tRun 7, epoch 8: 181.37\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t200.36 194.65 193.33 192.59 192.15 191.83 191.66 191.52 191.40 191.25\n",
      "\t184.69 182.63 181.86 181.45 181.32 181.37 181.15 181.13 181.09 180.97\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t199.41 194.43 193.20 192.45 192.04 191.73 191.50 191.35 191.22 191.14\n",
      "\t184.60 182.48 181.70 181.30 180.91 180.93 180.88 180.73 180.80 180.90\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t198.70 193.95 192.49 191.83 191.42 191.19 191.02 190.83 190.75 190.59\n",
      "\t183.93 181.95 181.27 180.81 180.70 180.67 180.46 180.62 180.62 180.83\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t199.55 194.49 193.11 192.40 192.02 191.76 191.54 191.44 191.29 191.18\n",
      "\t183.14 181.38 180.75 180.37 180.14 180.17 180.15 180.13 180.12 179.99\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t199.47 194.07 192.71 192.03 191.62 191.33 191.18 190.99 190.88 190.73\n",
      "\t183.18 181.80 180.95 180.63 180.28 180.08 180.09 180.15 180.07 180.05\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t199.59 194.77 193.26 192.50 192.12 191.79 191.54 191.40 191.29 191.13\n",
      "\t185.22 182.60 181.85 181.65 181.38 181.22 180.95 180.89 180.83 180.81\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t199.76 194.27 193.10 192.46 192.05 191.71 191.50 191.36 191.27 191.15\n",
      "\t185.97 183.34 182.55 182.00 181.74 181.56 181.67 181.37 181.43 181.50\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_profile_and_prior_losses(\"HepG2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 3\n",
      "\tBest epoch in run: 7\n",
      "\tAssociated value: 181.0568891398112\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 9: 181.77\n",
      "\tRun 2, epoch 9: 181.86\n",
      "\tRun 3, epoch 7: 181.06\n",
      "\tRun 4, epoch 10: 181.13\n",
      "\tRun 5, epoch 9: 181.66\n",
      "\tRun 6, epoch 6: 182.12\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t199.37 194.95 194.05 193.47 193.14 192.86 192.67 192.56 192.45 192.38\n",
      "\t184.64 183.46 182.80 182.50 182.20 182.05 182.10 182.17 181.77 182.11\n",
      "\t0.1124 0.0946 0.0776 0.0777 0.0766 0.0789 0.0812 0.0818 0.0797 0.0826\n",
      "2\n",
      "\t199.63 195.21 194.34 193.89 193.69 193.50 193.34 193.15 193.09 192.99\n",
      "\t184.50 183.09 183.09 182.21 182.38 182.05 182.14 182.13 181.86 181.90\n",
      "\t0.1013 0.0823 0.0744 0.0758 0.0762 0.0742 0.0755 0.0810 0.0776 0.0781\n",
      "3\n",
      "\t199.85 195.26 194.04 193.39 193.00 192.85 192.69 192.56 192.47 192.40\n",
      "\t184.08 182.69 182.14 181.53 181.53 181.49 181.06 181.22 181.28 181.22\n",
      "\t0.0980 0.0929 0.0930 0.0843 0.0784 0.0846 0.0813 0.0852 0.0845 0.0822\n",
      "4\n",
      "\t199.82 195.20 194.24 193.80 193.51 193.30 193.10 192.91 192.76 192.65\n",
      "\t183.96 182.62 182.11 181.85 181.70 181.50 181.43 181.22 181.40 181.13\n",
      "\t0.1094 0.0794 0.0788 0.0754 0.0791 0.0749 0.0783 0.0785 0.0795 0.0749\n",
      "5\n",
      "\t199.23 195.12 193.82 193.27 192.83 192.68 192.59 192.59 192.52 192.41\n",
      "\t184.73 183.14 182.46 181.99 182.13 181.96 181.82 182.02 181.66 181.94\n",
      "\t0.0954 0.0972 0.0842 0.0771 0.0815 0.0758 0.0910 0.0878 0.0840 0.0783\n",
      "6\n",
      "\t200.37 195.37 194.50 193.93 193.57 193.32\n",
      "\t184.55 183.42 182.78 182.41 182.35 182.12\n",
      "\t0.0857 0.0778 0.0786 0.0783 0.0748 0.0782\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_profile_and_prior_losses(\"HepG2_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/binary_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.3f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"train_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 6\n",
      "\tBest epoch in run: 4\n",
      "\tAssociated value: 0.36000752200682956\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 5:  0.393\n",
      "\tRun 2, epoch 4:  0.371\n",
      "\tRun 3, epoch 2:  0.393\n",
      "\tRun 4, epoch 3:  0.373\n",
      "\tRun 5, epoch 3:  0.405\n",
      "\tRun 6, epoch 4:  0.360\n",
      "\tRun 7, epoch 4:  0.363\n",
      "\tRun 8, epoch 3:  0.364\n",
      "\tRun 9, epoch 4:  0.366\n",
      "\tRun 10, epoch 6:  0.403\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.501  0.386  0.337  0.307  0.266  0.228  0.192  0.172  0.144  0.130\n",
      "\t 0.439  0.400  0.400  0.396  0.393  0.426  0.463  0.509  0.506  0.530\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "2\n",
      "\t 0.481  0.390  0.341  0.304  0.260  0.225  0.196  0.172  0.152  0.137\n",
      "\t 0.423  0.383  0.389  0.371  0.393  0.417  0.452  0.459  0.449  0.516\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "3\n",
      "\t 0.483  0.381  0.338  0.290  0.249  0.210  0.190  0.163  0.142  0.125\n",
      "\t 0.448  0.393  0.409  0.402  0.424  0.447  0.478  0.496  0.501  0.542\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "4\n",
      "\t 0.500  0.397  0.339  0.297  0.260  0.225  0.199  0.171  0.150  0.140\n",
      "\t 0.458  0.416  0.373  0.376  0.408  0.397  0.430  0.458  0.475  0.505\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "5\n",
      "\t 0.496  0.390  0.341  0.300  0.267  0.224  0.193  0.167  0.147  0.129\n",
      "\t 0.479  0.457  0.405  0.408  0.412  0.465  0.470  0.483  0.493  0.527\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "6\n",
      "\t 0.482  0.389  0.329  0.291  0.248  0.211  0.179  0.154  0.139  0.125\n",
      "\t 0.440  0.384  0.382  0.360  0.374  0.404  0.427  0.452  0.466  0.509\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "7\n",
      "\t 0.484  0.384  0.335  0.298  0.257  0.228  0.190  0.173  0.150  0.134\n",
      "\t 0.438  0.403  0.384  0.363  0.394  0.407  0.427  0.441  0.481  0.522\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "8\n",
      "\t 0.467  0.379  0.337  0.296  0.260  0.222  0.188  0.167  0.144  0.134\n",
      "\t 0.420  0.376  0.364  0.364  0.401  0.426  0.427  0.472  0.477  0.517\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "9\n",
      "\t 0.489  0.384  0.342  0.296  0.257  0.218  0.184  0.172  0.150  0.130\n",
      "\t 0.435  0.403  0.376  0.366  0.386  0.395  0.426  0.459  0.484  0.492\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "10\n",
      "\t 0.505  0.406  0.352  0.320  0.282  0.242  0.203  0.178  0.160  0.141\n",
      "\t 0.469  0.420  0.422  0.408  0.421  0.403  0.439  0.462  0.487  0.494\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_and_prior_losses(\"HepG2_keep1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 9\n",
      "\tBest epoch in run: 8\n",
      "\tAssociated value: 0.35352155566215515\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 10:  0.371\n",
      "\tRun 2, epoch 10:  0.368\n",
      "\tRun 3, epoch 6:  0.384\n",
      "\tRun 4, epoch 8:  0.371\n",
      "\tRun 5, epoch 4:  0.369\n",
      "\tRun 6, epoch 10:  0.379\n",
      "\tRun 7, epoch 9:  0.374\n",
      "\tRun 8, epoch 7:  0.384\n",
      "\tRun 9, epoch 8:  0.354\n",
      "\tRun 10, epoch 5:  0.388\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.515  0.434  0.392  0.365  0.346  0.324  0.305  0.368  0.342  0.314\n",
      "\t 0.498  0.429  0.427  0.387  0.390  0.391  0.382  0.402  0.391  0.371\n",
      "\t 0.111  0.064  0.093  0.076  0.090  0.089  0.096  0.132  0.104  0.106\n",
      "2\n",
      "\t 0.493  0.435  0.387  0.365  0.353  0.331  0.318  0.312  0.307  0.298\n",
      "\t 0.486  0.429  0.402  0.416  0.388  0.386  0.374  0.377  0.394  0.368\n",
      "\t 0.111  0.080  0.064  0.132  0.099  0.070  0.092  0.099  0.106  0.134\n",
      "3\n",
      "\t 0.517  0.432  0.388  0.365  0.342  0.323  0.300  0.346  0.352  0.327\n",
      "\t 0.472  0.439  0.392  0.399  0.386  0.384  0.400  0.408  0.401  0.415\n",
      "\t 0.103  0.100  0.072  0.071  0.091  0.075  0.080  0.093  0.091  0.112\n",
      "4\n",
      "\t 0.501  0.436  0.410  0.383  0.363  0.359  0.356  0.332  0.323  0.313\n",
      "\t 0.450  0.482  0.424  0.424  0.399  0.394  0.388  0.371  0.405  0.393\n",
      "\t 0.134  0.110  0.087  0.073  0.072  0.109  0.078  0.078  0.093  0.088\n",
      "5\n",
      "\t 0.521  0.427  0.374  0.343  0.323  0.332  0.397  0.402  0.446  0.417\n",
      "\t 0.478  0.423  0.400  0.369  0.373  0.499  0.449  0.488  0.466  0.436\n",
      "\t 0.110  0.088  0.081  0.068  0.095  0.161  0.131  0.144  0.143  0.104\n",
      "6\n",
      "\t 0.531  0.433  0.387  0.362  0.351  0.337  0.367  0.368  0.356  0.332\n",
      "\t 0.471  0.426  0.397  0.397  0.421  0.413  0.424  0.430  0.407  0.379\n",
      "\t 0.094  0.069  0.197  0.136  0.091  0.137  0.120  0.103  0.108  0.092\n",
      "7\n",
      "\t 0.500  0.419  0.378  0.356  0.359  0.330  0.314  0.308  0.292  0.285\n",
      "\t 0.500  0.439  0.425  0.387  0.406  0.389  0.393  0.391  0.374  0.391\n",
      "\t 0.114  0.080  0.070  0.093  0.114  0.106  0.094  0.099  0.100  0.103\n",
      "8\n",
      "\t 0.531  0.433  0.382  0.393  0.381  0.357  0.338  0.317  0.299  0.286\n",
      "\t 0.494  0.445  0.422  0.414  0.433  0.407  0.384  0.390  0.386  0.391\n",
      "\t 0.125  0.102  0.084  0.087  0.071  0.068  0.096  0.062  0.068  0.061\n",
      "9\n",
      "\t 0.542  0.465  0.410  0.383  0.364  0.346  0.329  0.313  0.295  0.285\n",
      "\t 0.515  0.444  0.408  0.390  0.383  0.383  0.361  0.354  0.365  0.385\n",
      "\t 0.129  0.106  0.076  0.077  0.071  0.078  0.068  0.070  0.077  0.086\n",
      "10\n",
      "\t 0.528  0.433  0.392  0.376  0.347  0.336  0.326  0.320  0.369  0.420\n",
      "\t 0.481  0.439  0.415  0.405  0.388  0.388  0.394  0.436  0.500  0.430\n",
      "\t 0.100  0.097  0.144  0.097  0.090  0.102  0.090  0.097  0.114  0.113\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_and_prior_losses(\"HepG2_prior_keep1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4e75aa9750>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation loss\"\n",
    "title += \"\\nTraining on only 1% of positive bins\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7637576384837572\n",
      "0.2274535370283432\n"
     ]
    }
   ],
   "source": [
    "np_vals, p_vals = np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(t)\n",
    "print(p / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
