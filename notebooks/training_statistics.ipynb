{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing saved metrics JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist, or the JSON is\n",
    "    not well-formed.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Metrics JSON at %s is not well-formed\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func, max_epoch=None):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return True if the _first_ one is better. If `max_epoch` is provided, will only\n",
    "    report everything up to this epoch (1-indexed).\n",
    "    Returns the number of the run, the (one-indexed) number of the epoch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                if i == max_epoch:\n",
    "                    break\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_profile_losses(models_path, max_epoch=None):\n",
    "    \"\"\"\n",
    "    Given the path to a condition containing many runs, prints out the best validation\n",
    "    profile NLL losses for each run, and the set of profile NLL losses for training\n",
    "    and validation over all epochs, as well as the validation prior loss. If given,\n",
    "    only consider up to `max_epoch` epochs total; anything afterward would be ignored.\n",
    "    \"\"\"\n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y,\n",
    "        max_epoch\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_models_path = \"/users/amtseng/att_priors/models/trained_models/profile/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 15\n",
      "\tBest epoch in run: 19\n",
      "\tAssociated value: 1124.7353904705558\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 16: 1337.16\n",
      "\tRun 2, epoch 16: 1202.70\n",
      "\tRun 3, epoch 19: 1178.05\n",
      "\tRun 4, epoch 15: 1171.67\n",
      "\tRun 5, epoch 20: 1206.72\n",
      "\tRun 6, epoch 20: 1229.60\n",
      "\tRun 7, epoch 20: 1194.91\n",
      "\tRun 8, epoch 7: 1208.16\n",
      "\tRun 9, epoch 16: 1278.28\n",
      "\tRun 10, epoch 20: 1267.31\n",
      "\tRun 11, epoch 13: 1222.74\n",
      "\tRun 12, epoch 20: 1300.97\n",
      "\tRun 13, epoch 11: 1145.27\n",
      "\tRun 14, epoch 16: 1182.13\n",
      "\tRun 15, epoch 19: 1124.74\n",
      "\tRun 16, epoch 10: 1180.36\n",
      "\tRun 17, epoch 20: 1238.16\n",
      "\tRun 18, epoch 15: 1264.91\n",
      "\tRun 19, epoch 19: 1202.23\n",
      "\tRun 20, epoch 5: 1329.16\n",
      "\tRun 21, epoch 20: 1207.09\n",
      "\tRun 22, epoch 7: 1219.05\n",
      "\tRun 23, epoch 20: 1265.47\n",
      "\tRun 24, epoch 10: 1168.66\n",
      "\tRun 25, epoch 19: 1369.37\n",
      "\tRun 26, epoch 19: 1165.81\n",
      "\tRun 27, epoch 20: 1268.48\n",
      "\tRun 28, epoch 17: 1399.02\n",
      "\tRun 29, epoch 17: 1247.74\n",
      "\tRun 30, epoch 19: 1213.50\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t1607.32 1572.54 1549.40 1703.14 1619.63 1611.42 1490.10 1519.81 1441.73 1422.69 1395.12 1367.05 1352.51 1323.05 1270.72 1244.98 1201.69 1163.15 1142.86 1131.66\n",
      "\t1464.52 1559.73 2491.55 1845.79 2983.45 1461.12 1473.22 1485.76 1382.93 1415.48 1540.63 1352.19 1756.86 1486.09 1402.23 1337.16 1417.25 1460.36 1355.86 1470.98\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t1422.38 1180.71 1074.99 1036.80 996.30 979.44 944.50 905.11 878.64 830.02 791.76 803.60 765.06 741.75 720.03 703.53 678.74 671.69 678.16 674.59\n",
      "\t1454.15 1320.64 1347.32 1413.37 1369.62 1231.36 1281.68 1305.26 1260.38 1332.77 1235.22 1279.22 1271.99 1222.32 1255.35 1202.70 1206.90 1250.20 1244.35 1273.57\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t1520.65 1365.12 1165.91 1107.58 1033.40 1008.57 1000.91 971.74 947.82 913.06 873.36 848.53 818.93 778.84 753.33 730.32 721.91 697.14 684.28 672.99\n",
      "\t1447.91 1465.94 1342.00 1299.38 1283.77 1363.85 1292.13 1255.82 1294.85 1260.85 1199.86 1255.98 1247.37 1221.94 1246.62 1297.63 1228.89 1199.68 1178.05 1224.24\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t1566.30 1357.30 1169.86 1081.73 1033.11 998.99 954.02 924.54 876.77 857.40 833.82 789.85 767.28 734.12 711.03 698.11 684.20 676.57\n",
      "\t1641.22 1474.93 1373.44 1469.62 1356.77 1344.11 1249.37 1309.61 1265.30 1366.24 1299.79 1227.27 1215.75 1189.94 1171.67 1217.12 1240.23 1263.14\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t1459.41 1264.93 1097.35 1039.86 1001.70 977.20 954.11 924.86 879.33 850.42 833.30 794.20 770.51 785.08 756.39 720.07 719.43 711.65 691.15 668.43\n",
      "\t1363.39 1352.62 1417.60 1349.78 1386.34 1356.23 1420.54 1343.10 1346.46 1298.57 1294.60 1293.07 1265.18 1290.75 1235.42 1323.64 1303.01 1221.95 1258.89 1206.72\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t1629.84 1484.75 1517.96 1598.45 1486.74 1336.89 1379.20 1261.25 1343.05 1225.94 1240.88 1195.47 1143.69 1150.03 1109.63 1104.52 1091.23 1070.62 1038.42 1038.15\n",
      "\t1438.71 1410.31 1717.96 1463.65 1309.31 1347.51 1391.25 1363.57 1301.33 1685.40 1337.66 1340.32 1325.09 1407.15 1402.99 1315.60 1252.20 1301.53 1288.46 1229.60\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t1452.20 1215.51 1113.51 1221.05 1118.81 1029.75 1008.52 988.39 993.40 953.48 922.86 888.43 866.04 859.12 906.65 880.25 798.94 786.66 781.50 750.47\n",
      "\t1383.54 1331.88 1347.49 1335.74 1400.77 1297.55 1244.54 1359.88 1285.39 1360.45 1272.93 1255.19 1277.28 1297.31 1399.76 1194.95 1217.26 1350.42 1214.39 1194.91\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "8\n",
      "\t1403.20 1159.82 1042.75 992.66 988.54 928.33 878.14 835.96 799.29 783.05\n",
      "\t1386.85 1456.06 1302.60 1585.69 1323.06 1331.94 1208.16 1227.19 1232.06 1263.36\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "9\n",
      "\t1555.63 1475.54 1516.26 1307.40 1202.29 1180.63 1171.58 1128.43 1094.45 1077.65 1056.53 1037.12 1394.77 1162.01 1024.73 998.27 982.78 988.34 971.06 960.01\n",
      "\t1527.63 1844.52 1658.96 1459.32 1545.35 1432.50 1385.97 1335.59 1342.15 1284.64 1335.45 1354.65 1326.84 1455.54 1316.39 1278.28 1401.68 1326.09 1302.26 1287.26\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "10\n",
      "\t1597.50 1513.61 1639.25 1550.80 1471.17 1426.64 1366.00 1243.13 1234.74 1126.85 1135.62 1170.47 1129.25 1074.48 1084.68 1058.18 1043.41 1025.52 1022.18 1031.67\n",
      "\t1885.84 1428.27 2239.67 1401.48 1941.03 1653.54 1615.15 1502.47 1363.57 1703.73 1560.97 1312.54 1341.57 1655.32 1393.77 1478.54 1357.63 1299.93 1283.07 1267.31\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "11\n",
      "\t1581.58 1406.19 1351.70 1231.76 1145.90 1107.78 1089.14 1092.78 1132.74 1104.66 1054.88 1014.68 995.54 973.12 958.61 995.17 930.72 904.40 892.08 858.27\n",
      "\t1495.21 1489.06 1467.12 1379.74 1361.30 1312.95 1413.36 1449.84 1268.60 1306.87 1291.91 1305.28 1222.74 1258.27 1699.86 1306.52 1239.08 1287.27 1250.15 1229.14\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "12\n",
      "\t1735.62 1529.89 1336.00 1224.02 1278.22 1330.86 1202.95 1158.50 1486.47 1144.57 1118.51 1089.18 1086.49 1042.72 1047.04 1013.49 1000.32 982.85 1004.99 954.04\n",
      "\t1512.44 1435.89 1454.27 1652.60 1405.99 1426.75 1409.55 1501.57 1658.50 1496.92 1391.98 1469.01 1381.50 1436.07 1331.33 1335.87 1339.55 1581.45 1317.79 1300.97\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "13\n",
      "\t1446.71 1145.81 1058.28 1004.38 970.50 943.06 887.61 835.03 813.20 775.85 746.79 725.39 717.50 691.81 671.38 664.61 650.50 637.05 622.91 625.33\n",
      "\t1393.71 1316.07 1298.33 1292.91 1288.03 1269.77 1259.60 1280.80 1230.60 1307.41 1145.27 1150.59 1167.02 1196.60 1185.74 1172.70 1173.51 1151.91 1170.79 1198.90\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "14\n",
      "\t1559.77 1271.29 1127.00 1063.96 1020.93 968.11 935.52 884.69 840.60 809.80 783.09 767.89 743.08 740.37 718.13 726.79 691.08 682.17 674.33 655.97\n",
      "\t1380.40 1408.87 1321.32 1315.61 1349.83 1307.20 1250.12 1282.99 1254.02 1264.24 1255.13 1258.65 1327.76 1238.43 1224.62 1182.13 1279.85 1199.42 1250.02 1216.54\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "15\n",
      "\t1521.81 1278.88 1106.80 1058.09 1003.97 972.37 896.21 859.15 827.36 789.82 747.44 716.79 693.07 689.60 677.60 658.53 647.92 633.02 620.45 612.41\n",
      "\t1358.24 1345.61 1301.38 1243.91 1305.54 1228.94 1319.56 1195.02 1228.58 1173.09 1212.85 1191.08 1177.96 1217.14 1193.50 1160.77 1167.60 1178.76 1124.74 1164.44\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "16\n",
      "\t1507.75 1288.72 1149.04 1055.32 1037.20 997.19 957.82 912.16 869.15 832.28 807.42 769.07 762.96 743.38 720.76 701.12 686.72 675.27 673.98 658.70\n",
      "\t1436.29 1357.07 1296.74 1354.27 1324.08 1267.01 1255.28 1310.53 1256.19 1180.36 1244.67 1209.00 1196.41 1307.40 1206.54 1193.34 1205.24 1215.61 1208.52 1189.06\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "17\n",
      "\t1539.13 1411.46 1216.50 1185.14 1081.93 1044.72 1047.72 986.18 958.94 934.42 901.54 882.87 887.21 840.23 819.94 812.45 778.68 760.25 743.13 744.04\n",
      "\t1429.78 1502.10 1417.44 1417.19 1399.20 1397.60 1379.41 1296.22 1286.20 1312.24 1315.74 1303.75 1390.23 1273.18 1238.55 1321.28 1266.81 1260.31 1239.34 1238.16\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "18\n",
      "\t1894.73 1575.76 1461.77 1283.73 1207.36 1182.16 1158.59 1090.15 1067.44 1085.95 1039.56 1011.77 1033.70 1045.51 994.42 970.07 969.23 943.84 927.82 925.88\n",
      "\t1561.16 1427.54 1411.53 1408.07 1387.21 1463.40 1321.47 1309.47 1339.18 1338.72 1281.48 1283.96 1343.35 1267.15 1264.91 1295.01 1293.03 1306.41 1295.82 1318.52\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "19\n",
      "\t1491.98 1336.89 1200.78 1126.54 1074.44 1046.86 1012.38 1006.53 971.33 961.22 966.80 934.22 947.72 915.08 870.07 844.75 843.04 824.98 808.36 799.24\n",
      "\t1614.60 1561.11 1500.03 1450.63 1362.54 1374.60 1332.38 1284.13 1288.37 1409.01 1323.97 1280.48 1269.36 1319.38 1274.86 1254.98 1275.57 1391.40 1202.23 1219.85\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "20\n",
      "\t1883.23 1712.74 1684.36 1746.30 1401.11 1300.37 1226.38 1168.59 1173.97 1174.45 1116.33 1106.30 1100.67 1089.14 1061.79 1066.49 1031.14 1020.89 1012.64 1008.47\n",
      "\t1978.88 2519.03 3896.09 1567.16 1329.16 1399.43 1385.48 1391.25 1470.99 1410.27 1376.22 1422.89 1494.32 1376.91 1436.77 1372.37 1417.40 1396.16 1386.04 1381.29\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1639.60 1477.75 1286.19 1186.41 1100.29 1073.24 1011.55 980.31 943.32 916.44 879.79 854.39 828.81 798.60 781.10 767.34 753.33 750.65 743.15 717.68\n",
      "\t1516.41 1557.38 1531.60 1489.20 1349.72 1278.81 1332.69 1297.01 1271.66 1292.08 1262.01 1287.18 1269.98 1238.77 1252.10 1275.01 1247.52 1288.02 1311.76 1207.09\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "22\n",
      "\t1520.33 1326.03 1240.55 1119.82 1151.58 1053.76 1117.48 1049.72 993.07 986.01 1026.68 945.16 920.80 927.37 895.31 887.90 859.86 849.89 831.20 796.28\n",
      "\t1395.79 1401.16 1407.73 1657.73 1324.46 1322.41 1219.05 1281.27 1335.49 1722.45 1286.09 1224.55 1249.67 1286.49 1230.94 1262.01 1247.16 1369.13 1263.71 1342.16\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "23\n",
      "\t1524.11 1406.25 1288.82 1155.86 1096.23 1058.00 1038.77 1039.59 1006.25 964.39 961.10 914.47 920.36 880.35 858.43 824.64 822.62 822.04 802.56 781.37\n",
      "\t1463.06 1391.07 1335.64 1346.32 1302.66 1359.02 1334.99 1474.44 1289.51 1304.14 1277.46 1324.43 1297.87 1332.13 1276.89 1285.11 1297.19 1322.14 1332.11 1265.47\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "24\n",
      "\t1362.28 1109.39 1035.76 988.06 936.79 892.20 855.40 809.77 757.76 735.21 698.40 686.23 672.40\n",
      "\t1488.22 1316.51 1344.89 1325.61 1273.32 1252.70 1209.22 1200.95 1308.80 1168.66 1191.42 1200.15 1268.76\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "25\n",
      "\t1523.59 1592.54 1618.04 1539.96 1596.55 1465.83 1486.18 1437.53 1405.88 1363.48 1352.90 1333.53 1321.42 1293.37 1262.05 1223.77 1213.44 1176.95 1181.77 1151.55\n",
      "\t1724.43 1470.12 1449.71 1973.11 1447.52 1496.33 1546.30 1428.93 1400.53 1392.71 1385.94 1383.73 1426.02 1435.67 1580.97 1390.28 1512.12 1484.10 1369.37 1390.43\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "26\n",
      "\t1463.72 1232.38 1107.49 1046.62 1012.56 984.33 953.26 913.59 869.90 837.34 814.44 799.46 757.69 727.24 705.10 683.02 667.89 656.28 657.33 651.38\n",
      "\t1392.44 1461.47 1313.89 1421.67 1338.04 1356.87 1273.93 1316.34 1255.13 1362.75 1225.54 1258.07 1236.90 1241.51 1201.46 1194.25 1178.55 1198.50 1165.81 1206.81\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "27\n",
      "\t1671.42 1568.80 1428.37 1265.41 1218.25 1210.11 1212.40 1162.18 1153.86 1134.38 1044.45 1304.92 1149.40 1030.34 1001.41 990.21 982.32 1039.88 984.56 952.04\n",
      "\t1590.08 1432.51 1625.01 1491.79 1388.25 1353.57 1653.06 1395.19 1366.09 1333.86 1314.37 1537.44 1329.05 1298.50 1303.43 1294.28 1297.14 1608.38 1296.59 1268.48\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "28\n",
      "\t1648.56 1542.51 1524.69 1509.02 1489.77 1475.65 1475.24 1443.73 1434.47 1382.97 1351.31 1314.03 1287.15 1252.95 1223.77 1201.32 1196.82 1173.52 1153.52 1140.63\n",
      "\t1893.63 1609.83 1658.13 1513.87 1527.80 1452.51 1447.67 1460.54 1456.13 1448.13 1508.94 1440.11 1467.31 1478.41 1453.14 1531.67 1399.02 1474.65 1416.67 1436.45\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "29\n",
      "\t1567.35 1351.40 1223.76 1160.60 1104.48 1087.68 1063.73 1007.60 982.55 1002.00 1049.26 958.94 926.00 1133.54 958.88 906.70 882.80 867.18 869.78 938.75\n",
      "\t1460.28 1536.48 1734.50 1424.99 1330.46 1464.17 1386.38 1291.47 1403.94 1378.08 1381.79 1326.62 1301.25 1328.58 1269.83 1276.58 1247.74 1264.15 1275.26 1300.60\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "30\n",
      "\t1634.93 1349.48 1204.36 1138.90 1095.25 1068.17 1027.23 1022.07 1006.22 964.66 983.32 937.41 919.26 911.39 874.58 931.58 896.98 844.94 820.95 809.25\n",
      "\t1520.05 1413.32 1461.06 1336.04 1670.62 1267.27 1431.73 1248.30 1338.67 1356.12 1348.07 1297.37 1327.47 1318.54 1256.74 1342.72 1268.14 1222.37 1213.50 1226.61\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_prof_val_losses = print_profile_losses(os.path.join(profile_models_path, \"K562_keep1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 15\n",
      "\tBest epoch in run: 20\n",
      "\tAssociated value: 1121.268041457573\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 16: 1145.12\n",
      "\tRun 2, epoch 12: 1324.14\n",
      "\tRun 3, epoch 10: 1198.87\n",
      "\tRun 4, epoch 19: 1243.37\n",
      "\tRun 5, epoch 20: 1238.43\n",
      "\tRun 6, epoch 20: 1301.01\n",
      "\tRun 7, epoch 19: 1429.87\n",
      "\tRun 8, epoch 15: 1123.96\n",
      "\tRun 9, epoch 18: 1136.69\n",
      "\tRun 10, epoch 18: 1208.01\n",
      "\tRun 11, epoch 19: 1152.21\n",
      "\tRun 12, epoch 17: 1349.26\n",
      "\tRun 13, epoch 19: 1214.80\n",
      "\tRun 14, epoch 14: 1393.12\n",
      "\tRun 15, epoch 20: 1121.27\n",
      "\tRun 16, epoch 17: 1201.22\n",
      "\tRun 17, epoch 12: 1236.76\n",
      "\tRun 18, epoch 20: 1224.48\n",
      "\tRun 19, epoch 10: 1212.30\n",
      "\tRun 20, epoch 20: 1229.42\n",
      "\tRun 21, epoch 15: 1277.50\n",
      "\tRun 22, epoch 18: 1183.88\n",
      "\tRun 23, epoch 19: 1305.58\n",
      "\tRun 24, epoch 17: 1249.45\n",
      "\tRun 25, epoch 18: 1145.00\n",
      "\tRun 26, epoch 17: 1285.29\n",
      "\tRun 27, epoch 19: 1265.11\n",
      "\tRun 28, epoch 14: 1216.95\n",
      "\tRun 29, epoch 17: 1132.72\n",
      "\tRun 30, epoch 17: 1247.44\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t1487.50 1202.92 1061.59 1013.55 963.74 914.59 870.19 838.54 794.35 765.75 735.86 707.52 687.28 672.44 692.27 646.35 628.42 641.32 631.02\n",
      "\t1394.31 1332.05 1364.71 1229.60 1340.91 1235.33 1196.36 1246.89 1189.99 1312.94 1196.92 1219.36 1179.72 1212.16 1164.11 1145.12 1171.72 1241.59 1347.04\n",
      "\t0.2663 0.2585 0.2504 0.2361 0.2275 0.2154 0.2112 0.2036 0.1977 0.1922 0.1857 0.1895 0.1827 0.1807 0.1773 0.1751 0.1741 0.1642 0.1736\n",
      "2\n",
      "\t1648.77 1543.97 1511.29 1465.40 1403.90 1449.60 1368.72 1230.54 1220.19 1213.16 1131.85 1100.51 1087.73 1065.59 1060.01 1062.13 1029.00 1012.30 1005.81 998.21\n",
      "\t1512.67 1851.99 1522.81 1394.70 1664.62 2375.15 1329.75 1337.06 1721.74 1341.42 1355.53 1324.14 1327.34 1325.46 1340.15 1335.17 1325.84 1367.34 1374.85 1409.43\n",
      "\t0.3016 0.3111 0.2666 0.2713 0.2748 0.2613 0.2552 0.2491 0.2541 0.2500 0.2487 0.2442 0.2474 0.2409 0.2422 0.2452 0.2376 0.2374 0.2376 0.2288\n",
      "3\n",
      "\t1531.93 1279.38 1147.74 1099.32 1025.60 993.68 955.08 916.56 890.62 835.24 798.85 785.04 756.27\n",
      "\t1543.99 1504.11 1325.62 1403.11 1349.92 1332.88 1345.08 1228.49 1266.01 1198.87 1211.76 1215.28 1250.85\n",
      "\t0.3058 0.2941 0.2644 0.2661 0.2443 0.2364 0.2204 0.2152 0.2078 0.1959 0.1882 0.1830 0.1809\n",
      "4\n",
      "\t1687.26 1582.65 1564.27 1356.90 1230.84 1195.84 1170.50 1143.48 1217.72 1094.79 1092.93 1196.42 1099.25 1055.57 1052.22 1025.66 1006.81 1001.15 985.33 981.44\n",
      "\t1463.94 1420.84 1433.08 1420.66 1395.43 1563.11 1467.31 1620.10 1342.54 1341.69 1358.91 1406.49 1345.76 1370.48 1397.76 1315.34 1308.98 1261.60 1243.37 1246.52\n",
      "\t0.2658 0.2844 0.3566 0.3528 0.3245 0.2821 0.2740 0.2572 0.2774 0.2541 0.2505 0.2680 0.2599 0.2448 0.2467 0.2367 0.2375 0.2346 0.2316 0.2315\n",
      "5\n",
      "\t2296.79 1884.38 1458.23 1345.18 1309.51 1282.12 1272.50 1198.70 1189.99 1178.44 1138.61 1136.14 1151.79 1114.08 1065.88 1050.13 1051.39 1107.45 1070.91 1068.07\n",
      "\t4478.57 1381.83 1458.62 1468.57 1472.03 1328.21 1455.69 1423.25 1291.89 1335.80 1274.16 1252.10 1262.36 1349.95 1285.62 1290.33 1271.06 1366.04 1443.48 1238.43\n",
      "\t0.3720 0.3590 0.3710 0.3569 0.3705 0.2893 0.3683 0.3645 0.3015 0.3295 0.3317 0.3139 0.3237 0.3471 0.3449 0.3432 0.3389 0.3481 0.3437 0.3074\n",
      "6\n",
      "\t1664.79 1594.84 1532.42 1488.31 1516.33 1475.18 1539.68 1434.94 1439.87 1374.96 1338.47 1312.88 1280.03 1248.53 1217.15 1176.85 1145.46 1120.63 1117.66 1101.70\n",
      "\t2049.04 1433.37 1457.34 1446.19 1429.82 1888.82 1418.61 1400.17 1393.03 1387.02 1439.21 1400.89 1380.58 1376.30 1447.23 1385.10 1388.70 1370.28 1485.83 1301.01\n",
      "\t0.3718 0.3276 0.2947 0.2713 0.2619 0.2800 0.2666 0.2571 0.2672 0.2639 0.2658 0.2611 0.2532 0.2492 0.2525 0.2385 0.2409 0.2366 0.2439 0.2272\n",
      "7\n",
      "\t1627.68 1571.54 1540.88 1482.18 1479.14 1529.72 1530.03 1470.99 1453.59 1455.91 1445.41 1438.99 1450.76 1430.91 1429.50 1427.50 1439.18 1410.52 1411.23 1406.63\n",
      "\t1534.33 1881.68 1500.52 1715.49 1742.39 1718.65 1438.68 1451.78 1452.82 1454.86 1511.54 1521.97 1478.18 1446.51 1479.21 1449.14 1515.56 1451.22 1429.87 1451.72\n",
      "\t0.1939 0.3450 0.3003 0.2668 0.2540 0.2576 0.2479 0.2554 0.2532 0.2539 0.2600 0.2615 0.2575 0.2613 0.2482 0.2551 0.2465 0.2378 0.2319 0.2262\n",
      "8\n",
      "\t1521.38 1296.09 1166.57 1095.59 1035.96 976.02 918.24 874.52 832.04 815.35 785.88 756.53 731.86 715.82 705.07 681.76 667.49 656.98 649.74 634.99\n",
      "\t1535.74 1510.77 1320.00 1270.15 1275.55 1428.77 1370.65 1248.18 1196.38 1244.35 1187.90 1214.58 1241.56 1171.54 1123.96 1153.45 1166.76 1174.29 1129.58 1153.16\n",
      "\t0.2841 0.2673 0.2514 0.2304 0.2316 0.2055 0.2008 0.1835 0.1821 0.1832 0.1801 0.1799 0.1706 0.1667 0.1630 0.1618 0.1597 0.1596 0.1554 0.1521\n",
      "9\n",
      "\t1498.34 1274.98 1156.17 1067.88 1010.31 1006.83 946.11 889.71 861.38 818.08 770.34 738.89 723.13 709.38 711.22 682.37 664.42 654.20 644.69 633.49\n",
      "\t1493.65 1440.47 1426.86 1331.34 1235.64 1257.23 1264.21 1208.90 1488.33 1242.87 1247.74 1213.70 1197.23 1176.35 1239.25 1138.35 1215.98 1136.69 1141.95 1143.76\n",
      "\t0.2344 0.2347 0.2468 0.2445 0.2495 0.2466 0.2471 0.2479 0.2464 0.2437 0.2373 0.2350 0.2243 0.2212 0.2112 0.2153 0.2056 0.2024 0.1971 0.1925\n",
      "10\n",
      "\t1518.61 1387.69 1217.96 1109.68 1081.16 1049.84 1013.50 1008.92 972.96 1042.03 962.19 919.70 901.35 899.62 867.64 845.02 850.06 834.97 800.88 778.58\n",
      "\t1432.72 1424.14 1549.21 1296.94 1357.37 1388.72 1307.58 1284.41 1297.46 1244.62 1359.70 1240.48 1311.48 1249.53 1305.42 1252.17 1283.62 1208.01 1298.41 1257.36\n",
      "\t0.3392 0.3080 0.3429 0.3311 0.3052 0.2863 0.2516 0.2581 0.2468 0.3305 0.3104 0.2759 0.2321 0.2229 0.2159 0.2168 0.2139 0.2126 0.2096 0.2046\n",
      "11\n",
      "\t1647.56 1476.75 1379.14 1243.78 1183.04 1150.49 1123.85 1116.63 1047.27 998.90 973.91 943.51 932.59 900.29 854.89 810.14 776.55 779.37 754.39 734.92\n",
      "\t1471.93 1438.85 1473.96 1443.04 1363.84 1442.94 1318.72 1361.32 1309.82 1268.21 1425.85 1351.32 1218.75 1181.71 1190.89 1255.22 1241.59 1222.67 1152.21 1164.38\n",
      "\t0.2657 0.2678 0.3201 0.2915 0.2702 0.2576 0.2575 0.2540 0.2456 0.2322 0.2315 0.2251 0.2238 0.2041 0.1934 0.1939 0.1901 0.1867 0.1760 0.1720\n",
      "12\n",
      "\t1611.84 1514.98 1476.12 1470.46 1483.32 1439.38 1467.34 1421.04 1420.55 1419.78 1414.84 1374.89 1339.90 1299.13 1234.21 1192.03 1158.39 1115.12 1078.14 1073.84\n",
      "\t1508.41 1489.67 1522.18 1582.87 1511.73 1477.75 1428.23 1439.55 1419.73 1571.50 1423.60 1419.92 1473.78 1433.69 1377.79 1372.58 1349.26 1398.95 1432.46 1510.75\n",
      "\t0.3080 0.2571 0.2563 0.2595 0.2593 0.2609 0.2468 0.2598 0.2406 0.2684 0.2544 0.2384 0.2507 0.2505 0.2524 0.2514 0.2425 0.2456 0.2471 0.2482\n",
      "13\n",
      "\t1598.62 1475.32 1270.65 1156.34 1097.81 1050.19 1033.22 1004.61 981.36 1019.30 966.62 928.10 890.54 874.46 886.26 939.25 847.47 812.15 795.05 784.04\n",
      "\t1620.87 1424.25 1495.42 1374.38 1326.08 1372.11 1352.10 1321.27 1374.15 1513.79 1383.02 1304.32 1300.16 1243.07 1235.54 1329.69 1245.81 1284.51 1214.80 1239.73\n",
      "\t0.2941 0.3629 0.3458 0.3322 0.3224 0.3166 0.3056 0.2844 0.2523 0.2569 0.2473 0.2346 0.2271 0.2186 0.2213 0.2343 0.2212 0.2167 0.2122 0.2115\n",
      "14\n",
      "\t1647.95 1567.73 1580.11 1556.49 1559.94 1531.05 1520.01 1524.84 1508.03 1491.27 1473.91 1463.42 1457.82 1427.92 1421.17 1402.77 1380.19 1364.65 1354.17 1332.74\n",
      "\t1499.09 1496.95 1449.31 1469.90 1571.99 1536.45 1442.35 1545.94 1468.09 1477.10 1481.77 1426.98 1479.19 1393.12 1436.03 1416.69 1413.60 1426.55 1442.79 1401.96\n",
      "\t0.2160 0.1961 0.2446 0.2566 0.2675 0.2672 0.2639 0.2752 0.2680 0.2653 0.2402 0.2293 0.2208 0.2058 0.2168 0.2212 0.2150 0.2093 0.2140 0.2197\n",
      "15\n",
      "\t1512.30 1265.53 1134.07 1050.40 1012.93 976.87 912.92 872.38 831.55 788.10 753.85 723.26 707.86 690.19 682.20 659.99 668.71 647.75 636.59 624.62\n",
      "\t1374.61 1441.69 1338.52 1382.34 1306.43 1266.01 1244.63 1286.37 1201.32 1234.59 1199.90 1179.02 1173.61 1156.58 1180.37 1121.40 1156.51 1125.03 1156.40 1121.27\n",
      "\t0.2750 0.2666 0.2526 0.2316 0.2263 0.2145 0.2090 0.1951 0.1944 0.1888 0.1836 0.1799 0.1722 0.1700 0.1671 0.1706 0.1635 0.1597 0.1577 0.1549\n",
      "16\n",
      "\t1551.55 1418.98 1290.09 1113.64 1131.29 1111.77 1029.60 1005.48 978.50 979.31 963.03 922.33 923.25 875.71 855.25 828.59 786.09 783.74 763.44 737.21\n",
      "\t1465.34 1665.64 1451.82 1368.90 1432.13 1439.88 1385.67 1349.86 1269.48 1447.69 1418.49 1385.19 1363.56 1247.86 1257.25 1295.12 1201.22 1201.70 1321.84 1230.69\n",
      "\t0.2686 0.2961 0.2573 0.2537 0.2830 0.2890 0.2659 0.2446 0.2395 0.2476 0.2384 0.2223 0.2314 0.2225 0.2228 0.2166 0.2118 0.2144 0.2034 0.2029\n",
      "17\n",
      "\t1517.90 1349.03 1249.70 1150.03 1088.93 1053.73 1025.41 1052.84 1019.01 975.49 961.24 1011.64 985.61 948.40 919.71 905.97 901.71 907.01 874.40 877.09\n",
      "\t1398.13 1439.73 1397.87 1317.33 1295.49 1314.78 1286.89 1263.91 1265.78 1292.94 1310.57 1236.76 1268.29 1263.72 1350.64 1250.19 1391.07 1264.83 1403.83 1389.29\n",
      "\t0.3417 0.3579 0.3555 0.3516 0.3554 0.3555 0.3546 0.3408 0.3524 0.3487 0.3525 0.3394 0.3341 0.3293 0.3270 0.3205 0.3118 0.3068 0.2897 0.2692\n",
      "18\n",
      "\t1521.91 1277.11 1163.16 1152.86 1118.31 1041.32 1215.32 1114.19 1012.93 987.31 959.24 1004.48 971.28 990.22 917.44 884.99 923.31 873.15 854.93 820.68\n",
      "\t1413.51 1345.64 1295.74 1401.56 1460.97 1289.26 1552.92 1267.53 1328.04 1324.01 1265.39 1802.43 1312.09 1294.60 1271.10 1328.62 1253.18 1239.75 1241.93 1224.48\n",
      "\t0.3622 0.3368 0.3377 0.3249 0.2974 0.3035 0.3125 0.2833 0.2692 0.2671 0.2681 0.2545 0.2583 0.2541 0.2524 0.2520 0.2484 0.2441 0.2457 0.2394\n",
      "19\n",
      "\t1504.53 1253.20 1122.24 1054.75 1030.79 1011.38 1019.25 986.12 932.65 886.13 855.81 827.59 814.31 797.38 784.90 758.25 772.45 733.39 728.75 717.02\n",
      "\t1451.56 1339.48 1312.00 1328.55 1270.16 1233.10 1340.94 1390.54 1235.64 1212.30 1302.66 1280.38 1282.05 1309.93 1268.70 1509.93 1214.55 1327.65 1242.29 1267.60\n",
      "\t0.3080 0.2866 0.2530 0.2519 0.2555 0.2612 0.2675 0.2633 0.2523 0.2465 0.2376 0.2346 0.2271 0.2175 0.2162 0.2148 0.2140 0.2022 0.1998 0.1964\n",
      "20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1637.13 1512.80 1411.12 1253.54 1161.68 1090.78 1046.01 1025.09 997.30 1086.03 973.64 946.28 925.90 913.92 906.74 898.46 859.20 872.04 832.82 834.49\n",
      "\t1539.38 1410.22 1360.85 1380.98 1407.46 1392.43 1370.44 1373.71 1287.52 1348.44 1248.48 1316.61 1297.54 1416.79 1339.46 1310.45 1614.24 1314.23 1248.62 1229.42\n",
      "\t0.3256 0.2886 0.3160 0.3259 0.3217 0.3168 0.3011 0.2746 0.2431 0.2758 0.2437 0.2351 0.2333 0.2294 0.2281 0.2215 0.2230 0.2166 0.2181 0.2163\n",
      "21\n",
      "\t1629.94 1583.72 1621.07 1298.26 1325.28 1163.07 1119.21 1161.77 1125.43 1114.28 1084.71 1354.70 1091.96 1033.90 1021.19 1008.95 1053.51 1025.40 987.82 1041.44\n",
      "\t1455.01 1623.61 1470.65 1395.94 1511.44 1321.41 1416.00 1347.51 1552.59 1327.09 1731.00 1383.96 1354.28 1293.77 1277.50 1392.24 1345.88 1304.94 1325.79 1295.69\n",
      "\t0.3209 0.3209 0.3153 0.2951 0.3617 0.3310 0.2895 0.2695 0.2737 0.2590 0.2569 0.3590 0.3320 0.3194 0.2945 0.2573 0.2513 0.2535 0.2398 0.2462\n",
      "22\n",
      "\t1731.63 1667.46 1494.56 1304.53 1175.65 1092.96 1034.69 994.72 965.11 928.80 872.03 823.43 802.37 769.85 748.75 731.41 706.85 698.05 687.24 689.93\n",
      "\t1593.81 1914.96 1527.72 1356.97 1361.08 1396.86 1371.25 1354.91 1336.94 1250.98 1244.91 1318.45 1266.62 1200.03 1288.32 1215.90 1208.88 1183.88 1193.15 1186.79\n",
      "\t0.1704 0.2581 0.2682 0.2552 0.2504 0.2395 0.2333 0.2233 0.2186 0.2148 0.2085 0.2038 0.1990 0.1939 0.1920 0.1870 0.1843 0.1827 0.1774 0.1765\n",
      "23\n",
      "\t1609.52 1462.90 1417.37 1368.01 1317.19 1177.18 1174.43 1191.63 1088.87 1088.08 1058.45 1087.57 1023.90 1036.79 1011.52 1038.92 983.33 964.71 947.59 928.86\n",
      "\t1454.49 1460.97 1483.19 1776.25 1361.44 1595.92 1396.58 1385.39 1355.65 1707.38 1378.45 1356.28 1405.00 1467.50 1337.99 1326.09 1332.19 1347.46 1305.58 1333.46\n",
      "\t0.3202 0.2948 0.2749 0.3183 0.2702 0.2569 0.2499 0.2502 0.2537 0.2523 0.2533 0.2514 0.2520 0.2517 0.2526 0.2532 0.2489 0.2501 0.2458 0.2429\n",
      "24\n",
      "\t1532.29 1293.85 1268.32 1119.02 1068.14 1193.78 1055.24 1009.58 989.27 973.50 991.27 962.60 951.12 921.33 918.33 869.81 857.92 832.65 813.24 829.67\n",
      "\t1435.46 1372.51 1346.85 1282.60 1263.80 1469.34 1330.41 1265.83 1291.74 1296.72 1280.72 1505.24 1270.66 1253.99 1281.67 1267.19 1249.45 1311.80 1271.37 1324.47\n",
      "\t0.3443 0.3369 0.2850 0.3211 0.2975 0.2624 0.2913 0.2752 0.2652 0.2599 0.2572 0.2598 0.2506 0.2487 0.2479 0.2457 0.2416 0.2396 0.2344 0.2330\n",
      "25\n",
      "\t1406.26 1159.40 1072.38 1015.67 955.95 938.00 872.87 813.98 783.30 763.15 736.89 705.33 685.05 672.21 664.41 660.40 640.97 658.24 635.08 635.20\n",
      "\t1350.72 1285.68 1293.41 1223.00 1311.44 1251.45 1215.47 1252.64 1191.37 1276.76 1194.71 1225.92 1280.43 1204.19 1194.21 1158.35 1181.69 1145.00 1308.73 1213.81\n",
      "\t0.2391 0.2486 0.2446 0.2467 0.2512 0.2434 0.2461 0.2363 0.2295 0.2231 0.2202 0.2152 0.2035 0.2050 0.2000 0.1975 0.1919 0.1898 0.1888 0.1819\n",
      "26\n",
      "\t1695.02 1514.82 1410.70 1236.50 1145.45 1125.65 1094.89 1048.09 1030.56 1034.34 1023.38 984.21 969.52 997.41 953.50 920.14 910.24 906.30 882.81 883.81\n",
      "\t1450.73 1414.77 1385.00 1460.96 1445.36 1549.42 1365.63 1372.53 1298.84 1294.18 1360.73 1361.54 1329.83 1340.02 1289.01 1332.86 1285.29 1306.88 1324.78 1353.52\n",
      "\t0.3065 0.2800 0.3439 0.2709 0.2667 0.2642 0.2668 0.2536 0.2536 0.2624 0.2516 0.2492 0.2531 0.2546 0.2505 0.2538 0.2500 0.2450 0.2490 0.2422\n",
      "27\n",
      "\t1466.57 1249.97 1179.86 1156.02 1204.70 1103.74 1081.09 1044.72 1534.87 1052.75 1014.85 998.60 994.20 972.51 954.21 937.58 919.45 914.23 896.98 884.10\n",
      "\t1518.32 1507.32 1399.94 1327.25 1432.63 1274.12 1482.44 1375.01 1427.05 1343.61 1295.69 1309.05 1307.79 1308.49 1302.40 1329.91 1275.99 1294.26 1265.11 1440.05\n",
      "\t0.3754 0.3518 0.3503 0.3437 0.3431 0.3272 0.3316 0.3153 0.3504 0.3516 0.3474 0.3454 0.3491 0.3483 0.3482 0.3486 0.3497 0.3452 0.3387 0.3252\n",
      "28\n",
      "\t1791.99 1498.27 1278.79 1188.02 1121.12 1062.58 1032.99 1005.26 985.55 967.96 944.95 947.59 914.61 904.14 887.45 865.58 845.00 815.37 809.12 798.76\n",
      "\t1815.04 1432.28 1466.33 1429.89 1350.72 1297.14 1265.62 1321.73 1442.99 1308.22 1297.38 1332.16 1323.85 1216.95 1291.26 1240.98 1309.77 1242.90 1398.46 1277.96\n",
      "\t0.3778 0.3468 0.3317 0.3065 0.2766 0.2493 0.2427 0.2460 0.2392 0.2347 0.2335 0.2315 0.2277 0.2273 0.2306 0.2209 0.2245 0.2196 0.2136 0.2140\n",
      "29\n",
      "\t1484.93 1283.95 1125.87 1058.16 1024.79 1032.23 968.95 936.43 892.57 872.10 841.93 797.74 779.42 752.64 743.59 737.12 719.47 697.98 692.61 676.85\n",
      "\t1409.45 1321.97 1323.01 1264.95 1268.35 1333.67 1320.11 1404.03 1318.68 1258.32 1206.47 1182.31 1210.95 1261.26 1232.04 1197.25 1132.72 1265.45 1292.21 1203.84\n",
      "\t0.3422 0.2873 0.3081 0.2840 0.2490 0.2546 0.2324 0.2241 0.2164 0.2127 0.2040 0.2014 0.1974 0.1957 0.1958 0.1894 0.1898 0.1884 0.1868 0.1832\n",
      "30\n",
      "\t1593.09 1531.19 1392.70 1405.67 1300.14 1238.56 1172.91 1495.35 1115.69 1077.72 1113.93 1461.85 1066.56 1050.42 1063.53 1048.30 1230.81 1022.26 994.02 978.37\n",
      "\t1444.64 1460.44 1497.71 1389.46 1382.85 1442.91 1338.08 1287.79 1416.13 1281.33 3736.05 1392.35 1365.78 1289.91 1314.71 1486.09 1247.44 1294.01 1319.76 1336.83\n",
      "\t0.3304 0.3634 0.2886 0.3345 0.2919 0.2597 0.2718 0.3151 0.2601 0.2630 0.2787 0.2620 0.2586 0.2740 0.2687 0.2635 0.2530 0.2644 0.2653 0.2635\n"
     ]
    }
   ],
   "source": [
    "prior_prof_val_losses = print_profile_losses(os.path.join(profile_models_path, \"K562_prior_keep1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean without priors: 1232.980928\n",
      "Mean with priors: 1233.107389\n",
      "One-sided t-test p: 0.497287\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_prof_val_losses.values())), np.array(list(prior_prof_val_losses.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With Fourier prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation profile loss without/with Fourier priors\"\n",
    "title += \"\\nK562, %d/%d profile models without/with Fourier priors\" % (len(noprior_prof_val_losses), len(prior_prof_val_losses))\n",
    "title += \"\\nTraining on all peaks\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()\n",
    "\n",
    "np_vals, p_vals = np.array(list(noprior_prof_val_losses.values())), np.array(list(prior_prof_val_losses.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(\"Mean without priors: %f\" % np.mean(np_vals))\n",
    "print(\"Mean with priors: %f\" % np.mean(p_vals))\n",
    "print(\"One-sided t-test p: %f\" % (p / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_binary_losses(models_path, max_epoch=None):\n",
    "    \"\"\"\n",
    "    Given the path to a condition containing many runs, prints out the best validation\n",
    "    losses for each run, and the set of losses for training and validation over all\n",
    "    epochs, as well as the validation prior loss. If given, only consider up to\n",
    "    `max_epoch` epochs total; anything afterward would be ignored.\n",
    "    \"\"\"\n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y,\n",
    "        max_epoch\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.3f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"train_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_models_path = \"/users/amtseng/att_priors/models/trained_models/binary/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 18\n",
      "\tBest epoch in run: 1\n",
      "\tAssociated value: 0.24132131622769895\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 1:  0.248\n",
      "\tRun 3, epoch 1:  0.255\n",
      "\tRun 4, epoch 1:  0.252\n",
      "\tRun 5, epoch 1:  0.254\n",
      "\tRun 7, epoch 1:  0.242\n",
      "\tRun 8, epoch 1:  0.254\n",
      "\tRun 9, epoch 1:  0.253\n",
      "\tRun 11, epoch 1:  0.251\n",
      "\tRun 12, epoch 1:  0.249\n",
      "\tRun 13, epoch 1:  0.253\n",
      "\tRun 14, epoch 1:  0.245\n",
      "\tRun 15, epoch 1:  0.246\n",
      "\tRun 16, epoch 1:  0.265\n",
      "\tRun 18, epoch 1:  0.241\n",
      "\tRun 19, epoch 1:  0.247\n",
      "\tRun 20, epoch 1:  0.254\n",
      "\tRun 23, epoch 1:  0.248\n",
      "\tRun 24, epoch 1:  0.253\n",
      "\tRun 25, epoch 1:  0.249\n",
      "\tRun 28, epoch 1:  0.249\n",
      "\tRun 29, epoch 1:  0.249\n",
      "\tRun 30, epoch 1:  0.250\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.215  0.142  0.109  0.089  0.078\n",
      "\t 0.248  0.281  0.331  0.372  0.400\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "3\n",
      "\t 0.219  0.143  0.109  0.090  0.078\n",
      "\t 0.255  0.314  0.346  0.384  0.423\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "4\n",
      "\t 0.224  0.146  0.113  0.092  0.079\n",
      "\t 0.252  0.282  0.311  0.384  0.415\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "5\n",
      "\t 0.214  0.140  0.106  0.089  0.077\n",
      "\t 0.254  0.270  0.334  0.370  0.409\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "7\n",
      "\t 0.217  0.143  0.110  0.092  0.079\n",
      "\t 0.242  0.289  0.320  0.351  0.431\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "8\n",
      "\t 0.218  0.144  0.110  0.091  0.079\n",
      "\t 0.254  0.290  0.344  0.413  0.413\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "9\n",
      "\t 0.214  0.140  0.108  0.090  0.077\n",
      "\t 0.253  0.307  0.333  0.368  0.424\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "11\n",
      "\t 0.218  0.143  0.109  0.091  0.078\n",
      "\t 0.251  0.293  0.334  0.397  0.451\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "12\n",
      "\t 0.220  0.144  0.110  0.091  0.078\n",
      "\t 0.249  0.308  0.336  0.397  0.416\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "13\n",
      "\t 0.215  0.140  0.108  0.089  0.077\n",
      "\t 0.253  0.297  0.330  0.382  0.418\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "14\n",
      "\t 0.217  0.142  0.108  0.090  0.078\n",
      "\t 0.245  0.292  0.327  0.385  0.423\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "15\n",
      "\t 0.216  0.140  0.107  0.089  0.078\n",
      "\t 0.246  0.288  0.334  0.367  0.416\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "16\n",
      "\t 0.224  0.144  0.111  0.093  0.080\n",
      "\t 0.265  0.292  0.349  0.389  0.448\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "18\n",
      "\t 0.216  0.140  0.108  0.090  0.078\n",
      "\t 0.241  0.290  0.317  0.362  0.401\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "19\n",
      "\t 0.218  0.143  0.110  0.091  0.079\n",
      "\t 0.247  0.289  0.343  0.389  0.402\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "20\n",
      "\t 0.216  0.141  0.108  0.089  0.079\n",
      "\t 0.254  0.293  0.350  0.405  0.414\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "23\n",
      "\t 0.215  0.140  0.107  0.088  0.076\n",
      "\t 0.248  0.275  0.333  0.384  0.413\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "24\n",
      "\t 0.214  0.141  0.109  0.089  0.077\n",
      "\t 0.253  0.298  0.349  0.405  0.442\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "25\n",
      "\t 0.217  0.141  0.108  0.089  0.077\n",
      "\t 0.249  0.310  0.353  0.405  0.413\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "28\n",
      "\t 0.221  0.144  0.110  0.091  0.079\n",
      "\t 0.249  0.287  0.326  0.359  0.415\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "29\n",
      "\t 0.221  0.144  0.110  0.091  0.078\n",
      "\t 0.249  0.293  0.350  0.377  0.423\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "30\n",
      "\t 0.215  0.140  0.108  0.090  0.077\n",
      "\t 0.250  0.289  0.333  0.373  0.415\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n"
     ]
    }
   ],
   "source": [
    "noprior_bin_val_losses = print_binary_losses(os.path.join(binary_models_path, \"K562\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 9\n",
      "\tBest epoch in run: 8\n",
      "\tAssociated value: 0.2318139820654168\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 3:  0.262\n",
      "\tRun 2, epoch 4:  0.255\n",
      "\tRun 3, epoch 7:  0.244\n",
      "\tRun 4, epoch 7:  0.243\n",
      "\tRun 5, epoch 3:  0.248\n",
      "\tRun 6, epoch 7:  0.236\n",
      "\tRun 7, epoch 7:  0.241\n",
      "\tRun 8, epoch 5:  0.245\n",
      "\tRun 9, epoch 8:  0.232\n",
      "\tRun 10, epoch 5:  0.238\n",
      "\tRun 11, epoch 3:  0.235\n",
      "\tRun 12, epoch 6:  0.232\n",
      "\tRun 13, epoch 5:  0.245\n",
      "\tRun 14, epoch 5:  0.245\n",
      "\tRun 15, epoch 3:  0.238\n",
      "\tRun 16, epoch 4:  0.244\n",
      "\tRun 18, epoch 4:  0.244\n",
      "\tRun 19, epoch 5:  0.244\n",
      "\tRun 20, epoch 3:  0.240\n",
      "\tRun 21, epoch 6:  0.246\n",
      "\tRun 22, epoch 5:  0.238\n",
      "\tRun 23, epoch 5:  0.244\n",
      "\tRun 24, epoch 8:  0.245\n",
      "\tRun 25, epoch 6:  0.235\n",
      "\tRun 27, epoch 7:  0.243\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.286  0.241  0.220  0.202  0.184  0.170  0.146  0.153  0.125  0.119\n",
      "\t 0.281  0.277  0.262  0.267  0.268  0.337  0.302  0.319  0.329  0.340\n",
      "\t 0.060  0.056  0.050  0.052  0.047  0.063  0.039  0.038  0.035  0.035\n",
      "2\n",
      "\t 0.299  0.247  0.224  0.219  0.205  0.193  0.185  0.156  0.138  0.122\n",
      "\t 0.303  0.260  0.264  0.255  0.263  0.286  0.277  0.291  0.319  0.361\n",
      "\t 0.082  0.061  0.087  0.057  0.070  0.078  0.058  0.045  0.043  0.040\n",
      "3\n",
      "\t 0.293  0.256  0.245  0.225  0.214  0.199  0.183  0.166  0.152  0.133\n",
      "\t 0.343  0.307  0.261  0.279  0.257  0.245  0.244  0.262  0.266  0.293\n",
      "\t 0.082  0.074  0.053  0.057  0.070  0.060  0.045  0.043  0.040  0.041\n",
      "4\n",
      "\t 0.326  0.263  0.253  0.233  0.222  0.209  0.196  0.202  0.199  0.199  0.184  0.170  0.159\n",
      "\t 0.396  0.300  0.269  0.259  0.251  0.261  0.243  0.251  0.256  0.248  0.261  0.273  0.278\n",
      "\t 0.087  0.086  0.057  0.062  0.054  0.068  0.057  0.066  0.066  0.069  0.068  0.059  0.066\n",
      "5\n",
      "\t 0.328  0.272  0.220  0.175  0.141  0.115\n",
      "\t 0.313  0.277  0.248  0.252  0.304  0.331\n",
      "\t 0.054  0.051  0.037  0.039  0.033  0.030\n",
      "6\n",
      "\t 0.314  0.268  0.238  0.225  0.208  0.199  0.187  0.165  0.152  0.137\n",
      "\t 0.325  0.271  0.262  0.245  0.258  0.241  0.236  0.250  0.272  0.287\n",
      "\t 0.069  0.045  0.051  0.044  0.079  0.064  0.046  0.053  0.047  0.040\n",
      "7\n",
      "\t 0.323  0.280  0.255  0.219  0.232  0.204  0.189  0.185  0.184  0.161  0.153  0.147\n",
      "\t 0.295  0.344  0.263  0.256  0.251  0.242  0.241  0.252  0.252  0.257  0.279  0.275\n",
      "\t 0.052  0.070  0.047  0.059  0.048  0.050  0.051  0.069  0.056  0.060  0.056  0.066\n",
      "8\n",
      "\t 0.320  0.278  0.239  0.230  0.206  0.176  0.148  0.126\n",
      "\t 0.349  0.288  0.253  0.252  0.245  0.257  0.280  0.330\n",
      "\t 0.076  0.067  0.054  0.059  0.040  0.041  0.033  0.032\n",
      "9\n",
      "\t 0.296  0.231  0.215  0.222  0.221  0.215  0.194  0.178  0.171  0.160  0.159\n",
      "\t 0.284  0.253  0.249  0.260  0.270  0.245  0.236  0.232  0.250  0.268  0.257\n",
      "\t 0.050  0.055  0.062  0.086  0.066  0.048  0.044  0.044  0.051  0.050  0.067\n",
      "10\n",
      "\t 0.330  0.256  0.219  0.211  0.192  0.175  0.154  0.132\n",
      "\t 0.286  0.261  0.246  0.247  0.238  0.248  0.274  0.285\n",
      "\t 0.080  0.047  0.097  0.056  0.051  0.042  0.036  0.043\n",
      "11\n",
      "\t 0.339  0.264  0.219  0.183  0.165  0.183\n",
      "\t 0.298  0.291  0.235  0.246  0.242  0.242\n",
      "\t 0.065  0.066  0.033  0.048  0.059  0.071\n",
      "12\n",
      "\t 0.332  0.256  0.242  0.235  0.213  0.189  0.168  0.142  0.119\n",
      "\t 0.296  0.275  0.260  0.269  0.236  0.232  0.239  0.257  0.286\n",
      "\t 0.046  0.068  0.058  0.078  0.048  0.036  0.038  0.036  0.040\n",
      "13\n",
      "\t 0.296  0.258  0.217  0.214  0.195  0.183  0.157  0.136  0.117  0.103  0.097\n",
      "\t 0.303  0.278  0.259  0.305  0.245  0.258  0.278  0.289  0.321  0.364  0.385\n",
      "\t 0.058  0.066  0.068  0.072  0.074  0.060  0.062  0.039  0.042  0.039  0.043\n",
      "14\n",
      "\t 0.309  0.256  0.237  0.217  0.192  0.163  0.138  0.119\n",
      "\t 0.289  0.271  0.300  0.253  0.245  0.257  0.291  0.329\n",
      "\t 0.058  0.056  0.080  0.064  0.037  0.034  0.034  0.041\n",
      "15\n",
      "\t 0.294  0.245  0.214  0.193  0.174  0.154\n",
      "\t 0.286  0.261  0.238  0.244  0.254  0.266\n",
      "\t 0.061  0.048  0.045  0.055  0.052  0.050\n",
      "16\n",
      "\t 0.289  0.243  0.217  0.196  0.176  0.176  0.159  0.176  0.166\n",
      "\t 0.302  0.262  0.250  0.244  0.261  0.259  0.266  0.266  0.276\n",
      "\t 0.081  0.060  0.057  0.055  0.061  0.061  0.062  0.064  0.074\n",
      "18\n",
      "\t 0.312  0.243  0.229  0.212  0.184  0.159  0.134  0.115\n",
      "\t 0.289  0.296  0.255  0.244  0.249  0.266  0.305  0.337\n",
      "\t 0.056  0.102  0.054  0.045  0.036  0.037  0.034  0.045\n",
      "19\n",
      "\t 0.296  0.246  0.237  0.221  0.193  0.165  0.140  0.128\n",
      "\t 0.297  0.274  0.257  0.269  0.244  0.261  0.281  0.286\n",
      "\t 0.061  0.085  0.061  0.062  0.049  0.048  0.044  0.059\n",
      "20\n",
      "\t 0.304  0.218  0.199  0.194  0.186  0.155  0.135  0.122  0.136\n",
      "\t 0.302  0.257  0.240  0.259  0.261  0.265  0.291  0.298  0.306\n",
      "\t 0.080  0.057  0.058  0.078  0.058  0.039  0.042  0.044  0.068\n",
      "21\n",
      "\t 0.315  0.263  0.229  0.225  0.212  0.197  0.174  0.170  0.149\n",
      "\t 0.321  0.275  0.258  0.265  0.252  0.246  0.257  0.260  0.269\n",
      "\t 0.074  0.052  0.038  0.065  0.052  0.051  0.044  0.054  0.060\n",
      "22\n",
      "\t 0.304  0.257  0.230  0.217  0.190  0.164  0.139  0.142\n",
      "\t 0.288  0.263  0.250  0.247  0.238  0.258  0.257  0.275\n",
      "\t 0.049  0.057  0.058  0.053  0.040  0.040  0.062  0.059\n",
      "23\n",
      "\t 0.341  0.270  0.236  0.215  0.202  0.177  0.160  0.145\n",
      "\t 0.360  0.294  0.257  0.254  0.244  0.252  0.267  0.269\n",
      "\t 0.063  0.063  0.050  0.055  0.047  0.047  0.044  0.051\n",
      "24\n",
      "\t 0.308  0.280  0.260  0.252  0.229  0.203  0.194  0.187  0.181  0.163  0.145\n",
      "\t 0.324  0.292  0.282  0.271  0.252  0.251  0.247  0.245  0.249  0.252  0.272\n",
      "\t 0.068  0.060  0.064  0.044  0.045  0.070  0.051  0.049  0.047  0.054  0.046\n",
      "25\n",
      "\t 0.313  0.264  0.237  0.209  0.191  0.174  0.161  0.149  0.138\n",
      "\t 0.317  0.279  0.262  0.245  0.243  0.235  0.254  0.269  0.292\n",
      "\t 0.065  0.062  0.061  0.057  0.043  0.043  0.052  0.039  0.040\n",
      "27\n",
      "\t 0.297  0.240  0.221  0.217  0.195  0.178  0.161  0.156  0.139  0.126\n",
      "\t 0.278  0.287  0.251  0.245  0.243  0.246  0.243  0.266  0.277  0.306\n",
      "\t 0.058  0.076  0.072  0.058  0.052  0.052  0.049  0.061  0.053  0.045\n"
     ]
    }
   ],
   "source": [
    "prior_bin_val_losses = print_binary_losses(os.path.join(binary_models_path, \"K562_prior\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_bin_val_losses.values())), np.array(list(prior_bin_val_losses.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With Fourier prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation loss without/with Fourier priors\"\n",
    "title += \"\\nGATA2, %d/%d binary models without/with priors\" % (len(noprior_bin_val_losses), len(prior_bin_val_losses))\n",
    "title += \"\\nTraining on all peaks\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "np_vals, p_vals = np.array(list(noprior_bin_val_losses.values())), np.array(list(prior_bin_val_losses.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(\"Mean without priors: %f\" % np.mean(np_vals))\n",
    "print(\"Mean with priors: %f\" % np.mean(p_vals))\n",
    "print(\"One-sided t-test p: %f\" % (p / 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
