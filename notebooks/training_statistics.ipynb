{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the (one-indexed) number of the epch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_validation_profile_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/profile/%s/\" % condition\n",
    "    \n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 8\n",
      "\tBest epoch in run: 10\n",
      "\tAssociated value: 85.18974167152687\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 9:  85.39\n",
      "\tRun 2, epoch 7:  85.24\n",
      "\tRun 3, epoch 8:  85.57\n",
      "\tRun 4, epoch 7:  85.59\n",
      "\tRun 5, epoch 10:  85.96\n",
      "\tRun 6, epoch 9:  85.64\n",
      "\tRun 7, epoch 9:  85.54\n",
      "\tRun 8, epoch 10:  85.19\n",
      "\tRun 9, epoch 10:  85.37\n",
      "\tRun 10, epoch 9:  85.65\n",
      "\tRun 11, epoch 9:  85.39\n",
      "\tRun 12, epoch 9:  85.40\n",
      "\tRun 13, epoch 10:  85.97\n",
      "\tRun 14, epoch 10:  85.29\n",
      "\tRun 15, epoch 10:  85.42\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 95.14  88.02  86.81  86.35  86.12  85.97  85.87  85.81  85.74  85.68\n",
      "\t 88.08  86.73  86.07  85.85  85.62  85.52  85.54  85.48  85.39  85.43\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t 91.23  86.87  86.23  85.94  85.74  85.64  85.57  85.49  85.43  85.40\n",
      "\t 87.47  86.15  85.79  85.47  85.41  85.29  85.24  85.30  85.28  85.33\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t 93.98  87.21  86.39  86.06  85.84  85.67  85.59  85.48  85.46  85.39\n",
      "\t 87.83  86.53  86.19  85.85  85.75  85.71  85.64  85.57  85.59  85.69\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t 92.61  87.37  86.63  86.29  86.08  85.94  85.85  85.78  85.71  85.66\n",
      "\t 87.69  86.54  86.15  85.89  85.73  85.76  85.59  85.71  85.69  85.61\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t 93.05  87.05  86.41  86.10  85.91  85.78  85.66  85.60  85.51  85.48\n",
      "\t 88.06  86.97  86.74  86.40  86.44  86.22  86.14  86.19  85.98  85.96\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t 93.84  87.39  86.70  86.31  86.06  85.93  85.81  85.72  85.68  85.64\n",
      "\t 88.45  86.76  86.67  86.10  85.92  85.94  85.85  85.96  85.64  85.69\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t 96.97  87.50  86.55  86.18  85.98  85.85  85.76  85.67  85.64  85.58\n",
      "\t 88.09  86.98  86.20  85.82  85.79  85.79  85.67  85.57  85.54  85.64\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "8\n",
      "\t 94.24  87.35  86.50  86.17  85.94  85.81  85.71  85.64  85.58  85.54\n",
      "\t 87.36  86.37  85.82  85.69  85.50  85.34  85.39  85.31  85.25  85.19\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "9\n",
      "\t 90.33  86.77  86.25  85.98  85.80  85.68  85.60  85.52  85.49  85.45\n",
      "\t 87.12  86.37  86.01  85.90  85.75  85.54  85.70  85.53  85.49  85.37\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "10\n",
      "\t 93.47  87.20  86.42  86.08  85.90  85.78  85.68  85.63  85.56  85.51\n",
      "\t 87.68  86.92  86.21  86.04  85.98  85.89  85.73  85.82  85.65  85.69\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "11\n",
      "\t 93.22  87.34  86.56  86.22  85.97  85.86  85.76  85.68  85.60  85.56\n",
      "\t 87.88  86.51  86.08  85.74  85.75  85.65  85.47  85.45  85.39  85.47\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "12\n",
      "\t 91.98  87.04  86.31  86.00  85.83  85.69  85.60  85.54  85.48  85.42\n",
      "\t 87.40  86.15  85.84  85.63  85.55  85.57  85.43  85.42  85.40  85.41\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "13\n",
      "\t 94.06  88.68  87.38  86.74  86.39  86.19  86.06  85.95  85.86  85.80\n",
      "\t 89.49  87.87  87.04  86.58  86.36  86.28  86.14  86.08  86.05  85.97\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "14\n",
      "\t 91.62  87.43  86.71  86.45  86.23  86.13  86.02  85.95  85.90  85.84\n",
      "\t 87.21  86.17  86.00  85.72  85.51  85.51  85.42  85.43  85.44  85.29\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "15\n",
      "\t 91.96  87.05  86.37  86.05  85.85  85.72  85.63  85.58  85.53  85.47\n",
      "\t 87.47  86.29  86.12  85.90  85.68  85.61  85.69  85.59  85.50  85.42\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_profile_and_prior_losses(\"SPI1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 3\n",
      "\tBest epoch in run: 10\n",
      "\tAssociated value: 85.32390595895272\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 10:  85.80\n",
      "\tRun 2, epoch 9:  85.98\n",
      "\tRun 3, epoch 10:  85.32\n",
      "\tRun 4, epoch 10:  85.70\n",
      "\tRun 5, epoch 10:  86.33\n",
      "\tRun 6, epoch 9:  85.95\n",
      "\tRun 7, epoch 4:  86.53\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 92.91  87.50  86.82  86.52  86.33  86.19  86.09  86.04  85.97  85.90\n",
      "\t 87.73  86.90  86.60  86.21  86.21  86.05  86.06  85.83  85.96  85.80\n",
      "\t0.0481 0.0408 0.0387 0.0365 0.0360 0.0349 0.0332 0.0342 0.0329 0.0317\n",
      "2\n",
      "\t 95.55  87.99  87.04  86.63  86.42  86.28  86.17  86.09  86.06  85.99\n",
      "\t 88.50  87.27  86.65  86.42  86.41  86.08  86.05  86.12  85.98  86.00\n",
      "\t0.0464 0.0417 0.0395 0.0367 0.0369 0.0348 0.0347 0.0336 0.0333 0.0330\n",
      "3\n",
      "\t 98.92  88.31  87.13  86.66  86.41  86.29  86.19  86.12  86.07  86.02\n",
      "\t 88.38  86.86  86.03  86.08  85.63  85.60  85.55  85.51  85.34  85.32\n",
      "\t0.0477 0.0425 0.0391 0.0368 0.0361 0.0348 0.0339 0.0335 0.0335 0.0331\n",
      "4\n",
      "\t 97.02  88.45  87.19  86.67  86.42  86.28  86.17  86.09  85.98  85.93\n",
      "\t 88.88  87.37  86.64  86.34  86.08  85.95  85.86  85.79  85.78  85.70\n",
      "\t0.0507 0.0434 0.0391 0.0365 0.0357 0.0347 0.0336 0.0331 0.0333 0.0326\n",
      "5\n",
      "\t 92.92  87.44  86.73  86.44  86.22  86.11  85.99  85.97  85.89  85.82\n",
      "\t 88.43  87.46  86.96  86.76  86.66  86.52  86.55  86.54  86.47  86.33\n",
      "\t0.0478 0.0411 0.0383 0.0351 0.0357 0.0338 0.0337 0.0332 0.0325 0.0319\n",
      "6\n",
      "\t 95.97  87.91  86.87  86.45  86.24  86.12  86.02  85.93  85.88  85.84\n",
      "\t 89.05  87.19  86.52  86.56  86.24  86.10  86.11  85.96  85.95  85.99\n",
      "\t0.0491 0.0429 0.0401 0.0381 0.0362 0.0358 0.0342 0.0337 0.0333 0.0323\n",
      "7\n",
      "\t 98.49  88.09  87.19  86.72\n",
      "\t 88.63  87.35  86.76  86.53\n",
      "\t0.0472 0.0413 0.0384 0.0375\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_profile_and_prior_losses(\"SPI1_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/binary/%s/\" % condition\n",
    "    \n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.3f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"train_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 12\n",
      "\tBest epoch in run: 2\n",
      "\tAssociated value: 0.26864333367058446\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 2:  0.270\n",
      "\tRun 2, epoch 2:  0.277\n",
      "\tRun 3, epoch 2:  0.274\n",
      "\tRun 4, epoch 2:  0.279\n",
      "\tRun 5, epoch 1:  0.281\n",
      "\tRun 6, epoch 2:  0.273\n",
      "\tRun 7, epoch 1:  0.281\n",
      "\tRun 8, epoch 2:  0.278\n",
      "\tRun 9, epoch 2:  0.275\n",
      "\tRun 10, epoch 2:  0.280\n",
      "\tRun 11, epoch 2:  0.270\n",
      "\tRun 12, epoch 2:  0.269\n",
      "\tRun 13, epoch 2:  0.270\n",
      "\tRun 14, epoch 2:  0.275\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.285  0.236  0.216  0.201  0.190  0.180  0.172  0.167  0.161  0.157\n",
      "\t 0.273  0.270  0.277  0.291  0.305  0.326  0.335  0.355  0.362  0.381\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "2\n",
      "\t 0.289  0.240  0.220  0.205  0.194  0.185  0.177  0.171  0.166  0.161\n",
      "\t 0.282  0.277  0.286  0.299  0.316  0.320  0.337  0.348  0.361  0.378\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "3\n",
      "\t 0.288  0.240  0.221  0.206  0.196  0.187  0.180  0.174  0.169  0.164\n",
      "\t 0.279  0.274  0.284  0.297  0.311  0.327  0.340  0.348  0.361  0.387\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "4\n",
      "\t 0.288  0.243  0.223  0.209  0.197  0.188  0.180  0.173  0.168  0.163\n",
      "\t 0.279  0.279  0.292  0.296  0.310  0.327  0.340  0.356  0.372  0.374\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "5\n",
      "\t 0.293  0.243  0.222  0.205  0.192  0.182  0.174  0.168  0.163  0.158\n",
      "\t 0.281  0.283  0.284  0.292  0.312  0.327  0.347  0.360  0.379  0.389\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "6\n",
      "\t 0.287  0.237  0.218  0.204  0.193  0.183  0.176  0.170  0.164  0.160\n",
      "\t 0.277  0.273  0.285  0.294  0.309  0.322  0.340  0.351  0.375  0.383\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "7\n",
      "\t 0.290  0.243  0.226  0.212  0.202  0.194  0.186  0.177  0.171  0.166\n",
      "\t 0.281  0.281  0.288  0.305  0.317  0.334  0.339  0.354  0.372  0.388\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "8\n",
      "\t 0.287  0.242  0.223  0.208  0.197  0.187  0.179  0.172  0.167  0.162\n",
      "\t 0.280  0.278  0.284  0.298  0.312  0.324  0.350  0.357  0.375  0.385\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "9\n",
      "\t 0.287  0.239  0.220  0.205  0.194  0.185  0.180  0.172  0.167  0.162\n",
      "\t 0.278  0.275  0.286  0.301  0.305  0.327  0.330  0.349  0.352  0.375\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "10\n",
      "\t 0.291  0.243  0.226  0.213  0.201  0.191  0.184  0.177  0.171  0.166\n",
      "\t 0.282  0.280  0.294  0.302  0.316  0.334  0.349  0.362  0.373  0.382\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "11\n",
      "\t 0.285  0.233  0.214  0.200  0.189  0.180  0.173  0.167  0.162  0.157\n",
      "\t 0.271  0.270  0.279  0.287  0.302  0.325  0.330  0.347  0.359  0.373\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "12\n",
      "\t 0.286  0.236  0.217  0.202  0.191  0.182  0.174  0.168  0.162  0.158\n",
      "\t 0.276  0.269  0.278  0.294  0.307  0.324  0.336  0.353  0.367  0.384\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "13\n",
      "\t 0.288  0.237  0.216  0.202  0.190  0.181  0.173  0.167  0.161  0.157\n",
      "\t 0.276  0.270  0.278  0.292  0.304  0.328  0.336  0.346  0.362  0.384\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "14\n",
      "\t 0.284  0.236  0.215  0.200\n",
      "\t 0.277  0.275  0.279  0.289\n",
      "\t 0.000  0.000  0.000  0.000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_and_prior_losses(\"SPI1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 1\n",
      "\tBest epoch in run: 6\n",
      "\tAssociated value: 0.26952209825107726\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 6:  0.270\n",
      "\tRun 2, epoch 3:  0.280\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.312  0.256  0.253  0.241  0.230  0.221  0.213  0.211  0.205  0.200\n",
      "\t 0.287  0.317  0.279  0.274  0.274  0.270  0.274  0.270  0.278  0.284\n",
      "\t 0.029  0.052  0.036  0.040  0.031  0.030  0.033  0.034  0.032  0.032\n",
      "2\n",
      "\t 0.330  0.278  0.261\n",
      "\t 0.304  0.287  0.280\n",
      "\t 0.033  0.030  0.028\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_and_prior_losses(\"SPI1_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7ec8bd5fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation loss\"\n",
    "title += \"\\nTraining on only 1% of peaks\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11280479867322166\n",
      "0.45589343351802236\n"
     ]
    }
   ],
   "source": [
    "np_vals, p_vals = np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(t)\n",
    "print(p / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
