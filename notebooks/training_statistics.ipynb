{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the (one-indexed) number of the epch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_validation_profile_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 10\n",
      "\tBest epoch in run: 9\n",
      "\tAssociated value: 84.99444200727675\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 10:  85.98\n",
      "\tRun 2, epoch 10:  85.34\n",
      "\tRun 3, epoch 10:  85.44\n",
      "\tRun 4, epoch 9:  85.01\n",
      "\tRun 5, epoch 10:  85.88\n",
      "\tRun 6, epoch 9:  85.07\n",
      "\tRun 7, epoch 9:  85.62\n",
      "\tRun 8, epoch 8:  85.33\n",
      "\tRun 9, epoch 9:  86.10\n",
      "\tRun 10, epoch 9:  84.99\n",
      "\tRun 11, epoch 10:  86.12\n",
      "\tRun 12, epoch 10:  85.67\n",
      "\tRun 13, epoch 10:  85.43\n",
      "\tRun 14, epoch 10:  85.39\n",
      "\tRun 15, epoch 10:  85.73\n",
      "\tRun 16, epoch 9:  85.56\n",
      "\tRun 17, epoch 10:  86.51\n",
      "\tRun 18, epoch 9:  85.51\n",
      "\tRun 19, epoch 10:  85.09\n",
      "\tRun 20, epoch 10:  85.82\n",
      "\tRun 21, epoch 9:  85.76\n",
      "\tRun 22, epoch 9:  85.76\n",
      "\tRun 23, epoch 10:  85.57\n",
      "\tRun 24, epoch 10:  85.87\n",
      "\tRun 25, epoch 9:  85.46\n",
      "\tRun 26, epoch 9:  85.45\n",
      "\tRun 27, epoch 10:  85.82\n",
      "\tRun 28, epoch 8:  85.62\n",
      "\tRun 29, epoch 9:  86.20\n",
      "\tRun 30, epoch 10:  85.42\n",
      "\tRun 31, epoch 9:  85.40\n",
      "\tRun 32, epoch 10:  85.36\n",
      "\tRun 33, epoch 8:  85.28\n",
      "\tRun 34, epoch 10:  85.63\n",
      "\tRun 35, epoch 9:  85.48\n",
      "\tRun 36, epoch 9:  85.15\n",
      "\tRun 37, epoch 10:  85.91\n",
      "\tRun 38, epoch 10:  86.24\n",
      "\tRun 39, epoch 5:  85.98\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 91.61  87.03  86.41  86.10  85.92  85.80  85.71  85.64  85.61  85.54\n",
      "\t 87.86  86.70  86.37  86.20  86.25  86.09  86.08  86.00  86.00  85.98\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t 95.69  87.62  86.68  86.29  86.06  85.91  85.81  85.72  85.66  85.60\n",
      "\t 87.77  86.37  85.99  85.88  85.65  85.42  85.37  85.47  85.41  85.34\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t 95.84  87.78  86.75  86.32  86.11  85.93  85.79  85.71  85.64  85.58\n",
      "\t 88.36  86.83  86.22  85.96  85.78  85.68  85.63  85.51  85.49  85.44\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t 93.90  87.51  86.62  86.31  86.09  85.95  85.85  85.77  85.70  85.66\n",
      "\t 87.42  86.16  85.56  85.44  85.35  85.19  85.17  85.07  85.01  85.02\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t 91.12  86.90  86.28  86.01  85.81  85.69  85.59  85.54  85.47  85.43\n",
      "\t 87.55  86.91  86.44  86.28  86.17  86.10  85.95  85.97  86.00  85.88\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t 94.08  87.48  86.57  86.21  86.00  85.84  85.73  85.63  85.57  85.53\n",
      "\t 87.94  86.13  85.63  85.41  85.30  85.18  85.13  85.19  85.07  85.16\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t 93.25  87.33  86.48  86.11  85.90  85.75  85.69  85.62  85.55  85.51\n",
      "\t 87.94  86.81  86.21  86.08  85.85  85.75  85.82  85.75  85.62  85.78\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "8\n",
      "\t 93.76  87.51  86.69  86.37  86.17  86.02  85.93  85.86  85.81  85.76\n",
      "\t 87.76  86.35  85.90  85.73  85.53  85.45  85.40  85.33  85.40  85.40\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "9\n",
      "\t 93.62  87.24  86.26  85.87  85.69  85.55  85.47  85.40  85.33  85.29\n",
      "\t 88.66  87.15  86.56  86.36  86.33  86.23  86.13  86.18  86.10  86.12\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "10\n",
      "\t 92.24  87.14  86.39  86.10  85.89  85.75  85.64  85.58  85.50  85.44\n",
      "\t 86.95  85.86  85.56  85.38  85.23  85.12  85.11  85.00  84.99  85.02\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "11\n",
      "\t 96.13  88.33  87.25  86.73  86.39  86.22  86.10  86.03  85.95  85.88\n",
      "\t 90.55  87.52  87.09  86.93  86.49  86.35  86.29  86.20  86.15  86.12\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "12\n",
      "\t 95.24  87.77  86.83  86.42  86.20  86.06  85.97  85.89  85.81  85.77\n",
      "\t 88.40  86.90  86.40  86.08  85.95  85.79  85.87  85.82  85.68  85.67\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "13\n",
      "\t 94.40  87.55  86.64  86.29  86.09  85.95  85.85  85.75  85.70  85.65\n",
      "\t 87.92  86.52  86.16  85.92  85.72  85.70  85.66  85.59  85.60  85.43\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "14\n",
      "\t 94.04  87.27  86.49  86.17  85.95  85.82  85.72  85.64  85.58  85.55\n",
      "\t 87.60  86.52  86.06  85.79  85.73  85.57  85.48  85.58  85.41  85.39\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "15\n",
      "\t 93.53  87.40  86.40  86.07  85.86  85.73  85.65  85.58  85.54  85.48\n",
      "\t 88.53  86.67  86.39  86.11  86.13  85.85  85.83  85.78  85.80  85.73\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "16\n",
      "\t 92.49  87.20  86.52  86.17  85.96  85.81  85.72  85.65  85.61  85.53\n",
      "\t 87.91  86.52  86.34  86.07  85.88  85.89  85.77  85.74  85.56  85.56\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "17\n",
      "\t 92.62  87.27  86.54  86.17  85.96  85.80  85.68  85.61  85.56  85.51\n",
      "\t 88.79  87.63  87.15  86.96  86.83  87.02  86.61  86.51  86.57  86.51\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "18\n",
      "\t 92.30  87.48  86.67  86.30  86.10  85.96  85.84  85.79  85.71  85.66\n",
      "\t 87.87  86.62  86.11  85.87  85.82  85.68  85.59  85.59  85.51  85.54\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "19\n",
      "\t 92.78  87.06  86.30  85.98  85.78  85.66  85.57  85.51  85.44  85.41\n",
      "\t 87.31  86.10  85.67  85.45  85.35  85.23  85.19  85.20  85.12  85.09\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "20\n",
      "\t 96.84  88.10  86.87  86.35  86.08  85.90  85.80  85.70  85.63  85.59\n",
      "\t 89.71  87.25  86.61  86.20  86.26  86.14  85.94  85.83  85.88  85.82\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "21\n",
      "\t 92.21  87.16  86.48  86.17  85.99  85.91  85.79  85.72  85.65  85.60\n",
      "\t 87.90  86.85  86.41  86.09  85.98  85.88  86.14  85.95  85.76  85.81\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "22\n",
      "\t 93.10  87.48  86.67  86.33  86.10  85.98  85.88  85.81  85.74  85.69\n",
      "\t 88.05  86.86  86.49  86.17  85.97  85.87  85.90  85.82  85.76  85.78\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "23\n",
      "\t 92.03  87.21  86.60  86.28  86.10  85.98  85.88  85.81  85.74  85.72\n",
      "\t 87.36  86.44  86.04  85.95  85.80  85.69  85.69  85.67  85.62  85.57\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "24\n",
      "\t 94.48  87.59  86.61  86.21  86.02  85.88  85.80  85.72  85.66  85.62\n",
      "\t 88.51  86.80  86.37  86.17  86.01  86.06  86.04  85.90  85.91  85.87\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "25\n",
      "\t 94.65  88.52  87.51  86.94  86.68  86.50  86.71  86.69  86.42  86.54\n",
      "\t 88.50  87.09  86.27  85.90  85.71  85.63  85.52  85.64  85.46  85.48\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "26\n",
      "\t 96.15  87.78  86.86  86.42  86.20  86.02  85.90  85.83  85.79  85.73\n",
      "\t 87.96  86.66  86.16  85.94  85.66  85.61  85.60  85.58  85.45  85.61\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "27\n",
      "\t 93.37  87.27  86.52  86.21  86.02  85.89  85.79  85.70  85.61  85.59\n",
      "\t 88.03  86.87  86.43  86.32  86.19  86.15  86.04  85.95  85.96  85.82\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "28\n",
      "\t 92.19  87.05  86.24  85.92  85.75  85.60  85.52  85.44  85.40  85.35\n",
      "\t 87.94  86.63  86.33  85.97  85.82  85.76  85.75  85.62  85.62  85.67\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "29\n",
      "\t 92.58  87.42  86.66  86.30  86.07  85.92  85.83  85.76  85.69  85.65\n",
      "\t 88.24  87.46  86.70  86.57  86.52  86.24  86.24  86.33  86.20  86.21\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "30\n",
      "\t 91.18  87.02  86.35  86.07  85.91  85.79  85.71  85.65  85.60  85.56\n",
      "\t 87.55  86.20  85.81  85.85  85.68  85.55  85.51  85.48  85.44  85.42\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "31\n",
      "\t 94.28  87.44  86.69  86.40  86.21  86.09  86.02  85.98  85.92  85.84\n",
      "\t 87.52  86.40  86.13  85.81  85.64  85.65  85.64  85.53  85.40  85.42\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "32\n",
      "\t 91.93  87.16  86.92  86.46  86.31  86.19  86.06  86.00  85.93  85.96\n",
      "\t 87.32  86.26  86.06  85.93  85.71  85.77  85.56  85.48  85.52  85.36\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "33\n",
      "\t 92.54  87.03  86.36  86.05  85.84  85.71  85.58  85.48  85.43  85.38\n",
      "\t 87.26  86.35  85.83  85.70  85.60  85.48  85.39  85.28  85.33  85.29\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 91.46  86.89  86.26  85.98  85.81  85.69  85.64  85.54  85.51  85.46\n",
      "\t 87.53  86.43  86.23  85.94  85.81  85.75  85.70  85.70  85.65  85.63\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "35\n",
      "\t 94.93  87.58  86.55  86.13  85.93  85.75  85.66  85.58  85.51  85.44\n",
      "\t 88.59  86.85  86.12  85.88  85.79  85.72  85.67  85.56  85.48  85.62\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "36\n",
      "\t 94.41  87.31  86.55  86.21  85.99  85.85  85.74  85.68  85.63  85.59\n",
      "\t 87.22  86.17  85.72  85.47  85.37  85.23  85.29  85.21  85.15  85.19\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "37\n",
      "\t 94.24  87.27  86.45  86.10  85.95  85.79  85.70  85.62  85.56  85.53\n",
      "\t 88.39  87.15  86.58  86.32  86.27  86.13  86.06  86.10  86.04  85.91\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "38\n",
      "\t 90.96  86.93  86.33  86.03  85.83  85.69  85.59  85.53  85.46  85.42\n",
      "\t 87.91  87.32  86.77  86.73  86.37  86.37  86.34  86.25  86.31  86.24\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "39\n",
      "\t 97.50  88.35  87.05  86.50  86.22\n",
      "\t 90.27  87.09  86.50  86.05  85.98\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_profile_and_prior_losses(\"SPI1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "Warning: Was not able to compute values for run 15\n",
      "\tBest run: 3\n",
      "\tBest epoch in run: 10\n",
      "\tAssociated value: 85.73426015641954\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 10:  86.37\n",
      "\tRun 2, epoch 10:  85.84\n",
      "\tRun 3, epoch 10:  85.73\n",
      "\tRun 4, epoch 10:  86.23\n",
      "\tRun 5, epoch 10:  86.06\n",
      "\tRun 6, epoch 9:  86.37\n",
      "\tRun 7, epoch 10:  85.89\n",
      "\tRun 8, epoch 10:  86.33\n",
      "\tRun 9, epoch 10:  86.36\n",
      "\tRun 10, epoch 9:  85.75\n",
      "\tRun 11, epoch 10:  85.90\n",
      "\tRun 12, epoch 9:  86.04\n",
      "\tRun 13, epoch 10:  86.47\n",
      "\tRun 14, epoch 9:  86.04\n",
      "\tRun 16, epoch 9:  86.56\n",
      "\tRun 17, epoch 9:  86.43\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 91.79  87.53  86.89  86.66  86.47  86.33  86.25  86.17  86.14  86.10\n",
      "\t 88.19  87.24  86.89  86.83  86.81  86.51  86.51  86.45  86.39  86.37\n",
      "\t0.0524 0.0457 0.0431 0.0404 0.0385 0.0381 0.0373 0.0366 0.0379 0.0370\n",
      "2\n",
      "\t 93.98  87.94  87.20  86.82  86.60  86.46  86.37  86.27  86.22  86.17\n",
      "\t 87.94  87.02  86.39  86.24  86.15  86.14  85.86  85.92  85.92  85.84\n",
      "\t0.0538 0.0480 0.0442 0.0433 0.0404 0.0407 0.0380 0.0377 0.0376 0.0371\n",
      "3\n",
      "\t 98.06  87.86  86.96  86.56  86.36  86.24  86.13  86.07  86.02  85.98\n",
      "\t 88.19  87.19  86.45  86.15  85.92  85.99  85.91  85.98  85.80  85.73\n",
      "\t0.0549 0.0483 0.0423 0.0409 0.0399 0.0387 0.0374 0.0366 0.0365 0.0358\n",
      "4\n",
      "\t 93.84  87.57  86.84  86.52  86.35  86.23  86.14  86.04  85.98  85.94\n",
      "\t 88.14  87.35  86.71  86.57  86.40  86.37  86.35  86.28  86.27  86.23\n",
      "\t0.0544 0.0467 0.0432 0.0431 0.0402 0.0396 0.0389 0.0370 0.0378 0.0373\n",
      "5\n",
      "\t 93.57  87.69  86.96  86.67  86.49  86.37  86.28  86.18  86.13  86.05\n",
      "\t 88.26  87.65  86.77  86.62  86.34  86.26  86.23  86.32  86.16  86.06\n",
      "\t0.0517 0.0438 0.0428 0.0408 0.0398 0.0385 0.0368 0.0363 0.0359 0.0360\n",
      "6\n",
      "\t 96.00  88.09  87.36  86.93  86.74  86.56  86.46  86.36  86.27  86.22\n",
      "\t 88.68  87.58  87.03  87.08  86.73  86.46  86.41  86.45  86.37  86.39\n",
      "\t0.0547 0.0474 0.0442 0.0427 0.0406 0.0405 0.0387 0.0375 0.0371 0.0369\n",
      "7\n",
      "\t 96.02  88.06  87.29  86.90  86.70  86.52  86.40  86.33  86.29  86.23\n",
      "\t 88.38  87.28  86.66  86.53  86.33  86.16  86.00  86.01  85.95  85.89\n",
      "\t0.0553 0.0466 0.0451 0.0435 0.0419 0.0401 0.0385 0.0387 0.0373 0.0381\n",
      "8\n",
      "\t 95.17  88.19  87.38  86.94  86.76  86.62  86.51  86.42  86.34  86.29\n",
      "\t 88.72  87.63  87.11  86.79  86.57  86.55  86.50  86.43  86.39  86.33\n",
      "\t0.0536 0.0475 0.0435 0.0413 0.0401 0.0391 0.0386 0.0375 0.0365 0.0361\n",
      "9\n",
      "\t 93.90  87.59  86.84  86.48  86.30  86.17  86.10  86.03  85.97  85.92\n",
      "\t 88.72  87.57  87.08  87.11  86.74  86.64  86.72  86.57  86.60  86.36\n",
      "\t0.0535 0.0458 0.0427 0.0422 0.0386 0.0390 0.0369 0.0364 0.0362 0.0361\n",
      "10\n",
      "\t 92.46  87.76  87.13  86.81  86.62  86.46  86.40  86.30  86.25  86.18\n",
      "\t 87.68  86.62  86.25  86.20  86.06  85.96  85.81  85.82  85.75  85.89\n",
      "\t0.0521 0.0475 0.0431 0.0407 0.0390 0.0386 0.0380 0.0368 0.0371 0.0350\n",
      "11\n",
      "\t 92.23  87.65  87.05  86.76  86.59  86.46  86.35  86.28  86.22  86.18\n",
      "\t 87.69  86.78  86.58  86.50  86.26  86.24  86.22  86.08  85.97  85.90\n",
      "\t0.0550 0.0472 0.0430 0.0423 0.0405 0.0397 0.0374 0.0380 0.0374 0.0376\n",
      "12\n",
      "\t 93.90  87.70  86.99  86.76  87.19  86.52  86.41  86.46  86.43  86.28\n",
      "\t 87.84  86.96  86.59  86.27  86.33  86.34  86.12  86.09  86.04  86.20\n",
      "\t0.0527 0.0478 0.0429 0.0418 0.0404 0.0392 0.0380 0.0385 0.0370 0.0361\n",
      "13\n",
      "\t 92.57  87.39  86.78  86.49  86.34  86.24  86.14  86.07  86.00  85.97\n",
      "\t 88.34  87.53  87.20  87.00  86.88  86.67  86.96  86.61  86.54  86.47\n",
      "\t0.0523 0.0452 0.0418 0.0412 0.0398 0.0387 0.0375 0.0380 0.0372 0.0365\n",
      "14\n",
      "\t 92.19  87.55  86.95  86.69  86.52  86.41  86.29  86.24  86.15  86.12\n",
      "\t 87.72  86.98  86.55  86.61  86.33  86.16  86.19  86.04  86.04  86.14\n",
      "\t0.0513 0.0442 0.0420 0.0429 0.0397 0.0398 0.0394 0.0384 0.0373 0.0357\n",
      "16\n",
      "\t 94.52  87.98  87.10  86.73  86.50  86.35  86.23  86.15  86.09  86.03\n",
      "\t 89.19  88.06  87.25  87.06  86.93  86.92  86.86  86.57  86.56  86.59\n",
      "\t0.0516 0.0457 0.0421 0.0407 0.0403 0.0395 0.0377 0.0368 0.0372 0.0364\n",
      "17\n",
      "\t 94.14  87.81  87.00  86.69  86.50  86.36  86.25  86.18  86.10  86.04\n",
      "\t 89.00  87.43  87.05  86.87  86.79  86.71  86.59  86.43  86.43  86.58\n",
      "\t0.0522 0.0460 0.0420 0.0418 0.0391 0.0392 0.0373 0.0373 0.0375 0.0373\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_profile_and_prior_losses(\"SPI1_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/binary_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.3f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"train_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 12\n",
      "\tBest epoch in run: 6\n",
      "\tAssociated value: 0.3960476294159889\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 4:  0.425\n",
      "\tRun 2, epoch 5:  0.439\n",
      "\tRun 3, epoch 5:  0.434\n",
      "\tRun 4, epoch 6:  0.434\n",
      "\tRun 5, epoch 4:  0.441\n",
      "\tRun 6, epoch 5:  0.425\n",
      "\tRun 7, epoch 5:  0.411\n",
      "\tRun 8, epoch 6:  0.403\n",
      "\tRun 9, epoch 6:  0.409\n",
      "\tRun 10, epoch 5:  0.416\n",
      "\tRun 11, epoch 5:  0.423\n",
      "\tRun 12, epoch 6:  0.396\n",
      "\tRun 13, epoch 4:  0.424\n",
      "\tRun 14, epoch 6:  0.428\n",
      "\tRun 15, epoch 5:  0.418\n",
      "\tRun 16, epoch 6:  0.424\n",
      "\tRun 17, epoch 5:  0.426\n",
      "\tRun 18, epoch 4:  0.431\n",
      "\tRun 19, epoch 5:  0.424\n",
      "\tRun 20, epoch 5:  0.422\n",
      "\tRun 21, epoch 6:  0.411\n",
      "\tRun 22, epoch 5:  0.421\n",
      "\tRun 23, epoch 5:  0.419\n",
      "\tRun 24, epoch 6:  0.435\n",
      "\tRun 25, epoch 5:  0.409\n",
      "\tRun 26, epoch 5:  0.441\n",
      "\tRun 27, epoch 5:  0.413\n",
      "\tRun 28, epoch 5:  0.414\n",
      "\tRun 29, epoch 6:  0.445\n",
      "\tRun 30, epoch 6:  0.429\n",
      "\tRun 31, epoch 5:  0.435\n",
      "\tRun 32, epoch 6:  0.418\n",
      "\tRun 33, epoch 6:  0.429\n",
      "\tRun 34, epoch 4:  0.409\n",
      "\tRun 35, epoch 4:  0.443\n",
      "\tRun 36, epoch 4:  0.434\n",
      "\tRun 37, epoch 5:  0.439\n",
      "\tRun 38, epoch 6:  0.427\n",
      "\tRun 39, epoch 5:  0.446\n",
      "\tRun 40, epoch 6:  0.419\n",
      "\tRun 41, epoch 6:  0.410\n",
      "\tRun 42, epoch 5:  0.433\n",
      "\tRun 43, epoch 4:  0.430\n",
      "\tRun 44, epoch 5:  0.414\n",
      "\tRun 45, epoch 5:  0.426\n",
      "\tRun 46, epoch 7:  0.454\n",
      "\tRun 47, epoch 6:  0.439\n",
      "\tRun 48, epoch 6:  0.421\n",
      "\tRun 49, epoch 5:  0.428\n",
      "\tRun 50, epoch 6:  0.434\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.649  0.533  0.436  0.374  0.326  0.291  0.258  0.218  0.191  0.169\n",
      "\t 0.614  0.516  0.456  0.425  0.435  0.441  0.450  0.486  0.548  0.576\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "2\n",
      "\t 0.637  0.521  0.442  0.394  0.349  0.311  0.275  0.240  0.212  0.183\n",
      "\t 0.600  0.496  0.458  0.440  0.439  0.454  0.449  0.495  0.498  0.546\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "3\n",
      "\t 0.647  0.576  0.479  0.414  0.372  0.324  0.285  0.253  0.223  0.201\n",
      "\t 0.660  0.564  0.487  0.462  0.434  0.453  0.474  0.487  0.503  0.529\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "4\n",
      "\t 0.659  0.536  0.442  0.391  0.349  0.326  0.287  0.244  0.217  0.187\n",
      "\t 0.651  0.507  0.453  0.441  0.437  0.434  0.437  0.464  0.471  0.508\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "5\n",
      "\t 0.644  0.522  0.441  0.391  0.355  0.319  0.280  0.254  0.224  0.194\n",
      "\t 0.615  0.505  0.472  0.441  0.449  0.466  0.461  0.499  0.507  0.530\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "6\n",
      "\t 0.655  0.533  0.444  0.388  0.347  0.300  0.263  0.233  0.200  0.176\n",
      "\t 0.616  0.506  0.466  0.431  0.425  0.435  0.469  0.501  0.539  0.569\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "7\n",
      "\t 0.659  0.530  0.432  0.379  0.330  0.297  0.264  0.237  0.204  0.175\n",
      "\t 0.606  0.489  0.434  0.424  0.411  0.426  0.429  0.463  0.501  0.554\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "8\n",
      "\t 0.664  0.543  0.455  0.403  0.362  0.327  0.291  0.256  0.219  0.197\n",
      "\t 0.628  0.499  0.449  0.437  0.427  0.403  0.423  0.451  0.477  0.505\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "9\n",
      "\t 0.651  0.557  0.455  0.400  0.356  0.320  0.285  0.245  0.218  0.188\n",
      "\t 0.623  0.523  0.457  0.422  0.410  0.409  0.419  0.437  0.460  0.508\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "10\n",
      "\t 0.644  0.525  0.437  0.384  0.333  0.295  0.253  0.222  0.189  0.169\n",
      "\t 0.608  0.479  0.441  0.422  0.416  0.421  0.439  0.486  0.528  0.549\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "11\n",
      "\t 0.626  0.491  0.419  0.373  0.334  0.295  0.258  0.226  0.196  0.170\n",
      "\t 0.567  0.484  0.433  0.438  0.423  0.423  0.452  0.472  0.517  0.529\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "12\n",
      "\t 0.640  0.533  0.432  0.382  0.337  0.299  0.268  0.236  0.196  0.166\n",
      "\t 0.616  0.478  0.429  0.405  0.396  0.396  0.445  0.446  0.471  0.533\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "13\n",
      "\t 0.636  0.522  0.436  0.386  0.350  0.314  0.284  0.245  0.211  0.186\n",
      "\t 0.603  0.480  0.449  0.424  0.435  0.442  0.447  0.457  0.523  0.499\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "14\n",
      "\t 0.644  0.538  0.448  0.399  0.359  0.321  0.282  0.249  0.217  0.192\n",
      "\t 0.614  0.498  0.453  0.445  0.441  0.428  0.436  0.466  0.500  0.511\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "15\n",
      "\t 0.629  0.512  0.436  0.385  0.349  0.308  0.272  0.245  0.219  0.190\n",
      "\t 0.584  0.496  0.447  0.433  0.418  0.443  0.449  0.492  0.520  0.555\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "16\n",
      "\t 0.639  0.515  0.436  0.387  0.348  0.303  0.271  0.236  0.207  0.187\n",
      "\t 0.605  0.492  0.457  0.456  0.430  0.424  0.438  0.480  0.504  0.525\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "17\n",
      "\t 0.644  0.517  0.425  0.371  0.330  0.291  0.258  0.228  0.200  0.178\n",
      "\t 0.608  0.490  0.441  0.439  0.426  0.450  0.446  0.496  0.522  0.550\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "18\n",
      "\t 0.649  0.539  0.448  0.390  0.341  0.308  0.266  0.223  0.194  0.169\n",
      "\t 0.622  0.514  0.468  0.431  0.439  0.460  0.461  0.504  0.529  0.576\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "19\n",
      "\t 0.644  0.528  0.435  0.379  0.335  0.299  0.265  0.229  0.204  0.175\n",
      "\t 0.627  0.502  0.450  0.434  0.424  0.429  0.437  0.455  0.486  0.532\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "20\n",
      "\t 0.645  0.527  0.436  0.384  0.340  0.295  0.258  0.224  0.190  0.172\n",
      "\t 0.629  0.502  0.468  0.445  0.422  0.425  0.454  0.493  0.504  0.564\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "21\n",
      "\t 0.653  0.553  0.443  0.390  0.353  0.315  0.272  0.237  0.213  0.184\n",
      "\t 0.633  0.503  0.443  0.424  0.421  0.411  0.437  0.459  0.489  0.493\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "22\n",
      "\t 0.647  0.528  0.431  0.373  0.328  0.287  0.256  0.214  0.189  0.166\n",
      "\t 0.636  0.503  0.447  0.424  0.421  0.432  0.454  0.506  0.554  0.566\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "23\n",
      "\t 0.664  0.535  0.439  0.384  0.345  0.306  0.266  0.233  0.203  0.179\n",
      "\t 0.626  0.494  0.461  0.427  0.419  0.436  0.468  0.472  0.519  0.538\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "24\n",
      "\t 0.650  0.547  0.447  0.398  0.353  0.321  0.282  0.256  0.233  0.197\n",
      "\t 0.635  0.519  0.471  0.447  0.435  0.435  0.448  0.466  0.485  0.516\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "25\n",
      "\t 0.638  0.516  0.432  0.388  0.351  0.312  0.283  0.243  0.212  0.185\n",
      "\t 0.594  0.483  0.430  0.430  0.409  0.413  0.436  0.450  0.492  0.520\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "26\n",
      "\t 0.644  0.560  0.459  0.402  0.353  0.306  0.269  0.242  0.204  0.180\n",
      "\t 0.634  0.534  0.481  0.451  0.441  0.445  0.464  0.453  0.507  0.551\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "27\n",
      "\t 0.635  0.514  0.434  0.382  0.339  0.293  0.260  0.228  0.202  0.173\n",
      "\t 0.604  0.489  0.460  0.437  0.413  0.430  0.441  0.483  0.512  0.580\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "28\n",
      "\t 0.643  0.520  0.427  0.376  0.332  0.287  0.254  0.224  0.192  0.168\n",
      "\t 0.604  0.488  0.451  0.430  0.414  0.429  0.448  0.480  0.489  0.522\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "29\n",
      "\t 0.645  0.544  0.459  0.412  0.375  0.344  0.307  0.273  0.242  0.212\n",
      "\t 0.634  0.515  0.476  0.445  0.454  0.445  0.448  0.469  0.476  0.520\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "30\n",
      "\t 0.651  0.552  0.453  0.400  0.355  0.319  0.282  0.254  0.219  0.193\n",
      "\t 0.628  0.519  0.461  0.446  0.434  0.429  0.432  0.471  0.481  0.525\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "31\n",
      "\t 0.635  0.515  0.432  0.381  0.337  0.299  0.260  0.231  0.205  0.176\n",
      "\t 0.583  0.480  0.447  0.435  0.435  0.438  0.463  0.489  0.502  0.564\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "32\n",
      "\t 0.650  0.560  0.455  0.397  0.357  0.309  0.274  0.241  0.208  0.190\n",
      "\t 0.656  0.525  0.466  0.432  0.421  0.418  0.432  0.447  0.473  0.488\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "33\n",
      "\t 0.656  0.543  0.451  0.393  0.347  0.312  0.271  0.238  0.201  0.181\n",
      "\t 0.628  0.509  0.449  0.431  0.429  0.429  0.457  0.472  0.517  0.565\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "34\n",
      "\t 0.645  0.527  0.431  0.382  0.341  0.305  0.275  0.266  0.214  0.185\n",
      "\t 0.615  0.472  0.445  0.409  0.426  0.423  0.455  0.464  0.501  0.549\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "35\n",
      "\t 0.640  0.531  0.448  0.391  0.347  0.310  0.277  0.239  0.218  0.190\n",
      "\t 0.635  0.520  0.468  0.443  0.446  0.450  0.475  0.515  0.538  0.555\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "36\n",
      "\t 0.643  0.520  0.446  0.394  0.344  0.305  0.275  0.233  0.200  0.177\n",
      "\t 0.610  0.494  0.466  0.434  0.435  0.442  0.453  0.460  0.543  0.546\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "37\n",
      "\t 0.650  0.562  0.459  0.401  0.365  0.318  0.284  0.250  0.220  0.190\n",
      "\t 0.637  0.539  0.463  0.450  0.439  0.456  0.482  0.501  0.542  0.569\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "38\n",
      "\t 0.636  0.548  0.460  0.408  0.364  0.327  0.298  0.262  0.237  0.211\n",
      "\t 0.629  0.548  0.474  0.456  0.453  0.427  0.453  0.475  0.516  0.541\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "39\n",
      "\t 0.643  0.538  0.448  0.398  0.352  0.310  0.271  0.242  0.210  0.188\n",
      "\t 0.634  0.519  0.482  0.452  0.446  0.454  0.475  0.532  0.530  0.579\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "40\n",
      "\t 0.644  0.535  0.450  0.398  0.359  0.321  0.285  0.255  0.222  0.187\n",
      "\t 0.626  0.519  0.459  0.436  0.421  0.419  0.431  0.443  0.482  0.516\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "41\n",
      "\t 0.656  0.532  0.446  0.386  0.336  0.298  0.264  0.231  0.207  0.179\n",
      "\t 0.614  0.495  0.452  0.429  0.419  0.410  0.433  0.462  0.497  0.519\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "42\n",
      "\t 0.640  0.526  0.442  0.396  0.346  0.305  0.266  0.238  0.208  0.185\n",
      "\t 0.621  0.496  0.469  0.447  0.433  0.439  0.460  0.520  0.524  0.551\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "43\n",
      "\t 0.633  0.508  0.423  0.377  0.331  0.301  0.265  0.228  0.203  0.182\n",
      "\t 0.596  0.486  0.452  0.430  0.433  0.455  0.458  0.504  0.507  0.541\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "44\n",
      "\t 0.642  0.509  0.421  0.374  0.345  0.313  0.279  0.248  0.217  0.198\n",
      "\t 0.607  0.477  0.436  0.431  0.414  0.417  0.434  0.437  0.458  0.496\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "45\n",
      "\t 0.654  0.535  0.435  0.384  0.342  0.303  0.265  0.236  0.207  0.223\n",
      "\t 0.629  0.501  0.452  0.435  0.426  0.437  0.474  0.527  0.515  0.540\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "46\n",
      "\t 0.646  0.538  0.444  0.396  0.359  0.322  0.285  0.250  0.218  0.193\n",
      "\t 0.629  0.521  0.483  0.464  0.454  0.457  0.454  0.480  0.516  0.528\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "47\n",
      "\t 0.650  0.541  0.450  0.389  0.343  0.305  0.270  0.235  0.205  0.178\n",
      "\t 0.605  0.502  0.467  0.450  0.451  0.439  0.479  0.490  0.541  0.589\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "48\n",
      "\t 0.646  0.529  0.438  0.387  0.342  0.307  0.268  0.243  0.204  0.174\n",
      "\t 0.606  0.488  0.467  0.429  0.429  0.421  0.449  0.473  0.488  0.520\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "49\n",
      "\t 0.643  0.527  0.435  0.375  0.329  0.288  0.253  0.221  0.190  0.167\n",
      "\t 0.613  0.497  0.447  0.429  0.428  0.441  0.488  0.511  0.555  0.584\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "50\n",
      "\t 0.649  0.575  0.467  0.401  0.355  0.312  0.276  0.245  0.210  0.186\n",
      "\t 0.628  0.558  0.492  0.442  0.445  0.434  0.452  0.496  0.516  0.549\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_and_prior_losses(\"SPI1_keep1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 12\n",
      "\tBest epoch in run: 24\n",
      "\tAssociated value: 0.34362347424030304\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 30:  0.377\n",
      "\tRun 2, epoch 12:  0.400\n",
      "\tRun 3, epoch 27:  0.413\n",
      "\tRun 4, epoch 29:  0.395\n",
      "\tRun 5, epoch 20:  0.383\n",
      "\tRun 6, epoch 22:  0.444\n",
      "\tRun 7, epoch 10:  0.387\n",
      "\tRun 8, epoch 15:  0.390\n",
      "\tRun 9, epoch 14:  0.402\n",
      "\tRun 10, epoch 16:  0.373\n",
      "\tRun 11, epoch 8:  0.396\n",
      "\tRun 12, epoch 24:  0.344\n",
      "\tRun 13, epoch 12:  0.427\n",
      "\tRun 14, epoch 16:  0.348\n",
      "\tRun 15, epoch 17:  0.365\n",
      "\tRun 16, epoch 19:  0.389\n",
      "\tRun 17, epoch 22:  0.395\n",
      "\tRun 18, epoch 9:  0.398\n",
      "\tRun 19, epoch 16:  0.380\n",
      "\tRun 20, epoch 16:  0.399\n",
      "\tRun 21, epoch 18:  0.356\n",
      "\tRun 22, epoch 10:  0.417\n",
      "\tRun 23, epoch 21:  0.400\n",
      "\tRun 24, epoch 19:  0.406\n",
      "\tRun 25, epoch 26:  0.385\n",
      "\tRun 26, epoch 9:  0.406\n",
      "\tRun 27, epoch 10:  0.371\n",
      "\tRun 28, epoch 12:  0.394\n",
      "\tRun 29, epoch 13:  0.431\n",
      "\tRun 30, epoch 9:  0.395\n",
      "\tRun 31, epoch 19:  0.398\n",
      "\tRun 32, epoch 19:  0.398\n",
      "\tRun 33, epoch 17:  0.412\n",
      "\tRun 34, epoch 8:  0.399\n",
      "\tRun 35, epoch 18:  0.417\n",
      "\tRun 36, epoch 19:  0.409\n",
      "\tRun 37, epoch 11:  0.393\n",
      "\tRun 38, epoch 13:  0.398\n",
      "\tRun 39, epoch 27:  0.391\n",
      "\tRun 40, epoch 13:  0.385\n",
      "\tRun 41, epoch 10:  0.393\n",
      "\tRun 42, epoch 22:  0.398\n",
      "\tRun 43, epoch 23:  0.377\n",
      "\tRun 44, epoch 22:  0.409\n",
      "\tRun 45, epoch 16:  0.400\n",
      "\tRun 46, epoch 11:  0.394\n",
      "\tRun 47, epoch 13:  0.402\n",
      "\tRun 48, epoch 11:  0.379\n",
      "\tRun 49, epoch 8:  0.389\n",
      "\tRun 50, epoch 24:  0.390\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.671  0.650  0.638  0.631  0.620  0.616  0.610  0.585  0.527  0.480  0.459  0.434  0.421  0.397  0.381  0.378  0.363  0.359  0.351  0.349  0.337  0.333  0.330  0.333  0.349  0.335  0.342  0.326  0.325  0.319\n",
      "\t 0.670  0.654  0.651  0.638  0.641  0.628  0.619  0.573  0.508  0.482  0.460  0.446  0.417  0.394  0.396  0.401  0.389  0.391  0.384  0.383  0.389  0.403  0.394  0.403  0.386  0.384  0.398  0.388  0.400  0.377\n",
      "\t 0.127  0.118  0.086  0.085  0.085  0.074  0.074  0.077  0.062  0.065  0.084  0.082  0.075  0.071  0.069  0.068  0.066  0.062  0.060  0.059  0.055  0.052  0.052  0.066  0.061  0.069  0.064  0.060  0.057  0.062\n",
      "2\n",
      "\t 0.661  0.621  0.558  0.493  0.456  0.429  0.418  0.405  0.398  0.377  0.364  0.346  0.331  0.320  0.324  0.306  0.293  0.273  0.247  0.241  0.240  0.237  0.231  0.210  0.200  0.178  0.168  0.170  0.173  0.181\n",
      "\t 0.656  0.617  0.529  0.490  0.467  0.439  0.431  0.415  0.413  0.412  0.401  0.400  0.401  0.442  0.403  0.409  0.427  0.418  0.431  0.459  0.483  0.496  0.469  0.512  0.532  0.537  0.576  0.583  0.569  0.555\n",
      "\t 0.134  0.102  0.083  0.061  0.074  0.079  0.125  0.075  0.079  0.071  0.062  0.059  0.057  0.067  0.067  0.089  0.064  0.061  0.050  0.088  0.071  0.080  0.077  0.075  0.066  0.057  0.053  0.057  0.083  0.071\n",
      "3\n",
      "\t 0.664  0.627  0.572  0.506  0.458  0.437  0.419  0.413  0.394  0.384  0.383  0.378  0.424  0.395  0.387  0.379  0.372  0.365  0.356  0.426  0.436  0.405  0.397  0.393  0.384  0.377  0.374  0.366  0.360  0.361\n",
      "\t 0.663  0.635  0.550  0.496  0.479  0.454  0.456  0.459  0.442  0.431  0.430  0.433  0.433  0.424  0.422  0.432  0.425  0.416  0.429  0.497  0.473  0.439  0.449  0.435  0.418  0.424  0.413  0.422  0.418  0.418\n",
      "\t 0.145  0.117  0.077  0.062  0.063  0.083  0.074  0.112  0.085  0.112  0.082  0.078  0.096  0.096  0.093  0.092  0.085  0.081  0.074  0.095  0.112  0.095  0.072  0.069  0.071  0.069  0.068  0.064  0.066  0.067\n",
      "4\n",
      "\t 0.653  0.596  0.511  0.456  0.433  0.407  0.394  0.379  0.358  0.337  0.321  0.335  0.439  0.380  0.349  0.348  0.325  0.436  0.402  0.378  0.344  0.452  0.435  0.399  0.371  0.348  0.342  0.336  0.325  0.421\n",
      "\t 0.637  0.562  0.495  0.466  0.445  0.429  0.427  0.448  0.430  0.431  0.423  0.621  0.460  0.444  0.429  0.432  0.432  0.458  0.493  0.430  0.429  0.527  0.479  0.443  0.421  0.400  0.406  0.401  0.395  0.597\n",
      "\t 0.135  0.115  0.088  0.088  0.081  0.107  0.105  0.095  0.103  0.078  0.067  0.120  0.106  0.101  0.090  0.088  0.088  0.093  0.116  0.088  0.086  0.104  0.089  0.082  0.080  0.076  0.071  0.074  0.068  0.109\n",
      "5\n",
      "\t 0.647  0.626  0.607  0.554  0.491  0.450  0.428  0.421  0.414  0.385  0.366  0.359  0.343  0.376  0.335  0.312  0.315  0.381  0.362  0.335  0.324  0.307  0.296  0.284  0.264  0.261  0.246  0.237  0.226  0.224\n",
      "\t 0.658  0.642  0.607  0.542  0.494  0.483  0.443  0.487  0.424  0.416  0.415  0.413  0.402  0.410  0.392  0.393  0.437  0.402  0.402  0.383  0.387  0.384  0.396  0.400  0.403  0.412  0.425  0.448  0.469  0.452\n",
      "\t 0.131  0.100  0.086  0.076  0.070  0.063  0.080  0.121  0.109  0.098  0.085  0.082  0.076  0.102  0.092  0.086  0.094  0.087  0.078  0.084  0.078  0.080  0.071  0.081  0.066  0.071  0.079  0.078  0.095  0.090\n",
      "6\n",
      "\t 0.669  0.623  0.548  0.495  0.483  0.453  0.431  0.420  0.408  0.396  0.384  0.373  0.361  0.354  0.346  0.542  0.485  0.434  0.408  0.394  0.383  0.372  0.362  0.350  0.341  0.332  0.323  0.310  0.301  0.290\n",
      "\t 0.670  0.606  0.523  0.541  0.518  0.484  0.481  0.477  0.477  0.467  0.467  0.469  0.478  0.490  0.476  0.574  0.498  0.467  0.468  0.459  0.454  0.444  0.461  0.464  0.455  0.474  0.462  0.463  0.481  0.483\n",
      "\t 0.136  0.090  0.100  0.147  0.118  0.109  0.098  0.094  0.081  0.109  0.112  0.108  0.103  0.086  0.104  0.148  0.130  0.110  0.099  0.095  0.094  0.099  0.101  0.091  0.091  0.091  0.089  0.105  0.092  0.084\n",
      "7\n",
      "\t 0.663  0.625  0.570  0.514  0.464  0.432  0.414  0.395  0.381  0.352  0.346  0.325  0.307  0.303  0.305  0.288  0.262  0.243  0.224  0.220  0.249  0.231  0.213  0.198  0.185  0.223  0.188  0.178  0.168  0.162\n",
      "\t 0.648  0.615  0.559  0.509  0.475  0.461  0.448  0.441  0.416  0.387  0.391  0.405  0.426  0.421  0.427  0.423  0.414  0.456  0.470  0.472  0.447  0.495  0.464  0.510  0.546  0.510  0.543  0.572  0.545  0.575\n",
      "\t 0.125  0.092  0.089  0.069  0.061  0.068  0.072  0.122  0.111  0.094  0.114  0.097  0.076  0.072  0.106  0.089  0.067  0.057  0.058  0.091  0.116  0.071  0.096  0.077  0.087  0.083  0.075  0.105  0.092  0.088\n",
      "8\n",
      "\t 0.667  0.641  0.628  0.595  0.540  0.494  0.456  0.432  0.404  0.379  0.363  0.349  0.333  0.322  0.304  0.298  0.287  0.270  0.265  0.259  0.247  0.231  0.223  0.227  0.208  0.208  0.199  0.182  0.178  0.184\n",
      "\t 0.665  0.651  0.654  0.580  0.555  0.511  0.480  0.463  0.429  0.436  0.417  0.400  0.407  0.399  0.390  0.416  0.415  0.417  0.434  0.455  0.453  0.476  0.481  0.496  0.495  0.473  0.502  0.496  0.523  0.554\n",
      "\t 0.159  0.117  0.100  0.066  0.073  0.090  0.106  0.089  0.076  0.070  0.063  0.072  0.068  0.061  0.057  0.075  0.067  0.064  0.075  0.076  0.077  0.067  0.079  0.075  0.070  0.071  0.061  0.061  0.083  0.067\n",
      "9\n",
      "\t 0.661  0.639  0.619  0.582  0.517  0.469  0.429  0.400  0.383  0.363  0.389  0.359  0.347  0.322  0.301  0.290  0.275  0.305  0.386  0.317  0.289  0.271  0.261  0.252  0.331  0.277  0.261  0.242  0.232  0.219\n",
      "\t 0.662  0.645  0.624  0.575  0.504  0.485  0.463  0.447  0.437  0.454  0.422  0.431  0.449  0.402  0.419  0.434  0.421  0.566  0.452  0.420  0.442  0.452  0.449  0.499  0.430  0.438  0.435  0.462  0.454  0.507\n",
      "\t 0.171  0.105  0.102  0.073  0.063  0.065  0.066  0.073  0.089  0.121  0.096  0.091  0.092  0.085  0.078  0.089  0.082  0.121  0.105  0.090  0.105  0.081  0.078  0.098  0.082  0.080  0.079  0.073  0.086  0.072\n",
      "10\n",
      "\t 0.658  0.587  0.500  0.452  0.428  0.408  0.391  0.371  0.360  0.341  0.328  0.309  0.292  0.386  0.347  0.326  0.300  0.295  0.312  0.272  0.262  0.263  0.287  0.283  0.239  0.226  0.224  0.204  0.234  0.227\n",
      "\t 0.642  0.544  0.471  0.464  0.420  0.420  0.396  0.405  0.384  0.387  0.384  0.405  0.387  0.424  0.398  0.373  0.404  0.501  0.399  0.406  0.454  0.452  0.474  0.433  0.425  0.468  0.451  0.479  0.503  0.497\n",
      "\t 0.119  0.083  0.079  0.136  0.102  0.118  0.085  0.079  0.073  0.070  0.063  0.060  0.056  0.134  0.103  0.091  0.084  0.101  0.077  0.070  0.106  0.081  0.089  0.070  0.069  0.081  0.073  0.064  0.076  0.097\n",
      "11\n",
      "\t 0.664  0.580  0.485  0.445  0.412  0.390  0.385  0.360  0.340  0.322  0.309  0.300  0.278  0.287  0.276  0.265  0.253  0.244  0.221  0.366  0.323  0.281  0.267  0.244  0.222  0.216  0.207  0.182  0.170  0.163\n",
      "\t 0.634  0.535  0.475  0.441  0.422  0.419  0.409  0.396  0.416  0.415  0.436  0.429  0.431  0.447  0.439  0.481  0.476  0.492  0.509  0.521  0.466  0.586  0.533  0.597  0.550  0.544  0.593  0.641  0.634  0.612\n",
      "\t 0.125  0.083  0.069  0.123  0.081  0.136  0.093  0.082  0.085  0.065  0.086  0.077  0.078  0.089  0.087  0.098  0.113  0.093  0.096  0.124  0.100  0.107  0.104  0.105  0.091  0.092  0.086  0.084  0.084  0.073\n",
      "12\n",
      "\t 0.661  0.631  0.621  0.574  0.532  0.479  0.441  0.410  0.393  0.389  0.376  0.372  0.370  0.366  0.362  0.365  0.355  0.356  0.347  0.334  0.322  0.310  0.302  0.320  0.297  0.291  0.284  0.281  0.270  0.264\n",
      "\t 0.664  0.653  0.608  0.553  0.515  0.448  0.423  0.387  0.406  0.374  0.371  0.385  0.369  0.383  0.379  0.377  0.377  0.363  0.366  0.356  0.357  0.356  0.347  0.344  0.365  0.356  0.371  0.360  0.361  0.368\n",
      "\t 0.152  0.141  0.097  0.094  0.080  0.074  0.067  0.081  0.081  0.088  0.079  0.072  0.073  0.078  0.073  0.084  0.079  0.095  0.081  0.079  0.071  0.073  0.076  0.071  0.064  0.080  0.078  0.072  0.066  0.069\n",
      "13\n",
      "\t 0.659  0.628  0.572  0.492  0.438  0.409  0.427  0.430  0.402  0.379  0.362  0.348  0.336  0.320  0.312  0.296  0.281  0.271  0.255  0.239  0.225  0.212  0.205  0.192  0.180  0.167  0.161  0.156  0.148  0.144\n",
      "\t 0.658  0.633  0.549  0.491  0.461  0.439  0.513  0.475  0.427  0.439  0.432  0.427  0.429  0.434  0.450  0.448  0.459  0.452  0.476  0.497  0.521  0.533  0.558  0.558  0.598  0.654  0.654  0.667  0.704  0.692\n",
      "\t 0.124  0.098  0.076  0.067  0.080  0.087  0.157  0.134  0.123  0.120  0.107  0.097  0.083  0.077  0.072  0.069  0.076  0.064  0.072  0.066  0.058  0.062  0.057  0.057  0.057  0.054  0.053  0.053  0.053  0.056\n",
      "14\n",
      "\t 0.642  0.545  0.469  0.432  0.398  0.375  0.363  0.345  0.329  0.320  0.298  0.342  0.385  0.338  0.322  0.311  0.307  0.297  0.325  0.325  0.299  0.276  0.268  0.253  0.254  0.238  0.228  0.223  0.240  0.217\n",
      "\t 0.610  0.502  0.464  0.421  0.400  0.393  0.376  0.374  0.352  0.355  0.348  0.467  0.388  0.377  0.362  0.348  0.361  0.396  0.386  0.369  0.359  0.385  0.370  0.383  0.397  0.388  0.410  0.418  0.411  0.438\n",
      "\t 0.139  0.100  0.112  0.071  0.072  0.066  0.062  0.078  0.066  0.067  0.066  0.087  0.069  0.066  0.065  0.066  0.064  0.064  0.078  0.064  0.065  0.071  0.061  0.065  0.065  0.061  0.063  0.060  0.069  0.068\n",
      "15\n",
      "\t 0.663  0.621  0.550  0.486  0.448  0.422  0.396  0.374  0.353  0.335  0.328  0.450  0.443  0.398  0.369  0.347  0.331  0.317  0.307  0.301  0.288  0.278  0.270  0.259  0.247  0.446  0.406  0.363  0.337  0.322\n",
      "\t 0.650  0.613  0.510  0.484  0.455  0.444  0.447  0.405  0.396  0.427  0.414  0.660  0.436  0.431  0.401  0.377  0.365  0.382  0.387  0.390  0.383  0.393  0.394  0.403  0.424  0.498  0.429  0.394  0.392  0.390\n",
      "\t 0.146  0.109  0.076  0.083  0.082  0.121  0.087  0.092  0.073  0.095  0.117  0.119  0.093  0.086  0.081  0.076  0.075  0.071  0.071  0.070  0.072  0.068  0.076  0.076  0.074  0.088  0.074  0.068  0.070  0.066\n",
      "16\n",
      "\t 0.665  0.636  0.615  0.564  0.508  0.469  0.448  0.418  0.393  0.383  0.373  0.365  0.366  0.357  0.350  0.347  0.340  0.327  0.323  0.400  0.328  0.305  0.290  0.280  0.269  0.262  0.360  0.294  0.271  0.252\n",
      "\t 0.660  0.659  0.618  0.561  0.515  0.485  0.464  0.448  0.409  0.421  0.419  0.412  0.418  0.405  0.410  0.404  0.408  0.413  0.389  0.405  0.400  0.408  0.404  0.405  0.420  0.428  0.411  0.393  0.403  0.441\n",
      "\t 0.144  0.102  0.084  0.082  0.078  0.093  0.099  0.077  0.071  0.064  0.066  0.059  0.070  0.079  0.073  0.069  0.074  0.062  0.075  0.084  0.075  0.065  0.065  0.065  0.068  0.074  0.084  0.077  0.066  0.071\n",
      "17\n",
      "\t 0.656  0.616  0.566  0.496  0.446  0.431  0.416  0.391  0.382  0.364  0.362  0.374  0.433  0.400  0.384  0.370  0.365  0.357  0.354  0.354  0.353  0.361  0.342  0.337  0.338  0.334  0.334  0.335  0.322  0.316\n",
      "\t 0.662  0.614  0.559  0.501  0.497  0.471  0.432  0.414  0.404  0.406  0.403  0.478  0.464  0.408  0.411  0.421  0.418  0.401  0.423  0.415  0.407  0.395  0.406  0.405  0.414  0.411  0.409  0.416  0.416  0.413\n",
      "\t 0.149  0.097  0.085  0.061  0.082  0.083  0.087  0.071  0.065  0.056  0.063  0.094  0.103  0.090  0.079  0.080  0.074  0.073  0.078  0.073  0.072  0.078  0.068  0.066  0.068  0.066  0.074  0.069  0.072  0.070\n",
      "18\n",
      "\t 0.664  0.624  0.577  0.515  0.470  0.440  0.409  0.403  0.359  0.339  0.329  0.304  0.292  0.269  0.255  0.244  0.228  0.210  0.201  0.227  0.224  0.197  0.167  0.157  0.170  0.192  0.161  0.141  0.171  0.204\n",
      "\t 0.665  0.638  0.563  0.517  0.479  0.473  0.508  0.414  0.398  0.406  0.404  0.408  0.407  0.422  0.442  0.435  0.462  0.487  0.488  0.485  0.510  0.508  0.514  0.546  0.611  0.587  0.560  0.603  0.611  0.612\n",
      "\t 0.118  0.093  0.075  0.088  0.096  0.082  0.098  0.080  0.073  0.060  0.074  0.061  0.058  0.055  0.053  0.051  0.052  0.051  0.052  0.070  0.088  0.066  0.058  0.057  0.100  0.076  0.061  0.056  0.077  0.088\n",
      "19\n",
      "\t 0.646  0.614  0.556  0.491  0.461  0.436  0.417  0.394  0.376  0.365  0.354  0.362  0.342  0.321  0.299  0.301  0.279  0.259  0.252  0.252  0.243  0.229  0.219  0.201  0.185  0.261  0.253  0.218  0.214  0.196\n",
      "\t 0.654  0.633  0.531  0.481  0.479  0.449  0.424  0.421  0.388  0.390  0.398  0.385  0.386  0.380  0.398  0.380  0.386  0.400  0.421  0.413  0.428  0.445  0.444  0.465  0.492  0.453  0.431  0.455  0.466  0.494\n",
      "\t 0.147  0.108  0.094  0.101  0.109  0.105  0.087  0.074  0.061  0.061  0.085  0.075  0.069  0.054  0.072  0.081  0.067  0.060  0.080  0.073  0.061  0.083  0.065  0.064  0.058  0.079  0.086  0.080  0.073  0.074\n",
      "20\n",
      "\t 0.663  0.632  0.604  0.539  0.484  0.473  0.467  0.448  0.415  0.437  0.427  0.403  0.392  0.388  0.385  0.377  0.442  0.451  0.418  0.402  0.392  0.379  0.388  0.393  0.369  0.356  0.348  0.341  0.328  0.322\n",
      "\t 0.653  0.650  0.588  0.525  0.492  0.486  0.502  0.444  0.430  0.478  0.419  0.422  0.405  0.406  0.407  0.399  0.531  0.456  0.448  0.436  0.416  0.425  0.453  0.411  0.407  0.405  0.412  0.403  0.411  0.401\n",
      "\t 0.133  0.096  0.079  0.075  0.123  0.118  0.109  0.082  0.076  0.093  0.087  0.081  0.074  0.077  0.083  0.076  0.100  0.091  0.091  0.093  0.089  0.084  0.108  0.087  0.087  0.080  0.079  0.076  0.076  0.079\n",
      "21\n",
      "\t 0.668  0.641  0.627  0.623  0.609  0.580  0.534  0.487  0.431  0.407  0.408  0.420  0.379  0.393  0.378  0.371  0.357  0.357  0.350  0.349  0.342  0.337  0.329  0.539  0.588  0.552  0.531  0.535  0.550  0.532\n",
      "\t 0.667  0.645  0.633  0.629  0.609  0.555  0.535  0.440  0.411  0.389  0.394  0.386  0.384  0.390  0.384  0.373  0.364  0.356  0.367  0.368  0.366  0.363  0.358  0.641  0.629  0.572  0.557  0.596  0.545  0.572\n",
      "\t 0.153  0.111  0.106  0.067  0.058  0.075  0.119  0.071  0.063  0.059  0.070  0.067  0.056  0.053  0.066  0.056  0.054  0.054  0.058  0.053  0.056  0.056  0.052  0.108  0.088  0.085  0.091  0.089  0.107  0.094\n",
      "22\n",
      "\t 0.644  0.593  0.512  0.468  0.438  0.406  0.392  0.372  0.353  0.342  0.318  0.292  0.275  0.280  0.248  0.273  0.302  0.243  0.220  0.196  0.180  0.167  0.155  0.150  0.136  0.128  0.118  0.117  0.119  0.124\n",
      "\t 0.622  0.555  0.479  0.467  0.445  0.442  0.447  0.420  0.429  0.417  0.434  0.430  0.439  0.429  0.476  0.475  0.481  0.493  0.501  0.531  0.558  0.560  0.605  0.627  0.628  0.699  0.686  0.705  0.689  0.729\n",
      "\t 0.139  0.100  0.093  0.088  0.075  0.074  0.065  0.068  0.058  0.063  0.059  0.062  0.060  0.070  0.058  0.086  0.077  0.075  0.063  0.057  0.055  0.055  0.053  0.054  0.051  0.051  0.050  0.055  0.052  0.051\n",
      "23\n",
      "\t 0.662  0.625  0.570  0.519  0.514  0.489  0.471  0.443  0.418  0.409  0.393  0.390  0.381  0.374  0.362  0.362  0.355  0.345  0.346  0.347  0.323  0.380  0.414  0.371  0.348  0.328  0.310  0.289  0.275  0.271\n",
      "\t 0.656  0.632  0.567  0.528  0.518  0.499  0.527  0.472  0.458  0.418  0.425  0.422  0.415  0.412  0.415  0.418  0.402  0.414  0.463  0.407  0.400  0.530  0.467  0.449  0.460  0.434  0.438  0.465  0.480  0.462\n",
      "\t 0.125  0.083  0.080  0.097  0.104  0.092  0.096  0.088  0.076  0.072  0.068  0.068  0.067  0.071  0.064  0.065  0.065  0.061  0.064  0.064  0.061  0.124  0.076  0.071  0.072  0.074  0.078  0.072  0.084  0.092\n",
      "24\n",
      "\t 0.660  0.616  0.538  0.477  0.449  0.437  0.411  0.407  0.394  0.380  0.377  0.379  0.417  0.384  0.373  0.367  0.363  0.361  0.350  0.340  0.335  0.325  0.313  0.304  0.293  0.276  0.266  0.251  0.237  0.266\n",
      "\t 0.662  0.619  0.522  0.471  0.475  0.453  0.445  0.438  0.431  0.424  0.412  0.428  0.421  0.417  0.419  0.428  0.447  0.421  0.406  0.406  0.415  0.417  0.407  0.422  0.436  0.441  0.439  0.460  0.453  0.501\n",
      "\t 0.130  0.095  0.074  0.086  0.101  0.104  0.105  0.102  0.092  0.079  0.085  0.089  0.084  0.088  0.081  0.077  0.087  0.074  0.070  0.078  0.084  0.093  0.082  0.077  0.073  0.073  0.069  0.069  0.075  0.079\n",
      "25\n",
      "\t 0.651  0.588  0.510  0.461  0.453  0.426  0.399  0.383  0.368  0.363  0.357  0.345  0.345  0.328  0.323  0.319  0.304  0.296  0.292  0.264  0.286  0.271  0.338  0.338  0.295  0.269  0.253  0.236  0.229  0.215\n",
      "\t 0.640  0.558  0.494  0.470  0.456  0.442  0.415  0.413  0.405  0.406  0.411  0.399  0.404  0.416  0.415  0.399  0.415  0.439  0.406  0.439  0.429  0.432  0.439  0.390  0.396  0.385  0.405  0.419  0.442  0.445\n",
      "\t 0.165  0.084  0.082  0.088  0.124  0.108  0.088  0.075  0.075  0.077  0.071  0.077  0.064  0.062  0.073  0.066  0.070  0.076  0.065  0.071  0.087  0.079  0.076  0.075  0.070  0.072  0.060  0.064  0.070  0.066\n",
      "26\n",
      "\t 0.649  0.620  0.590  0.530  0.482  0.451  0.471  0.414  0.392  0.405  0.380  0.365  0.355  0.363  0.357  0.348  0.338  0.328  0.323  0.314  0.298  0.288  0.291  0.277  0.260  0.240  0.233  0.214  0.206  0.192\n",
      "\t 0.660  0.644  0.586  0.517  0.502  0.478  0.468  0.430  0.406  0.410  0.414  0.414  0.437  0.420  0.432  0.406  0.427  0.419  0.432  0.447  0.433  0.445  0.438  0.420  0.436  0.462  0.469  0.524  0.524  0.558\n",
      "\t 0.139  0.098  0.089  0.070  0.106  0.110  0.091  0.074  0.093  0.086  0.077  0.072  0.085  0.077  0.068  0.066  0.064  0.059  0.059  0.058  0.054  0.065  0.087  0.066  0.060  0.059  0.059  0.060  0.057  0.054\n",
      "27\n",
      "\t 0.663  0.623  0.552  0.486  0.440  0.408  0.398  0.387  0.366  0.340  0.319  0.301  0.315  0.297  0.270  0.255  0.343  0.547  0.396  0.343  0.310  0.280  0.267  0.270  0.253  0.250  0.249  0.222  0.216  0.201\n",
      "\t 0.653  0.604  0.522  0.471  0.431  0.417  0.417  0.399  0.378  0.371  0.377  0.414  0.374  0.392  0.377  0.379  0.616  0.473  0.425  0.403  0.381  0.397  0.412  0.424  0.427  0.450  0.421  0.461  0.460  0.487\n",
      "\t 0.109  0.093  0.066  0.056  0.062  0.080  0.126  0.101  0.090  0.083  0.080  0.071  0.088  0.085  0.076  0.070  0.095  0.112  0.093  0.087  0.077  0.070  0.075  0.080  0.076  0.082  0.073  0.074  0.076  0.074\n",
      "28\n",
      "\t 0.668  0.636  0.578  0.499  0.460  0.438  0.415  0.387  0.380  0.363  0.353  0.347  0.342  0.331  0.304  0.288  0.271  0.258  0.240  0.226  0.214  0.198  0.198  0.209  0.188  0.172  0.156  0.149  0.145  0.130\n",
      "\t 0.671  0.638  0.564  0.492  0.480  0.472  0.429  0.422  0.427  0.400  0.405  0.394  0.430  0.402  0.399  0.403  0.429  0.433  0.470  0.474  0.488  0.485  0.539  0.523  0.511  0.545  0.559  0.597  0.612  0.647\n",
      "\t 0.119  0.081  0.073  0.088  0.095  0.105  0.077  0.070  0.078  0.066  0.064  0.055  0.083  0.065  0.060  0.061  0.054  0.058  0.051  0.051  0.049  0.053  0.095  0.076  0.067  0.064  0.063  0.061  0.061  0.052\n",
      "29\n",
      "\t 0.678  0.636  0.575  0.505  0.452  0.480  0.433  0.409  0.388  0.379  0.369  0.349  0.336  0.329  0.306  0.299  0.275  0.294  0.276  0.252  0.226  0.234  0.245  0.233  0.222  0.207  0.195  0.181  0.174  0.199\n",
      "\t 0.675  0.623  0.553  0.498  0.484  0.478  0.469  0.449  0.449  0.445  0.453  0.432  0.431  0.440  0.436  0.444  0.444  0.452  0.471  0.495  0.488  0.550  0.521  0.549  0.556  0.560  0.587  0.592  0.642  0.609\n",
      "\t 0.126  0.104  0.089  0.071  0.073  0.127  0.111  0.078  0.084  0.078  0.089  0.079  0.103  0.084  0.081  0.092  0.079  0.077  0.084  0.082  0.083  0.077  0.107  0.103  0.111  0.099  0.092  0.087  0.103  0.103\n",
      "30\n",
      "\t 0.659  0.631  0.584  0.515  0.450  0.397  0.381  0.361  0.334  0.323  0.306  0.292  0.273  0.265  0.257  0.240  0.234  0.215  0.205  0.194  0.214  0.255  0.213  0.192  0.173  0.174  0.194  0.204  0.165  0.156\n",
      "\t 0.658  0.639  0.574  0.520  0.440  0.421  0.437  0.398  0.395  0.414  0.423  0.431  0.425  0.434  0.443  0.455  0.478  0.495  0.507  0.550  0.535  0.510  0.552  0.576  0.571  0.594  0.622  0.590  0.607  0.657\n",
      "\t 0.140  0.090  0.076  0.122  0.084  0.059  0.094  0.068  0.059  0.061  0.095  0.075  0.067  0.075  0.067  0.063  0.059  0.057  0.058  0.067  0.078  0.094  0.081  0.074  0.063  0.092  0.076  0.074  0.079  0.083\n",
      "31\n",
      "\t 0.648  0.623  0.581  0.510  0.463  0.465  0.455  0.430  0.415  0.403  0.395  0.394  0.382  0.373  0.353  0.343  0.333  0.324  0.312  0.298  0.294  0.292  0.284  0.262  0.242  0.230  0.223  0.231  0.296  0.391\n",
      "\t 0.654  0.630  0.555  0.500  0.485  0.482  0.479  0.461  0.442  0.446  0.434  0.423  0.421  0.426  0.425  0.402  0.403  0.414  0.398  0.409  0.433  0.415  0.424  0.440  0.457  0.461  0.493  0.483  0.568  0.461\n",
      "\t 0.143  0.131  0.085  0.086  0.088  0.113  0.099  0.086  0.084  0.082  0.075  0.104  0.103  0.099  0.103  0.086  0.072  0.079  0.070  0.059  0.067  0.102  0.092  0.082  0.070  0.061  0.108  0.074  0.110  0.097\n",
      "32\n",
      "\t 0.660  0.636  0.600  0.525  0.482  0.453  0.432  0.455  0.420  0.409  0.417  0.385  0.378  0.375  0.366  0.357  0.350  0.343  0.335  0.320  0.312  0.298  0.296  0.280  0.292  0.302  0.270  0.259  0.245  0.240\n",
      "\t 0.680  0.669  0.589  0.512  0.499  0.485  0.450  0.465  0.452  0.494  0.430  0.417  0.428  0.420  0.413  0.409  0.418  0.420  0.398  0.422  0.421  0.430  0.401  0.424  0.437  0.425  0.440  0.428  0.449  0.477\n",
      "\t 0.135  0.090  0.075  0.059  0.089  0.112  0.090  0.112  0.112  0.092  0.088  0.083  0.078  0.070  0.071  0.069  0.071  0.069  0.069  0.067  0.062  0.075  0.066  0.069  0.090  0.065  0.076  0.070  0.062  0.074\n",
      "33\n",
      "\t 0.661  0.635  0.618  0.584  0.513  0.460  0.429  0.417  0.390  0.395  0.385  0.366  0.393  0.374  0.359  0.340  0.321  0.317  0.301  0.285  0.279  0.266  0.243  0.236  0.228  0.214  0.217  0.201  0.174  0.172\n",
      "\t 0.668  0.654  0.627  0.556  0.512  0.467  0.472  0.457  0.435  0.451  0.448  0.446  0.424  0.419  0.426  0.419  0.412  0.417  0.445  0.432  0.457  0.428  0.466  0.468  0.496  0.574  0.520  0.529  0.548  0.564\n",
      "\t 0.151  0.135  0.091  0.080  0.067  0.058  0.121  0.067  0.082  0.098  0.163  0.104  0.118  0.103  0.085  0.082  0.080  0.074  0.082  0.095  0.090  0.071  0.097  0.084  0.074  0.100  0.088  0.079  0.075  0.075\n",
      "34\n",
      "\t 0.649  0.559  0.478  0.424  0.395  0.358  0.345  0.324  0.324  0.305  0.279  0.259  0.237  0.227  0.224  0.221  0.191  0.179  0.189  0.253  0.179  0.163  0.150  0.155  0.187  0.160  0.152  0.144  0.128  0.122\n",
      "\t 0.631  0.533  0.468  0.447  0.419  0.401  0.421  0.399  0.424  0.416  0.448  0.455  0.468  0.486  0.492  0.506  0.534  0.573  0.711  0.560  0.546  0.591  0.602  0.626  0.612  0.616  0.639  0.719  0.700  0.715\n",
      "\t 0.128  0.103  0.084  0.077  0.079  0.071  0.092  0.083  0.094  0.090  0.090  0.085  0.083  0.083  0.089  0.080  0.077  0.078  0.087  0.077  0.077  0.070  0.070  0.074  0.084  0.089  0.085  0.087  0.081  0.081\n",
      "35\n",
      "\t 0.667  0.636  0.584  0.517  0.474  0.441  0.417  0.395  0.391  0.384  0.374  0.349  0.336  0.312  0.379  0.337  0.323  0.305  0.282  0.277  0.264  0.246  0.236  0.238  0.245  0.216  0.203  0.193  0.188  0.180\n",
      "\t 0.658  0.627  0.562  0.525  0.497  0.483  0.471  0.456  0.451  0.439  0.443  0.430  0.434  0.437  0.427  0.444  0.427  0.417  0.451  0.446  0.465  0.469  0.483  0.487  0.515  0.489  0.517  0.593  0.554  0.582\n",
      "\t 0.136  0.102  0.089  0.068  0.059  0.056  0.056  0.075  0.118  0.094  0.091  0.076  0.071  0.058  0.082  0.112  0.088  0.084  0.087  0.090  0.083  0.073  0.087  0.073  0.098  0.075  0.076  0.069  0.084  0.088\n",
      "36\n",
      "\t 0.661  0.637  0.610  0.581  0.521  0.474  0.450  0.447  0.429  0.402  0.385  0.366  0.362  0.423  0.384  0.362  0.371  0.343  0.327  0.316  0.348  0.307  0.283  0.267  0.285  0.293  0.276  0.270  0.254  0.258\n",
      "\t 0.665  0.653  0.616  0.580  0.535  0.502  0.511  0.487  0.463  0.449  0.435  0.423  0.462  0.437  0.439  0.423  0.413  0.412  0.409  0.409  0.412  0.409  0.424  0.448  0.475  0.464  0.451  0.472  0.473  0.481\n",
      "\t 0.163  0.107  0.107  0.078  0.066  0.072  0.083  0.107  0.108  0.085  0.070  0.067  0.114  0.093  0.089  0.094  0.079  0.074  0.065  0.075  0.095  0.068  0.062  0.074  0.099  0.104  0.085  0.084  0.093  0.085\n",
      "37\n",
      "\t 0.646  0.584  0.505  0.462  0.435  0.398  0.392  0.374  0.358  0.345  0.335  0.323  0.311  0.289  0.283  0.302  0.265  0.243  0.226  0.214  0.215  0.228  0.208  0.187  0.254  0.183  0.157  0.144  0.135  0.129\n",
      "\t 0.633  0.565  0.507  0.464  0.454  0.437  0.436  0.404  0.403  0.401  0.393  0.401  0.409  0.399  0.421  0.416  0.426  0.432  0.469  0.454  0.486  0.479  0.519  0.660  0.552  0.572  0.590  0.599  0.627  0.634\n",
      "\t 0.145  0.091  0.089  0.086  0.075  0.071  0.066  0.062  0.060  0.060  0.057  0.052  0.053  0.057  0.076  0.082  0.071  0.064  0.059  0.077  0.082  0.090  0.085  0.078  0.108  0.078  0.078  0.070  0.066  0.068\n",
      "38\n",
      "\t 0.656  0.577  0.481  0.426  0.412  0.394  0.378  0.363  0.352  0.333  0.320  0.303  0.288  0.286  0.289  0.265  0.259  0.239  0.233  0.223  0.229  0.212  0.212  0.262  0.241  0.199  0.187  0.305  0.270  0.255\n",
      "\t 0.632  0.545  0.478  0.448  0.430  0.425  0.416  0.421  0.409  0.413  0.400  0.400  0.398  0.415  0.408  0.433  0.433  0.446  0.429  0.451  0.458  0.478  0.519  0.499  0.469  0.513  0.608  0.473  0.504  0.464\n",
      "\t 0.146  0.086  0.091  0.082  0.124  0.099  0.092  0.091  0.098  0.080  0.080  0.066  0.067  0.081  0.079  0.098  0.071  0.066  0.096  0.083  0.095  0.078  0.108  0.093  0.074  0.064  0.124  0.113  0.108  0.115\n",
      "39\n",
      "\t 0.652  0.616  0.527  0.465  0.424  0.409  0.393  0.388  0.383  0.369  0.363  0.352  0.334  0.319  0.304  0.297  0.281  0.264  0.248  0.251  0.228  0.226  0.442  0.397  0.364  0.346  0.336  0.327  0.320  0.308\n",
      "\t 0.650  0.602  0.515  0.467  0.445  0.436  0.431  0.442  0.426  0.414  0.407  0.411  0.399  0.408  0.430  0.421  0.414  0.434  0.444  0.449  0.478  0.488  0.483  0.406  0.396  0.403  0.391  0.397  0.399  0.398\n",
      "\t 0.136  0.100  0.083  0.079  0.076  0.094  0.081  0.094  0.084  0.104  0.085  0.097  0.092  0.087  0.073  0.086  0.083  0.083  0.088  0.082  0.080  0.083  0.112  0.091  0.090  0.077  0.077  0.076  0.074  0.072\n",
      "40\n",
      "\t 0.669  0.648  0.608  0.549  0.485  0.438  0.408  0.381  0.373  0.369  0.361  0.356  0.350  0.336  0.334  0.311  0.293  0.273  0.266  0.247  0.228  0.210  0.199  0.219  0.209  0.212  0.175  0.165  0.158  0.146\n",
      "\t 0.660  0.635  0.583  0.540  0.479  0.442  0.423  0.397  0.399  0.405  0.398  0.395  0.385  0.416  0.410  0.388  0.393  0.413  0.413  0.425  0.439  0.465  0.469  0.476  0.497  0.479  0.522  0.569  0.561  0.596\n",
      "\t 0.158  0.122  0.096  0.086  0.066  0.066  0.066  0.068  0.064  0.080  0.078  0.084  0.084  0.091  0.110  0.082  0.066  0.059  0.063  0.053  0.049  0.051  0.062  0.130  0.126  0.075  0.071  0.067  0.057  0.054\n",
      "41\n",
      "\t 0.666  0.634  0.577  0.530  0.483  0.460  0.435  0.413  0.389  0.381  0.377  0.364  0.352  0.337  0.337  0.339  0.330  0.316  0.299  0.403  0.505  0.461  0.563  0.572  0.541  0.494  0.473  0.438  0.412  0.393\n",
      "\t 0.665  0.642  0.545  0.513  0.485  0.485  0.433  0.418  0.423  0.393  0.404  0.404  0.406  0.399  0.422  0.421  0.414  0.404  0.423  0.598  0.500  0.583  0.616  0.617  0.539  0.494  0.485  0.423  0.395  0.407\n",
      "\t 0.117  0.079  0.088  0.107  0.096  0.117  0.072  0.070  0.069  0.064  0.064  0.053  0.052  0.052  0.053  0.081  0.058  0.052  0.053  0.175  0.143  0.142  0.160  0.149  0.127  0.117  0.096  0.080  0.075  0.073\n",
      "42\n",
      "\t 0.677  0.651  0.634  0.609  0.560  0.502  0.469  0.434  0.411  0.392  0.373  0.359  0.342  0.323  0.306  0.301  0.304  0.331  0.302  0.272  0.267  0.302  0.259  0.236  0.224  0.227  0.220  0.199  0.189  0.173\n",
      "\t 0.672  0.654  0.656  0.616  0.554  0.532  0.494  0.474  0.453  0.453  0.429  0.432  0.419  0.418  0.414  0.429  0.487  0.429  0.425  0.430  0.457  0.398  0.432  0.451  0.454  0.492  0.499  0.541  0.550  0.559\n",
      "\t 0.132  0.107  0.087  0.081  0.081  0.088  0.070  0.059  0.062  0.054  0.055  0.060  0.054  0.063  0.071  0.146  0.128  0.128  0.121  0.103  0.112  0.112  0.096  0.089  0.096  0.099  0.095  0.080  0.065  0.074\n",
      "43\n",
      "\t 0.660  0.625  0.556  0.496  0.472  0.454  0.419  0.401  0.389  0.372  0.370  0.382  0.361  0.336  0.317  0.322  0.310  0.390  0.373  0.462  0.373  0.372  0.349  0.340  0.332  0.315  0.312  0.306  0.338  0.355\n",
      "\t 0.648  0.603  0.532  0.502  0.485  0.457  0.439  0.431  0.424  0.409  0.422  0.415  0.404  0.408  0.418  0.430  0.404  0.499  0.404  0.411  0.392  0.388  0.377  0.409  0.408  0.418  0.407  0.403  0.517  0.399\n",
      "\t 0.111  0.083  0.071  0.147  0.111  0.097  0.101  0.082  0.094  0.078  0.076  0.087  0.076  0.065  0.084  0.073  0.075  0.075  0.065  0.074  0.069  0.065  0.062  0.062  0.060  0.060  0.061  0.056  0.071  0.067\n",
      "44\n",
      "\t 0.669  0.640  0.623  0.608  0.552  0.499  0.458  0.439  0.432  0.410  0.403  0.406  0.382  0.365  0.355  0.344  0.333  0.327  0.347  0.314  0.325  0.299  0.275  0.258  0.245  0.236  0.221  0.223  0.207  0.196\n",
      "\t 0.672  0.637  0.628  0.598  0.551  0.506  0.491  0.482  0.476  0.457  0.471  0.447  0.429  0.436  0.420  0.421  0.421  0.477  0.420  0.429  0.419  0.409  0.441  0.452  0.462  0.479  0.503  0.505  0.504  0.527\n",
      "\t 0.147  0.092  0.095  0.078  0.067  0.069  0.073  0.104  0.094  0.076  0.138  0.089  0.080  0.104  0.084  0.065  0.075  0.084  0.094  0.091  0.089  0.086  0.074  0.070  0.062  0.063  0.061  0.056  0.057  0.070\n",
      "45\n",
      "\t 0.665  0.626  0.576  0.502  0.453  0.430  0.395  0.391  0.374  0.364  0.348  0.344  0.324  0.316  0.311  0.319  0.286  0.270  0.246  0.240  0.240  0.225  0.219  0.205  0.196  0.181  0.170  0.159  0.154  0.170\n",
      "\t 0.664  0.618  0.546  0.506  0.470  0.447  0.444  0.430  0.427  0.428  0.427  0.419  0.423  0.426  0.455  0.400  0.412  0.437  0.491  0.449  0.467  0.485  0.536  0.575  0.563  0.566  0.614  0.625  0.616  0.628\n",
      "\t 0.125  0.086  0.085  0.096  0.109  0.081  0.071  0.078  0.084  0.068  0.077  0.073  0.078  0.079  0.069  0.083  0.083  0.071  0.069  0.067  0.082  0.080  0.084  0.088  0.075  0.071  0.070  0.065  0.069  0.081\n",
      "46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0.667  0.593  0.506  0.451  0.417  0.402  0.384  0.376  0.371  0.344  0.326  0.318  0.306  0.319  0.326  0.319  0.298  0.272  0.260  0.248  0.227  0.212  0.217  0.241  0.377  0.276  0.237  0.221  0.222  0.200\n",
      "\t 0.646  0.563  0.500  0.457  0.445  0.429  0.420  0.398  0.396  0.408  0.394  0.402  0.430  0.399  0.407  0.404  0.398  0.417  0.431  0.447  0.450  0.490  0.509  0.522  0.469  0.439  0.466  0.484  0.513  0.516\n",
      "\t 0.133  0.124  0.087  0.071  0.112  0.077  0.112  0.096  0.098  0.075  0.083  0.094  0.088  0.091  0.078  0.083  0.069  0.060  0.064  0.060  0.053  0.057  0.088  0.089  0.094  0.076  0.070  0.072  0.079  0.071\n",
      "47\n",
      "\t 0.661  0.629  0.603  0.552  0.495  0.450  0.430  0.404  0.394  0.382  0.438  0.403  0.384  0.379  0.368  0.360  0.353  0.347  0.367  0.350  0.346  0.333  0.331  0.327  0.323  0.316  0.307  0.300  0.317  0.290\n",
      "\t 0.650  0.625  0.605  0.541  0.498  0.478  0.457  0.427  0.440  0.428  0.442  0.412  0.402  0.417  0.406  0.412  0.406  0.423  0.417  0.408  0.414  0.418  0.406  0.410  0.424  0.420  0.419  0.439  0.422  0.438\n",
      "\t 0.151  0.100  0.082  0.097  0.092  0.086  0.100  0.079  0.087  0.091  0.115  0.101  0.106  0.099  0.093  0.094  0.079  0.082  0.088  0.094  0.080  0.082  0.078  0.074  0.076  0.073  0.076  0.070  0.077  0.077\n",
      "48\n",
      "\t 0.670  0.617  0.545  0.485  0.444  0.413  0.414  0.392  0.376  0.366  0.356  0.345  0.325  0.316  0.302  0.288  0.274  0.256  0.239  0.227  0.231  0.223  0.201  0.186  0.177  0.187  0.224  0.554  0.406  0.389\n",
      "\t 0.655  0.601  0.513  0.484  0.446  0.435  0.423  0.399  0.404  0.391  0.379  0.381  0.392  0.406  0.390  0.385  0.380  0.403  0.429  0.427  0.427  0.451  0.457  0.488  0.481  0.568  0.519  0.523  0.446  0.574\n",
      "\t 0.127  0.104  0.102  0.074  0.075  0.080  0.081  0.083  0.069  0.069  0.077  0.069  0.074  0.076  0.071  0.073  0.071  0.064  0.064  0.066  0.088  0.074  0.065  0.062  0.068  0.078  0.084  0.100  0.082  0.158\n",
      "49\n",
      "\t 0.648  0.554  0.471  0.424  0.401  0.387  0.366  0.350  0.338  0.316  0.307  0.292  0.276  0.265  0.243  0.220  0.215  0.215  0.209  0.270  0.225  0.201  0.174  0.159  0.153  0.150  0.137  0.124  0.118  0.130\n",
      "\t 0.633  0.539  0.475  0.438  0.443  0.400  0.415  0.389  0.395  0.410  0.395  0.414  0.420  0.432  0.439  0.488  0.528  0.514  0.546  0.504  0.525  0.527  0.533  0.600  0.617  0.636  0.657  0.666  0.715  0.722\n",
      "\t 0.136  0.099  0.079  0.071  0.080  0.068  0.077  0.071  0.076  0.089  0.075  0.072  0.093  0.073  0.061  0.058  0.083  0.076  0.109  0.086  0.076  0.093  0.093  0.075  0.094  0.086  0.066  0.057  0.071  0.085\n",
      "50\n",
      "\t 0.660  0.637  0.617  0.581  0.516  0.474  0.437  0.417  0.392  0.377  0.369  0.360  0.387  0.370  0.349  0.334  0.324  0.331  0.321  0.323  0.299  0.297  0.317  0.297  0.278  0.267  0.263  0.252  0.239  0.248\n",
      "\t 0.663  0.652  0.636  0.557  0.519  0.494  0.473  0.450  0.448  0.443  0.437  0.444  0.449  0.436  0.415  0.422  0.410  0.411  0.401  0.418  0.402  0.424  0.426  0.390  0.413  0.439  0.411  0.444  0.491  0.439\n",
      "\t 0.153  0.113  0.102  0.087  0.071  0.083  0.073  0.063  0.061  0.069  0.067  0.082  0.098  0.113  0.099  0.090  0.100  0.095  0.088  0.095  0.084  0.081  0.089  0.097  0.083  0.094  0.082  0.083  0.096  0.075\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_and_prior_losses(\"SPI1_prior_keep1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99bb67f7d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation loss\"\n",
    "title += \"\\nTraining on only 1% of positive bins\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.2640587687389155\n",
      "5.566017270666034e-05\n"
     ]
    }
   ],
   "source": [
    "np_vals, p_vals = np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(t)\n",
    "print(p / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
