{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for the model and data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"binary\":\n",
    "    models_base_path = \"/users/amtseng/att_priors/models/trained_models/binary/\"\n",
    "else:\n",
    "    models_base_path = \"/users/amtseng/att_priors/models/trained_models/profile/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_name = \"BPNet\"\n",
    "noprior_models_path = os.path.join(models_base_path, \"BPNet\")\n",
    "prior_models_path = os.path.join(models_base_path, \"BPNet_prior\")\n",
    "peak_retention = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing saved metrics JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist, or the JSON is\n",
    "    not well-formed.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Metrics JSON at %s is not well-formed\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func, max_epoch=None):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return True if the _first_ one is better. If `max_epoch` is provided, will only\n",
    "    report everything up to this epoch (1-indexed).\n",
    "    Returns the number of the run, the (one-indexed) number of the epoch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                if i == max_epoch:\n",
    "                    break\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fetch_and_print_performance(models_path, max_epoch=None):\n",
    "    \"\"\"\n",
    "    Given the path to a condition containing many runs, prints out the best validation\n",
    "    losses for each run. If given, only consider up to `max_epoch` epochs total; anything\n",
    "    afterward would be ignored.\n",
    "    \"\"\"\n",
    "    if model_type == \"binary\":\n",
    "        val_key = \"val_corr_losses\"\n",
    "    else:\n",
    "        val_key = \"val_prof_corr_losses\"\n",
    "    \n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        val_key,\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y,\n",
    "        max_epoch\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %f\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.4f\" % (key[0], key[1], all_vals[key]))\n",
    "    return best_run, best_epoch, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_metric_distributions(noprior_models_path, prior_models_path, noprior_query_run=None, prior_query_run=None):\n",
    "    if model_type == \"binary\":\n",
    "        metric_keys = [\n",
    "            (\"test_acc\", \"test accuracy\", \"less\"),\n",
    "            (\"test_auroc\", \"test auROC\", \"less\"),\n",
    "            (\"test_auprc\", \"test auPRC\", \"less\"),\n",
    "            (\"test_corr_auprc\", \"estimated test auPRC\", \"less\")\n",
    "        ]\n",
    "    else:\n",
    "        metric_keys = [\n",
    "            (\"summit_prof_nll\", \"test profile NLL\", \"greater\"),\n",
    "            (\"summit_prof_jsd\", \"test profile JSD\", \"greater\")\n",
    "        ]\n",
    "    \n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    noprior_metrics = {run_num : import_metrics_json(noprior_models_path, run_num) for run_num in os.listdir(noprior_models_path)}\n",
    "    noprior_metrics = {key : val for key, val in noprior_metrics.items() if val}  # Remove empties\n",
    "    prior_metrics = {run_num : import_metrics_json(prior_models_path, run_num) for run_num in os.listdir(prior_models_path)}\n",
    "    prior_metrics = {key : val for key, val in prior_metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    vals_to_return = []\n",
    "    \n",
    "    for metric_key, metric_name, test_alternative in metric_keys:\n",
    "        noprior_vals = np.array([\n",
    "            np.mean(metrics[metric_key][\"values\"]) for metrics in noprior_metrics.values()\n",
    "        ])\n",
    "        prior_vals = np.array([\n",
    "            np.mean(metrics[metric_key][\"values\"]) for metrics in prior_metrics.values()\n",
    "        ])\n",
    "        vals_to_return.append((metric_name, noprior_vals, prior_vals))\n",
    "        bin_num = 20\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        all_vals = np.concatenate([noprior_vals, prior_vals])\n",
    "        bins = np.linspace(np.min(all_vals), np.max(all_vals), bin_num)\n",
    "        ax.hist(noprior_vals, bins=bins, color=\"coral\", label=\"No prior\", alpha=0.7)\n",
    "        ax.hist(prior_vals, bins=bins, color=\"slateblue\", label=\"With Fourier prior\", alpha=0.7)\n",
    "        title = \"Histogram of %s without/with Fourier priors\" % metric_name\n",
    "        title += \"\\n%s, %d/%d %s models\" % (condition_name, len(noprior_vals), len(prior_vals), model_type)\n",
    "        if peak_retention != \"all\":\n",
    "            title += \"\\nTraining on %s peaks\" % peak_retention\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(metric_name[0].upper() + metric_name[1:])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        u, p = scipy.stats.mannwhitneyu(noprior_vals, prior_vals, alternative=test_alternative)\n",
    "        print(\"Mean without priors: %f\" % np.mean(noprior_vals))\n",
    "        print(\"Mean with priors: %f\" % np.mean(prior_vals))\n",
    "        print(\"Standard error without priors: %f\" % scipy.stats.sem(noprior_vals))\n",
    "        print(\"Standard error with priors: %f\" % scipy.stats.sem(prior_vals))\n",
    "        print(\"One-sided Mann-Whitney U-test: U = %f, p = %f\" % (u, p))\n",
    "        if noprior_query_run is not None and prior_query_run is not None:\n",
    "            noprior_mean = np.mean(noprior_metrics[noprior_query_run][metric_key][\"values\"])\n",
    "            prior_mean = np.mean(prior_metrics[prior_query_run][metric_key][\"values\"])\n",
    "            print(\"No prior %s (run %s): %f\" % (metric_name, noprior_query_run, noprior_mean))\n",
    "            print(\"With Fourier prior %s (run %s): %f\" % (metric_name, prior_query_run, prior_mean))\n",
    "    \n",
    "    return vals_to_return  # List of metric name, noprior values, and prior values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noprior_best_run, noprior_best_epoch, noprior_val_losses = fetch_and_print_performance(noprior_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prior_best_run, prior_best_epoch, prior_val_losses = fetch_and_print_performance(prior_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = 30\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "noprior_vals, prior_vals = np.array(list(noprior_val_losses.values())), np.array(list(prior_val_losses.values()))\n",
    "all_vals = np.concatenate([noprior_vals, prior_vals])\n",
    "bins = np.linspace(np.min(all_vals), np.max(all_vals), bin_num)\n",
    "ax.hist(noprior_vals, bins=bins, color=\"coral\", label=\"No prior\", alpha=0.7)\n",
    "ax.hist(prior_vals, bins=bins, color=\"slateblue\", label=\"With Fourier prior\", alpha=0.7)\n",
    "if model_type == \"binary\":\n",
    "    title = \"Histogram of validation loss without/with Fourier priors\"\n",
    "else:\n",
    "    title = \"Histogram of validation profile NLL loss without/with Fourier priors\"\n",
    "title += \"\\n%s, %d/%d %s models\" % (condition_name, len(noprior_vals), len(prior_vals), model_type)\n",
    "if peak_retention != \"all\":\n",
    "    title += \"\\nTraining on %s peaks\" % peak_retention\n",
    "ax.set_title(title)\n",
    "if model_type == \"binary\":\n",
    "    ax.set_xlabel(\"Validation loss\")\n",
    "else:\n",
    "    ax.set_xlabel(\"Validation profile NLL loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "u, p = scipy.stats.mannwhitneyu(noprior_vals, prior_vals, alternative=\"greater\")\n",
    "print(\"Mean without priors: %f\" % np.mean(noprior_vals))\n",
    "print(\"Mean with priors: %f\" % np.mean(prior_vals))\n",
    "print(\"Standard error without priors: %f\" % scipy.stats.sem(noprior_vals))\n",
    "print(\"Standard error with priors: %f\" % scipy.stats.sem(prior_vals))\n",
    "print(\"One-sided Mann-Whitney U-test: U = %f, p = %f\" % (u, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_metrics = plot_test_metric_distributions(noprior_models_path, prior_models_path, noprior_best_run, prior_best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_violin_plots = 1 + len(test_metrics)\n",
    "fig, ax = plt.subplots(1, num_violin_plots, figsize=(7 * num_violin_plots, 8))\n",
    "\n",
    "def create_violin_pair(ax, noprior_data, prior_data, metric_name):\n",
    "    all_data = np.stack([noprior_data, prior_data], axis=0)\n",
    "    # Define the quartiles\n",
    "    q1, med, q3 = np.percentile(all_data, [25, 50, 70], axis=1)\n",
    "    iqr = q3 - q1\n",
    "    plot_parts = ax.violinplot(\n",
    "        [np.sort(all_data[0]), np.sort(all_data[1])], showmeans=False, showmedians=False, showextrema=False\n",
    "    )\n",
    "    violin_parts = plot_parts[\"bodies\"]\n",
    "    violin_parts[0].set_facecolor(\"coral\")\n",
    "    violin_parts[0].set_edgecolor(\"coral\")\n",
    "    violin_parts[0].set_alpha(0.7)\n",
    "    violin_parts[1].set_facecolor(\"slateblue\")\n",
    "    violin_parts[1].set_edgecolor(\"slateblue\")\n",
    "    violin_parts[1].set_alpha(0.7)\n",
    "\n",
    "    inds = np.arange(1, 3)\n",
    "    ax.vlines(inds, q1, q3, color=\"black\", linewidth=5, zorder=1)\n",
    "    ax.scatter(inds, med, marker=\"o\", color=\"white\", s=30, zorder=2)\n",
    "    ax.set_xticks(inds)\n",
    "    ax.set_xticklabels([\"No prior\", \"With Fourier prior\"])\n",
    "    ax.set_title(metric_name)\n",
    "\n",
    "create_violin_pair(ax[0], noprior_vals, prior_vals, \"Validation profile NLL loss\" if model_type == \"profile\" else \"Validation loss\")\n",
    "for i in range(0, len(test_metrics)):\n",
    "    create_violin_pair(ax[i + 1], test_metrics[i][1], test_metrics[i][2], test_metrics[i][0][0].upper() + test_metrics[i][0][1:])\n",
    "    \n",
    "title = \"Model performance without/with Fourier priors\"\n",
    "title += \"\\n%s, %d/%d %s models\" % (condition_name, len(noprior_vals), len(prior_vals), model_type)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "if peak_retention != \"all\":\n",
    "    title += \"\\nTraining on %s peaks\" % peak_retention\n",
    "    plt.subplots_adjust(top=0.80)\n",
    "fig.suptitle(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
