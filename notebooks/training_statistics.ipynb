{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func, max_epoch=None):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better. If `max_epoch` is provided, will\n",
    "    only report everything up to this epoch (1-indexed).\n",
    "    Returns the number of the run, the (one-indexed) number of the epoch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                if i == max_epoch:\n",
    "                    break\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_profile_losses(condition, max_epoch=None):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/profile/%s/\" % condition\n",
    "    \n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y,\n",
    "        max_epoch\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 1\n",
      "\tBest epoch in run: 13\n",
      "\tAssociated value: 107.96219924615355\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 13: 107.96\n",
      "\tRun 2, epoch 12: 108.49\n",
      "\tRun 3, epoch 16: 108.07\n",
      "\tRun 4, epoch 20: 108.10\n",
      "\tRun 5, epoch 17: 108.37\n",
      "\tRun 6, epoch 15: 108.29\n",
      "\tRun 7, epoch 17: 108.84\n",
      "\tRun 8, epoch 18: 108.74\n",
      "\tRun 9, epoch 18: 108.41\n",
      "\tRun 10, epoch 16: 108.37\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t113.16 109.13 107.71 107.00 106.60 106.41 106.21 106.09 105.96 105.84 105.79 105.72 105.64 105.60 105.56\n",
      "\t112.21 110.52 109.57 109.24 108.77 108.50 108.30 108.22 108.20 108.06 108.17 108.00 107.96 108.08 107.97\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t113.40 109.13 108.00 107.32 106.84 106.52 106.24 106.07 105.94 105.81 105.73 105.65 105.60 105.53 105.51 105.46 105.40\n",
      "\t112.51 110.93 110.12 109.88 109.49 109.03 108.89 108.70 108.71 108.65 108.51 108.49 108.53 108.50 108.59 108.54 108.56\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t114.49 110.73 108.74 107.81 107.06 106.52 106.19 105.98 105.81 105.71 105.60 105.51 105.43 105.40 105.33 105.28 105.24 105.20 105.17 105.16\n",
      "\t114.61 112.30 110.87 110.08 109.33 108.88 108.59 108.39 108.50 108.37 108.35 108.33 108.15 108.27 108.12 108.07 108.16 108.12 108.14 108.13\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t113.56 109.47 108.35 107.55 106.92 106.58 106.30 106.12 105.94 105.84 105.78 105.67 105.63 105.57 105.52 105.50 105.45 105.40 105.39 105.35\n",
      "\t112.72 111.13 110.22 109.55 109.10 108.85 108.56 108.45 108.48 108.37 108.49 108.19 108.12 108.14 108.27 108.15 108.30 108.17 108.17 108.10\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t114.79 110.25 108.50 107.43 106.74 106.42 106.17 105.98 105.87 105.76 105.63 105.57 105.51 105.45 105.39 105.40 105.29 105.29 105.27\n",
      "\t114.49 111.77 110.74 109.50 109.25 108.91 108.83 108.73 108.56 108.81 108.51 108.50 108.42 108.51 108.38 108.38 108.37 108.41 108.50\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t115.29 111.10 109.42 108.23 107.29 106.82 106.56 106.39 106.25 106.13 106.02 105.96 105.89 105.84 105.77 105.74 105.71 105.69 105.66 105.63\n",
      "\t114.81 112.95 111.15 110.04 109.36 109.01 109.09 108.70 108.59 108.61 108.41 108.33 108.34 108.41 108.29 108.30 108.31 108.34 108.41 108.29\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t113.90 109.91 108.38 107.38 106.85 106.53 106.30 106.12 105.99 105.88 105.83 105.75 105.66 105.60 105.55 105.49 105.47 105.44\n",
      "\t115.75 112.10 110.77 110.30 109.94 109.61 109.36 109.25 109.07 109.07 109.09 109.00 108.96 108.92 108.96 108.97 108.84 108.88\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "8\n",
      "\t111.57 108.13 107.18 106.63 106.34 106.11 105.93 105.83 105.75 105.67 105.55 105.51 105.47 105.40 105.39 105.34 105.31 105.31 105.25 105.21\n",
      "\t112.04 110.70 110.22 109.61 109.40 109.12 109.01 109.16 109.00 108.94 109.02 108.81 108.85 108.98 108.81 108.95 108.88 108.74 108.99 108.84\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "9\n",
      "\t112.48 109.62 108.18 107.51 107.13 106.84 106.58 106.35 106.19 106.05 105.92 105.84 105.74 105.69 105.60 105.55 105.50 105.45\n",
      "\t113.35 111.27 110.62 110.05 109.77 109.58 109.50 109.26 109.12 109.05 108.89 108.76 108.72 108.59 108.54 108.57 108.48 108.41\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "10\n",
      "\t115.44 111.27 109.31 108.21 107.43 106.91 106.62 106.36 106.22 106.08 105.98 105.90 105.83 105.76 105.70 105.67 105.62 105.58 105.56 105.53\n",
      "\t115.06 112.17 111.23 110.29 109.87 109.35 109.00 108.75 108.76 108.64 108.53 108.60 108.51 108.68 108.49 108.37 108.46 108.43 108.52 108.41\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_profile_losses(\"GATA2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 1\n",
      "\tBest epoch in run: 18\n",
      "\tAssociated value: 108.64256467235332\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 18: 108.64\n",
      "\tRun 2, epoch 19: 108.73\n",
      "\tRun 3, epoch 20: 109.13\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t116.35 111.96 110.06 109.26 108.52 107.86 107.38 107.03 106.83 106.68 106.59 106.52 106.44 106.38 106.31 106.26 106.17 106.14 106.15 106.10\n",
      "\t116.09 113.15 112.10 111.17 110.69 110.04 109.50 109.40 109.36 109.35 109.14 108.97 108.93 108.83 108.82 108.85 108.66 108.64 108.70 108.95\n",
      "\t0.1657 0.1597 0.1554 0.0655 0.0588 0.0556 0.0568 0.0532 0.0528 0.0527 0.0503 0.0541 0.0522 0.0485 0.0488 0.0474 0.0467 0.0456 0.0442 0.0461\n",
      "2\n",
      "\t116.92 113.36 110.80 109.14 108.30 107.70 107.19 106.81 106.54 106.38 106.27 106.23 106.15 106.11 106.05 106.02 105.98 105.95 105.93 105.89\n",
      "\t117.35 114.90 112.31 111.24 110.66 110.03 109.65 109.35 109.22 109.08 108.91 108.96 108.77 109.02 108.81 108.85 108.73 108.77 108.73 108.73\n",
      "\t0.1197 0.0659 0.0659 0.0558 0.0461 0.0451 0.0449 0.0432 0.0435 0.0446 0.0427 0.0474 0.0469 0.0482 0.0457 0.0435 0.0436 0.0449 0.0429 0.0427\n",
      "3\n",
      "\t116.76 113.38 111.11 109.94 108.71 108.05 107.59 107.26 107.04 106.84 106.71 106.62 106.54 106.45 106.41 106.34 106.30 106.27 106.24 106.22\n",
      "\t117.03 114.96 112.94 112.00 111.16 110.52 110.48 109.90 109.76 109.58 109.45 109.42 109.41 109.48 109.35 109.38 109.21 109.21 109.23 109.13\n",
      "\t0.0986 0.1007 0.0798 0.0570 0.0503 0.0566 0.0531 0.0483 0.0471 0.0450 0.0446 0.0428 0.0424 0.0434 0.0423 0.0428 0.0446 0.0438 0.0455 0.0442\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_profile_losses(\"GATA2_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation profile loss\"\n",
    "title += \"\\nTraining on only 1% of peaks\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()\n",
    "\n",
    "np_vals, p_vals = np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(t)\n",
    "print(p / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_binary_losses(condition, max_epoch=None):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/binary/%s/\" % condition\n",
    "    \n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y,\n",
    "        max_epoch\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.3f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"train_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_corr_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)[:max_epoch]]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 17\n",
      "\tBest epoch in run: 2\n",
      "\tAssociated value: 0.2657462121730868\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 2:  0.274\n",
      "\tRun 2, epoch 2:  0.279\n",
      "\tRun 3, epoch 2:  0.275\n",
      "\tRun 4, epoch 2:  0.268\n",
      "\tRun 5, epoch 2:  0.272\n",
      "\tRun 6, epoch 2:  0.271\n",
      "\tRun 7, epoch 2:  0.278\n",
      "\tRun 8, epoch 2:  0.272\n",
      "\tRun 9, epoch 2:  0.276\n",
      "\tRun 10, epoch 1:  0.277\n",
      "\tRun 11, epoch 2:  0.272\n",
      "\tRun 12, epoch 2:  0.271\n",
      "\tRun 13, epoch 2:  0.270\n",
      "\tRun 14, epoch 2:  0.281\n",
      "\tRun 15, epoch 2:  0.278\n",
      "\tRun 16, epoch 1:  0.280\n",
      "\tRun 17, epoch 2:  0.266\n",
      "\tRun 18, epoch 2:  0.271\n",
      "\tRun 19, epoch 2:  0.272\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.290  0.239  0.218  0.203  0.191\n",
      "\t 0.281  0.274  0.282  0.295  0.313\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "2\n",
      "\t 0.289  0.242  0.223  0.207  0.194\n",
      "\t 0.283  0.279  0.285  0.304  0.314\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "3\n",
      "\t 0.292  0.242  0.221  0.206  0.193\n",
      "\t 0.280  0.275  0.284  0.292  0.308\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "4\n",
      "\t 0.286  0.236  0.217  0.203  0.192\n",
      "\t 0.274  0.268  0.276  0.288  0.301\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "5\n",
      "\t 0.287  0.237  0.217  0.203  0.193\n",
      "\t 0.277  0.272  0.281  0.293  0.305\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "6\n",
      "\t 0.287  0.238  0.218  0.204  0.193\n",
      "\t 0.277  0.271  0.280  0.288  0.302\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "7\n",
      "\t 0.287  0.241  0.224  0.211  0.201\n",
      "\t 0.279  0.278  0.285  0.305  0.318\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "8\n",
      "\t 0.287  0.236  0.216  0.201  0.190\n",
      "\t 0.280  0.272  0.283  0.289  0.303\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "9\n",
      "\t 0.295  0.241  0.221  0.207  0.195\n",
      "\t 0.284  0.276  0.280  0.295  0.301\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "10\n",
      "\t 0.285  0.237  0.219  0.205  0.193\n",
      "\t 0.277  0.278  0.286  0.293  0.316\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "11\n",
      "\t 0.287  0.238  0.217  0.203  0.191\n",
      "\t 0.276  0.272  0.280  0.290  0.307\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "12\n",
      "\t 0.286  0.234  0.216  0.199  0.187\n",
      "\t 0.274  0.271  0.280  0.291  0.300\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "13\n",
      "\t 0.286  0.236  0.214  0.200  0.189\n",
      "\t 0.279  0.270  0.278  0.295  0.304\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "14\n",
      "\t 0.290  0.245  0.225  0.210  0.200\n",
      "\t 0.289  0.281  0.286  0.303  0.314\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "15\n",
      "\t 0.289  0.242  0.222  0.205  0.192\n",
      "\t 0.283  0.278  0.286  0.290  0.313\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "16\n",
      "\t 0.290  0.243  0.223  0.207  0.194\n",
      "\t 0.280  0.281  0.281  0.291  0.306\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "17\n",
      "\t 0.285  0.234  0.214  0.199  0.188\n",
      "\t 0.275  0.266  0.271  0.292  0.299\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "18\n",
      "\t 0.284  0.235  0.216  0.202  0.191\n",
      "\t 0.278  0.271  0.279  0.295  0.295\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n",
      "19\n",
      "\t 0.290  0.240  0.218  0.203  0.191\n",
      "\t 0.281  0.272  0.278  0.293  0.306\n",
      "\t 0.000  0.000  0.000  0.000  0.000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_binary_losses(\"SPI1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 1\n",
      "\tBest epoch in run: 4\n",
      "\tAssociated value: 0.2703861015471767\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 4:  0.270\n",
      "\tRun 2, epoch 3:  0.273\n",
      "\tRun 3, epoch 5:  0.273\n",
      "\tRun 4, epoch 8:  0.275\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.339  0.280  0.261  0.243  0.235  0.227  0.219\n",
      "\t 0.295  0.283  0.274  0.270  0.273  0.277  0.278\n",
      "\t 0.035  0.032  0.033  0.030  0.035  0.032  0.033\n",
      "2\n",
      "\t 0.350  0.272  0.255  0.242  0.234  0.228  0.222  0.217  0.210\n",
      "\t 0.300  0.282  0.273  0.275  0.275  0.275  0.278  0.283  0.291\n",
      "\t 0.034  0.034  0.030  0.030  0.031  0.031  0.030  0.030  0.034\n",
      "3\n",
      "\t 0.322  0.265  0.248  0.238  0.228  0.221  0.215  0.211\n",
      "\t 0.303  0.277  0.274  0.284  0.273  0.274  0.280  0.280\n",
      "\t 0.034  0.028  0.030  0.037  0.029  0.028  0.032  0.033\n",
      "4\n",
      "\t 0.335  0.271  0.252  0.246  0.238  0.227  0.226  0.234  0.217  0.211  0.204\n",
      "\t 0.294  0.289  0.276  0.292  0.276  0.277  0.277  0.275  0.278  0.285  0.292\n",
      "\t 0.038  0.035  0.035  0.044  0.034  0.040  0.040  0.033  0.032  0.034  0.034\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_binary_losses(\"SPI1_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.65347337937071\n",
      "0.007256762865705637\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation loss\"\n",
    "title += \"\\nTraining on only 1% of peaks\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "np_vals, p_vals = np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(t)\n",
    "print(p / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
