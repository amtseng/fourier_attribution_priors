{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the (one-indexed) number of the epch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_validation_profile_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "Warning: Was not able to compute values for run 8\n",
      "\tBest run: 4\n",
      "\tBest epoch in run: 10\n",
      "\tAssociated value: 179.99440738677978\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 10: 180.97\n",
      "\tRun 2, epoch 8: 180.73\n",
      "\tRun 3, epoch 7: 180.46\n",
      "\tRun 4, epoch 10: 179.99\n",
      "\tRun 5, epoch 10: 180.05\n",
      "\tRun 6, epoch 10: 180.81\n",
      "\tRun 7, epoch 8: 181.37\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t200.36 194.65 193.33 192.59 192.15 191.83 191.66 191.52 191.40 191.25\n",
      "\t184.69 182.63 181.86 181.45 181.32 181.37 181.15 181.13 181.09 180.97\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t199.41 194.43 193.20 192.45 192.04 191.73 191.50 191.35 191.22 191.14\n",
      "\t184.60 182.48 181.70 181.30 180.91 180.93 180.88 180.73 180.80 180.90\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t198.70 193.95 192.49 191.83 191.42 191.19 191.02 190.83 190.75 190.59\n",
      "\t183.93 181.95 181.27 180.81 180.70 180.67 180.46 180.62 180.62 180.83\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t199.55 194.49 193.11 192.40 192.02 191.76 191.54 191.44 191.29 191.18\n",
      "\t183.14 181.38 180.75 180.37 180.14 180.17 180.15 180.13 180.12 179.99\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t199.47 194.07 192.71 192.03 191.62 191.33 191.18 190.99 190.88 190.73\n",
      "\t183.18 181.80 180.95 180.63 180.28 180.08 180.09 180.15 180.07 180.05\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t199.59 194.77 193.26 192.50 192.12 191.79 191.54 191.40 191.29 191.13\n",
      "\t185.22 182.60 181.85 181.65 181.38 181.22 180.95 180.89 180.83 180.81\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t199.76 194.27 193.10 192.46 192.05 191.71 191.50 191.36 191.27 191.15\n",
      "\t185.97 183.34 182.55 182.00 181.74 181.56 181.67 181.37 181.43 181.50\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_profile_and_prior_losses(\"HepG2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 3\n",
      "\tBest epoch in run: 7\n",
      "\tAssociated value: 181.0568891398112\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 9: 181.77\n",
      "\tRun 2, epoch 9: 181.86\n",
      "\tRun 3, epoch 7: 181.06\n",
      "\tRun 4, epoch 10: 181.13\n",
      "\tRun 5, epoch 9: 181.66\n",
      "\tRun 6, epoch 6: 182.12\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t199.37 194.95 194.05 193.47 193.14 192.86 192.67 192.56 192.45 192.38\n",
      "\t184.64 183.46 182.80 182.50 182.20 182.05 182.10 182.17 181.77 182.11\n",
      "\t0.1124 0.0946 0.0776 0.0777 0.0766 0.0789 0.0812 0.0818 0.0797 0.0826\n",
      "2\n",
      "\t199.63 195.21 194.34 193.89 193.69 193.50 193.34 193.15 193.09 192.99\n",
      "\t184.50 183.09 183.09 182.21 182.38 182.05 182.14 182.13 181.86 181.90\n",
      "\t0.1013 0.0823 0.0744 0.0758 0.0762 0.0742 0.0755 0.0810 0.0776 0.0781\n",
      "3\n",
      "\t199.85 195.26 194.04 193.39 193.00 192.85 192.69 192.56 192.47 192.40\n",
      "\t184.08 182.69 182.14 181.53 181.53 181.49 181.06 181.22 181.28 181.22\n",
      "\t0.0980 0.0929 0.0930 0.0843 0.0784 0.0846 0.0813 0.0852 0.0845 0.0822\n",
      "4\n",
      "\t199.82 195.20 194.24 193.80 193.51 193.30 193.10 192.91 192.76 192.65\n",
      "\t183.96 182.62 182.11 181.85 181.70 181.50 181.43 181.22 181.40 181.13\n",
      "\t0.1094 0.0794 0.0788 0.0754 0.0791 0.0749 0.0783 0.0785 0.0795 0.0749\n",
      "5\n",
      "\t199.23 195.12 193.82 193.27 192.83 192.68 192.59 192.59 192.52 192.41\n",
      "\t184.73 183.14 182.46 181.99 182.13 181.96 181.82 182.02 181.66 181.94\n",
      "\t0.0954 0.0972 0.0842 0.0771 0.0815 0.0758 0.0910 0.0878 0.0840 0.0783\n",
      "6\n",
      "\t200.37 195.37 194.50 193.93 193.57 193.32\n",
      "\t184.55 183.42 182.78 182.41 182.35 182.12\n",
      "\t0.0857 0.0778 0.0786 0.0783 0.0748 0.0782\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_profile_and_prior_losses(\"HepG2_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/binary_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.3f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"train_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 7\n",
      "\tBest epoch in run: 1\n",
      "\tAssociated value: 0.23903661345442137\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 1:  0.260\n",
      "\tRun 2, epoch 1:  0.261\n",
      "\tRun 3, epoch 1:  0.242\n",
      "\tRun 4, epoch 1:  0.252\n",
      "\tRun 5, epoch 1:  0.243\n",
      "\tRun 6, epoch 1:  0.244\n",
      "\tRun 7, epoch 1:  0.239\n",
      "\tRun 8, epoch 1:  0.241\n",
      "\tRun 9, epoch 1:  0.244\n",
      "\tRun 10, epoch 1:  0.239\n",
      "\tRun 11, epoch 1:  0.254\n",
      "\tRun 12, epoch 1:  0.247\n",
      "\tRun 13, epoch 1:  0.242\n",
      "\tRun 14, epoch 1:  0.240\n",
      "\tRun 15, epoch 1:  0.243\n",
      "\tRun 16, epoch 1:  0.245\n",
      "\tRun 17, epoch 1:  0.254\n",
      "\tRun 18, epoch 1:  0.241\n",
      "\tRun 19, epoch 1:  0.242\n",
      "\tRun 20, epoch 1:  0.242\n",
      "\tRun 21, epoch 1:  0.255\n",
      "\tRun 22, epoch 1:  0.249\n",
      "\tRun 23, epoch 1:  0.242\n",
      "\tRun 24, epoch 1:  0.245\n",
      "\tRun 25, epoch 1:  0.247\n",
      "\tRun 26, epoch 1:  0.257\n",
      "\tRun 27, epoch 1:  0.255\n",
      "\tRun 28, epoch 1:  0.253\n",
      "\tRun 29, epoch 1:  0.241\n",
      "\tRun 30, epoch 1:  0.243\n",
      "\tRun 31, epoch 1:  0.245\n",
      "\tRun 32, epoch 1:  0.246\n",
      "\tRun 33, epoch 1:  0.247\n",
      "\tRun 34, epoch 1:  0.245\n",
      "\tRun 35, epoch 1:  0.245\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.249  0.194  0.166  0.146  0.129  0.115  0.105  0.098  0.092  0.088\n",
      "\t 0.260  0.274  0.300  0.341  0.356  0.375  0.403  0.432  0.435  0.464\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "2\n",
      "\t 0.250  0.195  0.167  0.148  0.134  0.123  0.115  0.107  0.099  0.091\n",
      "\t 0.261  0.269  0.293  0.332  0.348  0.378  0.389  0.436  0.434  0.458\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "3\n",
      "\t 0.239  0.180  0.151  0.131  0.116  0.105  0.097  0.090  0.085  0.080\n",
      "\t 0.242  0.259  0.289  0.326  0.340  0.380  0.401  0.442  0.459  0.488\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "4\n",
      "\t 0.249  0.194  0.165  0.147  0.132  0.121  0.113  0.106  0.101  0.096\n",
      "\t 0.252  0.275  0.296  0.324  0.358  0.402  0.426  0.432  0.450  0.483\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "5\n",
      "\t 0.240  0.180  0.151  0.131  0.116  0.106  0.097  0.090  0.085  0.081\n",
      "\t 0.243  0.259  0.283  0.316  0.352  0.371  0.402  0.443  0.450  0.474\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "6\n",
      "\t 0.239  0.179  0.149  0.129  0.114  0.103  0.095  0.088  0.083  0.078\n",
      "\t 0.244  0.259  0.281  0.307  0.343  0.371  0.406  0.421  0.436  0.461\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "7\n",
      "\t 0.243  0.183  0.153  0.132  0.117  0.105  0.097  0.091  0.085  0.080\n",
      "\t 0.239  0.255  0.291  0.324  0.353  0.361  0.384  0.429  0.440  0.467\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "8\n",
      "\t 0.238  0.178  0.149  0.129  0.114  0.103  0.094  0.088  0.083  0.078\n",
      "\t 0.241  0.259  0.285  0.314  0.343  0.355  0.378  0.404  0.424  0.455\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "9\n",
      "\t 0.241  0.181  0.151  0.130  0.115  0.104  0.095  0.089  0.084  0.079\n",
      "\t 0.244  0.256  0.285  0.317  0.339  0.367  0.381  0.424  0.453  0.450\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "10\n",
      "\t 0.238  0.179  0.150  0.129  0.114  0.104  0.095  0.089  0.083  0.079\n",
      "\t 0.239  0.251  0.286  0.317  0.341  0.365  0.393  0.412  0.426  0.449\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "11\n",
      "\t 0.245  0.193  0.165  0.145  0.132  0.118  0.107  0.099  0.092  0.087\n",
      "\t 0.254  0.267  0.297  0.319  0.357  0.362  0.393  0.439  0.445  0.457\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "12\n",
      "\t 0.239  0.181  0.152  0.131  0.117  0.106  0.097  0.090  0.084  0.081\n",
      "\t 0.247  0.260  0.280  0.324  0.343  0.366  0.397  0.412  0.427  0.464\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "13\n",
      "\t 0.236  0.179  0.149  0.128  0.113  0.103  0.095  0.088  0.083  0.078\n",
      "\t 0.242  0.257  0.283  0.310  0.342  0.362  0.395  0.407  0.440  0.471\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "14\n",
      "\t 0.238  0.180  0.149  0.129  0.115  0.103  0.095  0.089  0.083  0.079\n",
      "\t 0.240  0.254  0.295  0.312  0.342  0.379  0.405  0.420  0.440  0.480\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "15\n",
      "\t 0.236  0.180  0.152  0.131  0.116  0.105  0.096  0.090  0.084  0.080\n",
      "\t 0.243  0.254  0.281  0.303  0.348  0.369  0.411  0.424  0.438  0.450\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "16\n",
      "\t 0.238  0.180  0.152  0.133  0.118  0.107  0.099  0.092  0.087  0.082\n",
      "\t 0.245  0.258  0.277  0.311  0.341  0.376  0.394  0.418  0.437\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "17\n",
      "\t 0.248  0.188  0.156  0.135  0.120  0.108  0.099  0.093  0.087  0.082\n",
      "\t 0.254  0.261  0.289  0.320  0.350  0.372  0.395  0.427  0.448  0.454\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "18\n",
      "\t 0.236  0.177  0.147  0.127  0.113  0.102  0.094  0.088  0.082  0.078\n",
      "\t 0.241  0.261  0.283  0.323  0.341  0.373  0.413  0.412  0.439  0.453\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "19\n",
      "\t 0.240  0.180  0.151  0.130  0.115  0.104  0.096  0.089  0.084  0.079\n",
      "\t 0.242  0.254  0.288  0.311  0.339  0.361  0.387  0.425  0.427  0.455\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "20\n",
      "\t 0.239  0.180  0.151  0.132  0.117  0.106  0.097  0.090  0.085  0.080\n",
      "\t 0.242  0.257  0.287  0.313  0.340  0.366  0.385  0.426  0.433  0.460\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "21\n",
      "\t 0.248  0.193  0.157  0.136  0.121  0.109  0.102  0.094  0.088  0.083\n",
      "\t 0.255  0.267  0.295  0.318  0.353  0.389  0.402  0.441  0.447  0.482\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "22\n",
      "\t 0.237  0.178  0.149  0.129  0.115  0.104  0.096  0.089  0.083  0.079\n",
      "\t 0.249  0.259  0.289  0.316  0.345  0.366  0.394  0.415  0.446  0.472\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "23\n",
      "\t 0.242  0.180  0.150  0.130  0.115  0.104  0.096  0.089  0.084  0.079\n",
      "\t 0.242  0.255  0.280  0.315  0.336  0.359  0.401  0.416  0.460  0.473\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "24\n",
      "\t 0.243  0.183  0.154  0.132  0.117  0.106  0.097  0.090  0.085  0.080\n",
      "\t 0.245  0.265  0.283  0.332  0.348  0.389  0.407  0.440  0.449  0.465\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "25\n",
      "\t 0.239  0.181  0.152  0.131  0.117  0.106  0.097\n",
      "\t 0.247  0.251  0.290  0.313  0.349  0.373  0.393\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "26\n",
      "\t 0.248  0.194  0.159  0.138  0.124  0.113  0.104  0.097  0.091  0.086\n",
      "\t 0.257  0.271  0.297  0.319  0.342  0.370  0.396  0.412  0.429  0.438\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "27\n",
      "\t 0.246  0.186  0.155  0.135  0.121  0.110  0.102  0.094  0.089  0.084\n",
      "\t 0.255  0.261  0.290  0.317  0.347  0.373  0.406  0.418  0.438  0.459\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "28\n",
      "\t 0.247  0.194  0.165  0.146  0.132  0.122  0.114  0.107  0.101  0.096\n",
      "\t 0.253  0.270  0.301  0.333  0.358  0.379  0.402  0.432  0.446  0.484\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "29\n",
      "\t 0.237  0.180  0.150  0.130  0.114  0.104  0.095  0.089  0.083  0.079\n",
      "\t 0.241  0.257  0.314  0.315  0.344  0.375  0.396  0.418  0.447  0.459\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "30\n",
      "\t 0.238  0.180  0.150  0.130  0.116  0.105  0.097  0.090  0.087  0.080\n",
      "\t 0.243  0.254  0.288  0.314  0.339  0.363  0.390  0.416  0.444  0.460\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "31\n",
      "\t 0.243  0.182  0.152  0.132  0.118  0.106  0.098  0.091  0.086  0.081\n",
      "\t 0.245  0.262  0.296  0.329  0.361  0.382  0.408  0.433  0.469  0.489\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "32\n",
      "\t 0.240  0.181  0.150  0.130  0.115  0.104  0.096  0.089  0.084  0.079\n",
      "\t 0.246  0.259  0.292  0.312  0.339  0.387  0.399  0.432  0.447  0.467\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "33\n",
      "\t 0.238  0.179  0.151  0.130  0.116  0.107  0.096  0.090  0.084  0.079\n",
      "\t 0.247  0.257  0.290  0.329  0.343  0.393  0.401  0.418  0.448  0.467\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "34\n",
      "\t 0.242  0.183  0.154  0.133  0.118  0.108  0.099  0.092  0.086  0.081\n",
      "\t 0.245  0.255  0.280  0.306  0.361  0.359  0.413  0.417  0.446  0.455\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "35\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8812d2bc250d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnoprior_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_validation_and_prior_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HepG2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-26f9a83700ea>\u001b[0m in \u001b[0;36mprint_validation_and_prior_losses\u001b[0;34m(condition)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_metrics_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"%6.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_corr_losses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"%6.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_corr_losses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ca6cdb10af23>\u001b[0m in \u001b[0;36mimport_metrics_json\u001b[0;34m(models_path, run_num)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/att-priors/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_and_prior_losses(\"HepG2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 5\n",
      "\tBest epoch in run: 5\n",
      "\tAssociated value: 0.2355518709279989\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 5:  0.248\n",
      "\tRun 2, epoch 8:  0.251\n",
      "\tRun 3, epoch 9:  0.251\n",
      "\tRun 4, epoch 8:  0.248\n",
      "\tRun 5, epoch 5:  0.236\n",
      "\tRun 6, epoch 2:  0.252\n",
      "\tRun 7, epoch 7:  0.267\n",
      "\tRun 8, epoch 5:  0.251\n",
      "\tRun 9, epoch 4:  0.242\n",
      "\tRun 10, epoch 10:  0.256\n",
      "\tRun 11, epoch 8:  0.246\n",
      "\tRun 12, epoch 5:  0.250\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.329  0.288  0.272  0.247  0.219  0.201  0.188  0.175  0.175  0.178\n",
      "\t 0.322  0.293  0.302  0.262  0.248  0.254  0.261  0.268  0.268  0.272\n",
      "\t 0.079  0.068  0.081  0.041  0.035  0.035  0.032  0.030  0.074  0.044\n",
      "2\n",
      "\t 0.336  0.331  0.300  0.274  0.247  0.226  0.210  0.198  0.190  0.182\n",
      "\t 0.317  0.391  0.305  0.284  0.281  0.256  0.252  0.251  0.257  0.271\n",
      "\t 0.066  0.082  0.061  0.052  0.051  0.046  0.044  0.045  0.045  0.076\n",
      "3\n",
      "\t 0.330  0.289  0.275  0.269  0.268  0.249  0.241  0.224  0.211  0.198\n",
      "\t 0.328  0.289  0.275  0.306  0.278  0.277  0.258  0.256  0.251  0.251\n",
      "\t 0.067  0.055  0.062  0.090  0.066  0.067  0.053  0.045  0.040  0.053\n",
      "4\n",
      "\t 0.334  0.277  0.255  0.233  0.246  0.239  0.218  0.207  0.192  0.178\n",
      "\t 0.303  0.317  0.267  0.282  0.273  0.252  0.263  0.248  0.259  0.254\n",
      "\t 0.068  0.107  0.046  0.074  0.081  0.059  0.045  0.041  0.032  0.046\n",
      "5\n",
      "\t 0.336  0.279  0.263  0.237  0.208  0.190  0.173  0.158  0.145  0.135\n",
      "\t 0.308  0.353  0.318  0.246  0.236  0.242  0.255  0.278  0.294  0.319\n",
      "\t 0.065  0.095  0.074  0.035  0.034  0.032  0.029  0.029  0.029  0.033\n",
      "6\n",
      "\t 0.305  0.239  0.211  0.198  0.179  0.160  0.204  0.246  0.242  0.243\n",
      "\t 0.289  0.252  0.252  0.258  0.268  0.292  0.292  0.280  0.311  0.272\n",
      "\t 0.039  0.038  0.038  0.038  0.039  0.033  0.096  0.101  0.102  0.109\n",
      "7\n",
      "\t 0.351  0.311  0.299  0.291  0.281  0.261  0.251  0.243  0.240  0.241\n",
      "\t 0.331  0.335  0.321  0.301  0.297  0.268  0.267  0.274  0.284  0.296\n",
      "\t 0.074  0.086  0.088  0.069  0.071  0.055  0.059  0.068  0.113  0.067\n",
      "8\n",
      "\t 0.318  0.249  0.225  0.211  0.197  0.188  0.176  0.165  0.155  0.146\n",
      "\t 0.296  0.279  0.324  0.252  0.251  0.259  0.264  0.268  0.285  0.289\n",
      "\t 0.054  0.043  0.068  0.038  0.036  0.036  0.037  0.036  0.035  0.034\n",
      "9\n",
      "\t 0.335  0.271  0.223  0.198  0.179  0.168  0.161  0.147  0.138  0.136\n",
      "\t 0.321  0.265  0.244  0.242  0.254  0.253  0.267  0.287  0.283  0.290\n",
      "\t 0.089  0.036  0.029  0.028  0.029  0.043  0.031  0.033  0.032  0.062\n",
      "10\n",
      "\t 0.329  0.319  0.292  0.276  0.290  0.291  0.267  0.241  0.224  0.208\n",
      "\t 0.340  0.328  0.310  0.290  0.311  0.313  0.294  0.265  0.262  0.256\n",
      "\t 0.109  0.117  0.084  0.073  0.079  0.088  0.056  0.038  0.043  0.051\n",
      "11\n",
      "\t 0.336  0.294  0.283  0.262  0.254  0.255  0.225  0.207  0.192  0.178\n",
      "\t 0.295  0.328  0.271  0.284  0.292  0.265  0.248  0.246  0.254  0.268\n",
      "\t 0.059  0.086  0.046  0.083  0.073  0.048  0.044  0.038  0.033  0.032\n",
      "12\n",
      "\t 0.302  0.287  0.254  0.239  0.223  0.208  0.201  0.184\n",
      "\t 0.300  0.296  0.264  0.258  0.250  0.256  0.250  0.251\n",
      "\t 0.055  0.054  0.051  0.055  0.047  0.051  0.041  0.043\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_and_prior_losses(\"HepG2_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4e71499490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation loss\"\n",
    "title += \"\\nTraining on only 1% of positive bins\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.791537416772594\n",
      "0.00602662526589495\n"
     ]
    }
   ],
   "source": [
    "np_vals, p_vals = np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(t)\n",
    "print(p / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
