{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_extract_func, metric_compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    where the metric value is fetched by `metric_extract_func`. This function must\n",
    "    take the imported metrics JSON and return the (scalar) value to use for\n",
    "    comparison. The best metric value is determiend by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = metric_extract_func(metrics[run_num])\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to extract metric for run %s\" % run_num)\n",
    "            continue\n",
    "        all_vals[run_num] = val\n",
    "        if best_val is None or metric_compare_func(val, best_val):\n",
    "            best_val, best_run = val, run_num\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run: 25\n",
      "Associated value: 201.23614661308147\n",
      "1 226.36899854799196\n",
      "2 226.30391280538535\n",
      "3 226.22971023101152\n",
      "4 226.35281953279554\n",
      "5 226.26141019501912\n",
      "6 226.25596731619774\n",
      "7 226.26553161435345\n",
      "8 226.2545102013937\n",
      "9 226.26304027148996\n",
      "10 226.31571769714355\n",
      "11 226.290281580547\n",
      "12 226.450161585992\n",
      "14 218.1957363996383\n",
      "15 226.25977567746403\n",
      "16 226.25721335733743\n",
      "17 226.2721361806976\n",
      "18 226.26501096076024\n",
      "19 203.23399234493402\n",
      "21 226.26376073572996\n",
      "22 220.9367990053302\n",
      "23 226.2398168648432\n",
      "25 201.23614661308147\n",
      "26 226.27806446964578\n",
      "27 226.269721755654\n"
     ]
    }
   ],
   "source": [
    "models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/SPI1_prior_savedata_gradabs/\"\n",
    "best_run, best_val, all_vals = get_best_metric(\n",
    "    models_path,\n",
    "    lambda metrics: np.mean(metrics[\"summit_prof_corr_losses\"][\"values\"]),\n",
    "    lambda x, y: x < y\n",
    ")\n",
    "print(\"Best run: %s\" % best_run)\n",
    "print(\"Associated value: %s\" % best_val)\n",
    "for key in sorted(all_vals.keys(), key=lambda x: int(x)):\n",
    "    print(key, all_vals[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
