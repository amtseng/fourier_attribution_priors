{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the (one-indexed) number of the epch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_validation_profile_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_pos_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 13\n",
      "\tBest epoch in run: 5\n",
      "\tAssociated value: 85.22217027169687\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 5:  86.00\n",
      "\tRun 2, epoch 5:  85.75\n",
      "\tRun 3, epoch 5:  86.44\n",
      "\tRun 4, epoch 5:  86.10\n",
      "\tRun 5, epoch 5:  85.95\n",
      "\tRun 6, epoch 5:  85.28\n",
      "\tRun 7, epoch 5:  85.34\n",
      "\tRun 8, epoch 5:  85.50\n",
      "\tRun 9, epoch 5:  85.71\n",
      "\tRun 10, epoch 5:  85.76\n",
      "\tRun 11, epoch 5:  86.04\n",
      "\tRun 12, epoch 5:  85.25\n",
      "\tRun 13, epoch 5:  85.22\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 91.37  86.82  86.22  85.96  85.79\n",
      "\t 87.56  86.77  86.35  86.20  86.00\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t 91.33  86.77  86.19  85.89  85.71\n",
      "\t 87.38  86.54  86.04  85.98  85.75\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t 93.08  87.88  86.56  86.16  85.93\n",
      "\t 89.12  87.51  86.85  86.62  86.44\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t 91.77  87.37  86.59  86.20  85.98\n",
      "\t 88.13  87.14  86.68  86.20  86.10\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t 90.32  86.62  86.05  85.84  85.69\n",
      "\t 87.42  86.48  86.17  86.00  85.95\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t 90.71  86.79  86.20  85.91  85.73\n",
      "\t 86.70  85.96  85.57  85.39  85.28\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t 90.04  86.54  85.97  85.70  85.52\n",
      "\t 86.67  86.04  85.58  85.37  85.34\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "8\n",
      "\t 90.54  86.87  86.24  85.95  85.76\n",
      "\t 87.19  86.17  85.89  85.68  85.50\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "9\n",
      "\t 91.84  87.04  86.38  86.08  85.89\n",
      "\t 87.55  86.55  86.05  86.03  85.71\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "10\n",
      "\t 92.75  87.15  86.34  85.96  85.75\n",
      "\t 87.78  86.66  86.33  86.00  85.76\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "11\n",
      "\t 94.13  87.21  86.44  86.08  85.89\n",
      "\t 88.98  86.77  86.46  86.19  86.04\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "12\n",
      "\t 91.39  86.69  86.14  85.87  85.69\n",
      "\t 86.63  85.78  85.66  85.36  85.25\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "13\n",
      "\t 94.09  87.63  86.58  86.17  85.94\n",
      "\t 88.59  86.18  85.62  85.38  85.22\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_profile_and_prior_losses(\"exploring_data_dropping_1/SPI1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 7\n",
      "\tBest epoch in run: 5\n",
      "\tAssociated value: 85.98335747895418\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 4:  86.95\n",
      "\tRun 2, epoch 5:  86.69\n",
      "\tRun 3, epoch 5:  86.16\n",
      "\tRun 4, epoch 3:  86.89\n",
      "\tRun 5, epoch 5:  86.63\n",
      "\tRun 6, epoch 5:  86.56\n",
      "\tRun 7, epoch 5:  85.98\n",
      "\tRun 8, epoch 2:  86.75\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 92.18  87.24  86.73  86.50  86.33\n",
      "\t 88.05  87.27  87.17  86.95  86.95\n",
      "\t0.1640 0.0804 0.0698 0.0598 0.0566\n",
      "2\n",
      "\t 91.01  87.23  86.89  86.71  86.58\n",
      "\t 87.69  87.11  87.04  87.00  86.69\n",
      "\t0.1529 0.0861 0.0732 0.0662 0.0583\n",
      "3\n",
      "\t 90.60  87.11  86.77  86.63  86.51\n",
      "\t 87.01  86.52  86.32  86.19  86.16\n",
      "\t0.1602 0.0843 0.0711 0.0648 0.0610\n",
      "4\n",
      "\t 93.79  87.74  87.04  86.77  86.60\n",
      "\t 88.31  87.27  86.89  86.92  86.97\n",
      "\t0.1677 0.0872 0.0725 0.0678 0.0617\n",
      "5\n",
      "\t 94.11  87.37  86.86  86.65  86.55\n",
      "\t 87.69  86.94  86.82  86.68  86.63\n",
      "\t0.1541 0.0825 0.0719 0.0666 0.0610\n",
      "6\n",
      "\t 90.98  87.23  86.77  86.56  86.41\n",
      "\t 87.58  87.37  87.02  86.74  86.56\n",
      "\t0.2059 0.0856 0.0720 0.0688 0.0632\n",
      "7\n",
      "\t 93.20  87.63  86.99  86.72  86.58\n",
      "\t 87.39  86.53  86.18  86.10  85.98\n",
      "\t0.1701 0.0884 0.0747 0.0668 0.0647\n",
      "8\n",
      "\t 93.13  87.58\n",
      "\t 87.74  86.75\n",
      "\t0.1628 0.0840\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_profile_and_prior_losses(\"exploring_data_dropping_1/SPI1_prior_freqlimit160\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 6\n",
      "\tBest epoch in run: 19\n",
      "\tAssociated value: 91.9427117948179\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 20:  93.40\n",
      "\tRun 2, epoch 20:  92.65\n",
      "\tRun 3, epoch 18: 105.04\n",
      "\tRun 4, epoch 20:  91.99\n",
      "\tRun 5, epoch 19:  93.78\n",
      "\tRun 6, epoch 19:  91.94\n",
      "\tRun 7, epoch 19:  94.60\n",
      "\tRun 8, epoch 19: 102.17\n",
      "\tRun 9, epoch 18:  92.81\n",
      "\tRun 10, epoch 19:  93.73\n",
      "\tRun 11, epoch 20:  95.92\n",
      "\tRun 12, epoch 20:  94.08\n",
      "\tRun 13, epoch 11:  93.98\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t572.42 443.62 379.60 329.01 300.95 287.60 279.16 273.79 271.58 266.72 262.77 261.26 257.82 256.61 254.67 253.49 252.24 251.08 249.46 248.24\n",
      "\t116.58 109.75 104.72 101.59  98.59  97.74  96.95  96.22  95.53  95.39  94.91  94.62  94.49  94.12  94.16  93.98  93.71  93.79  93.66  93.40\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t610.45 482.66 396.23 339.99 304.45 287.79 277.43 272.42 266.56 263.32 260.31 258.85 254.76 252.53 251.44 249.18 247.87 247.17 246.37 246.13\n",
      "\t118.71 112.51 108.80 100.82  97.65  96.47  95.69  95.05  94.65  94.18  94.65  93.88  93.56  93.22  93.37  93.10  92.91  93.13  94.07  92.65\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t598.20 481.81 443.20 419.15 400.33 368.75 357.65 335.74 327.28 316.39 311.65 304.86 297.39 305.06 291.74 288.70 290.94 282.39 279.50 277.78\n",
      "\t117.06 113.19 115.68 115.17 111.41 112.18 108.03 108.64 108.38 108.23 109.57 105.61 105.95 106.14 105.34 105.73 106.32 105.04 105.30 105.20\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t591.29 464.06 385.82 338.49 308.00 290.96 279.16 274.04 267.81 264.81 261.82 259.98 258.04 256.10 255.25 253.61 252.18 251.91 250.85 248.84\n",
      "\t117.08 110.00 105.69 101.48  98.81  96.03  94.87  94.18  93.53  93.59  93.31  93.08  92.70  92.80  92.55  92.54  92.78  92.41  92.18  91.99\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t613.53 521.22 433.64 380.85 333.51 307.09 290.58 283.29 276.19 270.94 267.75 264.34 262.90 261.01 257.90 256.61 255.71 254.64 253.29 251.61\n",
      "\t119.43 113.74 110.93 106.11 102.02  99.28  97.71  97.45  95.86  95.79  95.14  94.94  94.89  94.45  94.85  94.50  94.70  94.03  93.78  94.88\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t624.59 618.60 574.96 458.64 372.99 321.09 298.87 284.58 279.17 270.89 268.07 264.55 260.76 259.64 256.64 257.68 254.76 251.91 250.77 253.12\n",
      "\t123.47 120.88 115.00 109.40 103.60 101.00  99.46  96.50  95.83  94.93  94.38  93.83  93.25  92.88  92.75  93.01  92.19  92.38  91.94  92.24\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "7\n",
      "\t602.98 496.50 433.02 397.79 362.53 330.96 311.36 297.26 288.36 282.40 276.56 272.69 269.68 266.39 265.98 261.93 260.56 259.74 257.56 255.81\n",
      "\t117.15 112.25 111.89 109.03 105.10 102.53 100.18  99.00  98.39  97.32  97.25  96.51  96.34  96.02  95.80  95.26  95.37  94.94  94.60  94.65\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "8\n",
      "\t589.79 488.12 437.58 407.10 373.25 346.74 330.78 314.80 304.82 299.65 292.15 287.95 283.99 282.38 278.42 275.52 271.79 271.40 275.23 266.10\n",
      "\t121.67 112.92 111.48 114.53 108.19 106.37 105.21 106.68 104.40 105.85 103.56 105.14 104.83 103.51 102.53 102.66 106.24 104.79 102.17 102.29\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "9\n",
      "\t593.87 467.54 397.63 346.35 310.26 295.17 285.41 276.51 272.62 270.52 267.00 262.50 260.20 257.89 256.35 255.02 252.89 252.39 251.40 250.78\n",
      "\t116.86 112.19 110.82 101.40  98.39  96.82  95.82  94.91  94.87  94.06  93.90  93.78  93.38  93.57  93.28  93.27  92.83  92.81  92.88  93.31\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "10\n",
      "\t589.93 472.26 399.45 348.63 315.20 295.70 284.96 276.96 271.47 268.09 264.32 262.11 259.12 257.84 255.85 253.71 252.79 251.86 251.34 249.76\n",
      "\t116.08 111.81 107.99 102.97  99.75  98.01  96.40  95.55  95.25  94.97  94.41  94.27  94.68  94.24  94.29  94.17  93.79  93.75  93.73  94.18\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "11\n",
      "\t602.78 482.79 431.99 406.54 373.72 343.22 321.01 307.32 293.22 286.41 282.58 275.31 272.45 269.44 266.52 265.15 263.68 261.28 260.52 260.04\n",
      "\t118.82 112.40 112.79 111.34 107.19 105.78 103.72 102.80 100.37  99.60  99.64  98.48  98.16  97.33  97.14  97.05  96.74  96.80  96.12  95.92\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "12\n",
      "\t608.89 491.11 428.21 389.83 352.16 322.16 305.09 291.97 283.29 277.50 273.87 270.08 267.53 265.80 262.74 260.17 259.62 256.56 255.04 254.23\n",
      "\t117.60 114.47 110.15 107.39 104.42 100.84 100.07  97.71  97.06  96.30  96.18  95.90  95.74  95.59  95.06  94.51  94.24  94.73  94.86  94.08\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "13\n",
      "\t616.46 519.44 422.94 367.16 324.51 302.93 288.88 281.30 275.93 271.48 267.18 263.69 261.53\n",
      "\t119.40 112.35 108.22 104.25 100.00  98.15  96.52  95.80  94.79  94.71  93.98  94.04\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_profile_and_prior_losses(\"SPI1_keep1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "Warning: Was not able to compute values for run 12\n",
      "\tBest run: 2\n",
      "\tBest epoch in run: 19\n",
      "\tAssociated value: 91.07023725156431\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 20:  91.89\n",
      "\tRun 2, epoch 19:  91.07\n",
      "\tRun 3, epoch 20:  92.43\n",
      "\tRun 4, epoch 19: 105.30\n",
      "\tRun 5, epoch 20:  91.96\n",
      "\tRun 6, epoch 20:  99.70\n",
      "\tRun 7, epoch 20:  98.19\n",
      "\tRun 8, epoch 20:  91.64\n",
      "\tRun 9, epoch 19:  92.28\n",
      "\tRun 10, epoch 20:  91.46\n",
      "\tRun 11, epoch 20:  91.62\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t619.08 618.07 593.37 489.39 414.57 360.76 322.61 303.17 292.02 283.24 274.93 270.42 266.21 263.27 260.26 258.37 257.16 255.68 253.90 253.37\n",
      "\t121.50 121.13 118.27 116.20 107.56 102.08  99.50  98.02  97.36  95.16  94.52  94.44  93.98  93.49  92.59  92.31  92.18  91.96  91.92  91.89\n",
      "\t0.4167 0.3928 0.3219 0.3588 0.2896 0.2433 0.2135 0.1925 0.1707 0.1771 0.1703 0.1612 0.1604 0.1634 0.1503 0.1584 0.1489 0.1579 0.1510 0.1403\n",
      "2\n",
      "\t616.63 517.88 432.53 384.06 345.42 306.89 291.27 281.07 275.39 268.99 265.56 261.77 259.28 257.23 255.02 254.09 252.78 251.37 250.61 249.55\n",
      "\t118.97 112.36 110.89 105.82 100.58  97.29  96.33  94.65  94.56  93.11  92.73  92.58  92.15  91.93  91.54  91.66  91.52  91.27  91.07  91.09\n",
      "\t0.4899 0.4095 0.3890 0.3760 0.2783 0.2499 0.2567 0.2177 0.2068 0.1934 0.1867 0.1809 0.1741 0.1652 0.1701 0.1671 0.1521 0.1498 0.1534 0.1420\n",
      "3\n",
      "\t600.22 473.09 430.29 392.37 355.87 318.89 294.80 280.59 271.86 267.14 264.78 261.27 258.87 257.60 256.34 254.72 253.12 252.92 251.63 251.67\n",
      "\t118.31 113.07 111.74 110.06 106.74 102.81  97.93  96.59  94.66  94.84  93.64  93.39  93.14  92.99  93.57  92.71  92.86  92.88  92.61  92.43\n",
      "\t0.4674 0.4465 0.4317 0.4274 0.4242 0.4043 0.3476 0.2679 0.2463 0.2360 0.2206 0.2185 0.2127 0.2070 0.1980 0.1965 0.1863 0.1804 0.1865 0.1775\n",
      "4\n",
      "\t576.97 468.33 431.07 408.05 385.24 359.39 343.84 337.00 319.03 316.43 304.36 303.45 296.69 291.87 290.60 287.35 284.82 280.38 277.51 274.92\n",
      "\t119.52 112.51 113.19 110.75 110.29 109.36 110.55 108.06 109.82 111.15 107.54 107.95 106.04 105.35 106.87 105.96 105.45 106.33 105.30 106.44\n",
      "\t0.4928 0.4827 0.4765 0.4750 0.4759 0.4746 0.4694 0.4605 0.4523 0.4315 0.4276 0.4194 0.3918 0.3715 0.3523 0.3454 0.3353 0.3271 0.3195 0.3111\n",
      "5\n",
      "\t595.60 494.21 425.41 385.75 338.10 307.22 291.00 280.32 274.14 270.30 265.49 262.07 261.27 258.61 257.03 254.22 252.69 251.30 250.47 249.51\n",
      "\t117.92 111.63 112.86 106.79 102.38  99.58  96.60  95.20  95.01  94.28  93.75  93.42  93.98  92.89  93.01  92.46  92.35  92.75  92.25  91.96\n",
      "\t0.4866 0.4476 0.4211 0.3827 0.3157 0.2663 0.2315 0.2095 0.2044 0.1918 0.1853 0.1782 0.1798 0.1710 0.1685 0.1629 0.1574 0.1558 0.1567 0.1538\n",
      "6\n",
      "\t604.52 493.21 441.62 422.80 402.23 376.92 351.10 335.43 317.12 312.06 297.33 291.86 287.34 288.07 279.03 276.18 274.57 271.29 272.04 270.59\n",
      "\t116.98 113.59 112.45 113.34 111.57 111.77 107.75 110.39 106.62 104.55 103.54 102.23 103.82 101.31 101.56 100.59 100.80 100.47 100.24  99.70\n",
      "\t0.5053 0.4791 0.4710 0.4659 0.4667 0.4645 0.4597 0.4524 0.4285 0.3953 0.3556 0.3219 0.3114 0.2964 0.2992 0.2905 0.2813 0.2744 0.2714 0.2652\n",
      "7\n",
      "\t590.32 479.83 432.64 406.53 376.70 358.46 325.45 308.29 295.60 290.53 281.67 277.65 273.97 270.55 268.02 265.29 262.17 261.17 260.12 258.04\n",
      "\t120.57 114.82 112.62 110.86 111.31 107.81 106.84 103.99 104.61 101.63 101.38 100.84 100.65 100.09  99.69  99.39  99.21  99.12  99.40  98.19\n",
      "\t0.4672 0.4584 0.4572 0.4597 0.4619 0.4199 0.3283 0.2922 0.2911 0.2670 0.2662 0.2554 0.2518 0.2499 0.2487 0.2425 0.2391 0.2350 0.2333 0.2285\n",
      "8\n",
      "\t609.28 484.29 388.28 331.50 302.04 289.46 279.21 272.08 266.60 262.30 258.97 256.72 255.10 252.96 251.70 250.22 249.63 247.94 246.72 246.17\n",
      "\t118.72 111.75 106.76  99.71  98.11  97.07  95.28  94.69  94.31  93.63  93.47  93.00  92.44  92.56  92.27  92.32  92.05  91.69  91.72  91.64\n",
      "\t0.4558 0.3999 0.3459 0.2805 0.2345 0.2106 0.2056 0.2001 0.1890 0.1772 0.1736 0.1703 0.1630 0.1587 0.1528 0.1476 0.1526 0.1452 0.1429 0.1417\n",
      "9\n",
      "\t620.35 601.74 520.69 434.72 376.55 327.13 304.69 290.72 284.23 277.41 274.80 268.29 264.65 262.48 259.84 257.98 256.55 254.35 252.75 251.83\n",
      "\t122.18 119.60 113.97 110.12 103.42  99.28  97.59  96.63  97.29  95.35  94.75  94.56  93.72  94.22  93.20  93.35  92.55  92.58  92.28  92.49\n",
      "\t0.4654 0.4067 0.3983 0.3757 0.3091 0.2303 0.1926 0.1796 0.1663 0.1556 0.1509 0.1492 0.1410 0.1420 0.1400 0.1385 0.1348 0.1303 0.1295 0.1277\n",
      "10\n",
      "\t605.02 472.83 397.38 337.94 301.67 285.40 275.82 268.81 265.46 261.89 258.02 255.69 253.84 251.69 250.53 249.72 249.04 247.76 246.64 245.23\n",
      "\t118.42 110.53 106.19 100.79  97.50  95.75  94.75  94.38  93.47  93.44  92.68  92.58  92.25  92.27  92.05  92.02  91.69  91.94  91.52  91.46\n",
      "\t0.4739 0.4415 0.3950 0.3228 0.2320 0.2054 0.1965 0.1853 0.1784 0.1732 0.1652 0.1620 0.1584 0.1576 0.1533 0.1528 0.1530 0.1489 0.1467 0.1444\n",
      "11\n",
      "\t605.28 469.43 370.43 318.20 292.81 281.16 272.65 266.73 260.49 257.69 254.42 252.63 250.57 248.81 247.83 246.32 245.44 244.74 243.78 243.22\n",
      "\t119.30 109.96 103.07  98.93  96.84  95.61  94.80  93.87  93.43  93.03  92.58  92.41  92.42  92.16  92.05  91.99  91.96  92.07  91.92  91.62\n",
      "\t0.4419 0.3966 0.3460 0.2842 0.2346 0.2017 0.1854 0.1736 0.1681 0.1610 0.1533 0.1474 0.1474 0.1433 0.1408 0.1401 0.1407 0.1365 0.1367 0.1340\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_profile_and_prior_losses(\"SPI1_prior_keep1_freqlimit160_limitsoft0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcd38376610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5wV9X3/8ddbrgbwghAjouItaRJAVCRcrKikjUYT46MkqFUwjQ9qGmtNMDbxl1i0l19sjEabNlbjPUYxxhhi9WdsK6IVxZUActFI1egGooCCgkEFP78/5nvWw9lzds/unrNncd7Px+M8di7f78xnZmc/O+c7M99RRGBmZvmxU6MDMDOz7uXEb2aWM078ZmY548RvZpYzTvxmZjnjxG9mljNO/D2UpOWSjm50HI0k6WRJL0naJOnQGi73TEmPFI1vknRANWU7sa77JM3obP3uImlPSfMlvSHpe5IulPSjNG+EpJDUuxPLnS3px7WP2Lqiw79I6zpJLwBnRcR/Fk07M007EiAiPl7FckYAzwN9ImJrPWJtsMuAcyLiF/VcSUQMrMVyJM0GDoqI04uWfXwtlt0NZgLrgF3CD/e87/mM3yrqzBleje0HLG9wDDu8Kn+P+wErnPTzwYm/h5L0gqRPpuFxkpokvS7pZUmXp2Lz088NqbligqSdJH1L0m8lvSLpZkm7Fi13epq3XtK3S9YzW9Kdkn4s6XXgzLTuBZI2SFoj6QeS+hYtLyT9laRnUzPB30s6MNV5XdIdxeVLtrFsrJL6SdoE9AKWSPrfMnWvlnRZybRfSPpaGv6GpP9NMa2QdHIb+zokHZSG95A0N8W+EDiwpOyVqfnpdUlPSvrjNP044EJgWvpdLEnT50k6q63tTfMKzSkzJL0oaZ2k/9NGzDemffBA2saHJO1Xsk1fkfQs8GyaNlHSE5I2pp8TC8sCZgAXpNg/2VYTTfodXZeOh99J+gdJvSrFWlL3s8qaMTekffPRonl/m5b3hqRnJE1J0ysd/9ZZEeFPN3+AF4BPlkw7E3ikXBlgAXBGGh4IjE/DI4AAehfV+wtgFXBAKnsXcEua9zFgE3Ak0JesKeWdovXMTuOfIzsp2Bk4HBhP1iw4AlgJnFe0vgDmArsAHwfeAv4rrX9XYAUwo8J+qBhr0bIPqlD3KOAlQGl8d+APwLA0/nlgWNqOacBmYK8K+7plPcDtwB3AAGAk8LuSsqcDe6T9MQv4PdC/aP/9uCTOeWRNeO39bgq/y2vTfj8k7cuPVtj+G4E30n7oB1xZZpseAAan5Q0GXgPOSLGfmsb3KFrePxTVb9kWSo4z4G7g39M++iCwEPjLCnEWL+fD6ffwJ0Af4IK0P/oCH0m/z2FF6zywrePfn85/fMbfOHens54NkjYA/9ZG2XeAgyQNiYhNEfFYG2X/HLg8Ip6LiE3AN4FTlH3dnwr8MiIeiYi3gYvI/qCLLYiIuyPi3Yj4Q0Q8GRGPRcTWiHiB7A9+ckmdSyPi9YhYDiwDfpXWvxG4D6h0YbatWNvzcIr9j9P41BT7aoCI+GlErE7bMYfsrHdcWwtMZ61/BlwUEZsjYhlwU3GZiPhxRKxP++N7ZEn3I1XEW+32Xpz2+xJgCdk/gEr+IyLmR8RbwP8BJkjap2j+/42IVyPiD8AJwLMRcUuK/TbgaeAzVcYOZBeBgePJ/vlvjohXgCuAU6qoPi3F/EBEvEN24rEzMBHYRrYvPyapT0S8EBGFb3odOf6tCk78jfO5iNit8AH+qo2yXyI7W3o6fUU/sY2yw4DfFo3/luwMb88076XCjIh4E1hfUv+l4hFJH5Z0j6Tfp+affwKGlNR5uWj4D2XGK108bSvWNkVEkJ2dn5omnQbcWhT3dEmLi/6xjiwTd6mhaf3F+6A4PiTNkrQyNZdsIPtW095yC6rZ3t8XDb9J5X0H2/8uNwGvpnW0ml9m3YX1791u1Nvbj+xsfU3Rvv13sjP/9mwXQ0S8m2LcOyJWAeeRfUN4RdLtkgrb0pHj36rgxL8DiIhnI+JUsj+uS4E7JQ2g9dk6wGqyP86CfYGtZMl4DTC8MEPSzmTNFtutrmT8h2RnhgdHxC5k7djq/NZUHWs1bgOmprbtTwA/A0jj1wLnkDVl7Eb2TaS9uNem9RefNe9bGEjt+X8LfAHYPS13Y9Fy27sw2tXtLdUSp6SBZM05q4vmF8dTuu7C+n/XwXW+RNYENaToxGWXqOIutNIYJIlsG34HEBE/ieyutv1S7Jem6ZWOf+skJ/4dgKTTJQ1NZ0gb0uRtZInqXbI244LbgK9K2j8lg38C5kR2u+edwGfSRb6+wMW0nwwHAa8DmyT9EfDlmm1Y27G2KyJ+TbYPfgTcHxGFfVP4p7gWQNIXyc7421veNrJ299mSPiDpY2QXPQsGkSXqtUBvSReRXdsoeBkYIanS31WXtreMT0s6Mv0u/x54PCJeqlD2XuDDkk6T1FvSNLJrPvd0ZIURsQb4FfA9SbukC9YHSipt/ivnDuAESVMk9SG7RvIW8Kikj0g6VlI/YAvZN8Vt0Obxb53kxL9jOA5YruxOlyuBUyJiS2qq+Ufgf9LX7vHA9cAtZHf8PE/2R/TXAKkN/q/JmkjWkF0cfIXsj6+S88maUd4gO4ueU8PtqhhrB9wGfBL4SWFCRKwAvkd2UfBlYBTwP1Uu7xyy5pXfk13wvKFo3v1k1yx+Q9ZksYXtm1N+mn6ul7SozLJrsb3FfgL8HVkTz+Fk1xDKioj1wIlkyXY92YXVEyNiXSfWO53sguwKsgvEdwJ7tVcpIp4huzj+L2TPDHwG+Ey63tQP+E6a/nuys/sLU9Wyx38n4rakcEeE5VA669xA1ozzfKPjseqlWzCbI+JbjY7Fdjw+488ZSZ9JzRgDyO6qeIrs1lEzywkn/vw5iewi22rgYLKvzf7aZ5YjbuoxM8sZn/GbmeVMwzrhGjJkSIwYMaJRqzcz2yE9+eST6yJiaFeW0bDEP2LECJqamhq1ejOzHZKk0iewO8xNPWZmOePEb2aWM078ZmY50+g3LJnZ+8Q777xDc3MzW7a4N4Va6N+/P8OHD6dPnz41X7YTv5nVRHNzM4MGDWLEiBFkHW9aZ0UE69evp7m5mf3337/my3dTj5nVxJYtW9hjjz2c9GtAEnvssUfdvj21m/gl9Ze0UNKS9K7Mi8uU6SdpjqRVkh6XNKIewZpZz+akXzv13JfVnPG/BRwbEYcAY4DjUve/xb4EvBYRB5G9hu3S2oZpZma10m7ij8ymNNonfUo7+DmJ995NeicwRf7Xb5ZvUm0/7a5OzJo1q2X8sssuY/bs2XXZtIkTJ9Zlud2lqjZ+Sb0kLSZ7accDEfF4SZG9SS+kSG8T2kjrV/ohaaakJklNa9eu7XzUXTg4zOz9qV+/ftx1112sW9eZd8tUZ9u27MVfjz76aIfr9CRVJf6I2BYRY8je1zpOUulr7Mpl3FbdfkbENRExNiLGDh3apa4mzMy207t3b2bOnMkVV1zRat5vf/tbpkyZwujRo5kyZQovvvhiqzKzZ8/mjDPO4Nhjj+Xggw/m2muvBWDevHkcc8wxnHbaaYwaNQqAgQMHAtndN1//+tcZOXIko0aNYs6cORXr9CQdup0zIjZImkf2KrRlRbOayV6a3CypN7Ar2evgzMy6zVe+8hVGjx7NBRdcsN30c845h+nTpzNjxgyuv/56zj33XO6+++5W9ZcuXcpjjz3G5s2bOfTQQznhhBMAWLhwIcuWLWt1a+Vdd93F4sWLWbJkCevWreOII47gqKOOarNOT1DNXT1DJe2Whncme7/p0yXF5vLeS6mnAv/tl3uYWXfbZZddmD59OlddddV20xcsWMBpp50GwBlnnMEjjzxStv5JJ53EzjvvzJAhQzjmmGNYuHAhAOPGjSubwB955BFOPfVUevXqxZ577snkyZN54okn2qzTE1TT1LMX8KCkpcATZG3890i6RNJnU5nrgD0krQK+BnyjPuGambXtvPPO47rrrmPz5s0Vy1S696R0emF8wIABZcu3dX5bqU5PUM1dPUsj4tCIGB0RIyPikjT9ooiYm4a3RMTnI+KgiBgXEc/VO3Azs3IGDx7MF77wBa677rqWaRMnTuT2228H4NZbb+XII48sW/cXv/gFW7ZsYf369cybN48jjjiizXUdddRRzJkzh23btrF27Vrmz5/PuHHjarcxdeInd82sPiJq++mAWbNmbXd3z1VXXcUNN9zA6NGjueWWW7jyyivL1hs3bhwnnHAC48eP59vf/jbDhg1rcz0nn3wyo0eP5pBDDuHYY4/ln//5n/nQhz7UoVgboWHv3B07dmx0+kUsbd226UsLZg2xcuVKPvrRjzY6jE6bPXs2AwcO5Pzzz290KC3K7VNJT0bE2K4s12f8ZmY54945zcygbk/59kQ+4zczyxknfjOznHHiNzPLGSd+M7OcceI3s7rozl6Zv/rVr/L973+/ZfxTn/oUZ511Vsv4rFmzuPzyy1m9ejVTp04FYPHixdx7770tZWbPns1ll11Ws+2/+uqrufnmm2u2vFpy4jezHd7EiRNbukp+9913WbduHcuXL2+Z/+ijjzJp0iSGDRvGnXfeCbRO/LW0detWzj77bKZPn96hOt3Fid/MdniTJk1qSfzLly9n5MiRDBo0iNdee4233nqLlStXcuihh/LCCy8wcuRI3n77bS666CLmzJnDmDFjWrpTXrFiBUcffTQHHHBAq47eCgYOHMisWbM47LDDmDJlCoV3ixx99NFceOGFTJ48mSuvvHK7bxCLFy9m/PjxjB49mpNPPpnXXnutbJ3u4sRvZju8YcOG0bt3b1588UUeffRRJkyYwCc+8QkWLFhAU1MTo0ePpm/fvi3l+/btyyWXXMK0adNYvHgx06ZNA+Dpp5/m/vvvZ+HChVx88cW88847rda1efNmDjvsMBYtWsTkyZO5+OL3XkO+YcMGHnrooe3eBAYwffp0Lr30UpYuXcqoUaOqqlNPTvxm9r5QOOsvJP4JEya0jFf7qsQTTjiBfv36MWTIED74wQ/y8ssvtyqz0047tfyjOP3007fr4rkwvdjGjRvZsGEDkydPBmDGjBnMnz+/zTr15sRvZu8LhXb+p556ipEjRzJ+/HgWLFjQ0r5fjX79+rUM9+rVq6p29+KunDvTFXMjum924jez94VJkyZxzz33MHjwYHr16sXgwYPZsGEDCxYsYMKECa3KDxo0iDfeeKPD63n33XdbLhD/5Cc/qdjFc8Guu+7K7rvvzsMPPwzALbfc0nL23yjuq8fM6qK7O8odNWoU69ata3nTVmHapk2bGDJkSKvyxxxzDN/5zncYM2YM3/zmN6tez4ABA1i+fDmHH344u+66a8uF4bbcdNNNnH322bz55psccMAB3HDDDVWvrx7cLbOZ1cSO3i1ztQYOHMimTZu6ZV3ultnMzGrCid/MrAO662y/npz4zaxmGtV0/H5Uz33pxG9mNdG/f3/Wr1/v5F8DEcH69evp379/XZbvu3rMrCaGDx9Oc3NzSxcG1jX9+/dn+PDhdVm2E7+Z1USfPn3Yf//9Gx2GVcFNPWZmOePEb2aWM+0mfkn7SHpQ0kpJyyX9TZkyR0vaKGlx+lxUn3DNzKyrqmnj3wrMiohFkgYBT0p6ICJWlJR7OCJOrH2IZmZWS+2e8UfEmohYlIbfAFYCe9c7MDMzq48OtfFLGgEcCjxeZvYESUsk3Sfp4xXqz5TUJKnJt3yZmTVG1Ylf0kDgZ8B5EfF6yexFwH4RcQjwL8Dd5ZYREddExNiIGDt06NDOxmxmZl1QVeKX1Ics6d8aEXeVzo+I1yNiUxq+F+gjqXU/qGZm1nDV3NUj4DpgZURcXqHMh1I5JI1Ly11fy0DNzKw2qrmrZxJwBvCUpMVp2oXAvgARcTUwFfiypK3AH4BTwh12mJn1SO0m/oh4BGjjzScQET8AflCroMzMrH785K6ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5Uy7iV/SPpIelLRS0nJJf1OmjCRdJWmVpKWSDqtPuGZm1lW9qyizFZgVEYskDQKelPRARKwoKnM8cHD6fAL4YfppZmY9TLtn/BGxJiIWpeE3gJXA3iXFTgJujsxjwG6S9qp5tGZm1mUdauOXNAI4FHi8ZNbewEtF4820/ueApJmSmiQ1rV27tmORWmNJ5T893Y4at9VMIw+Bnnr4VZ34JQ0EfgacFxGvl84uUyVaTYi4JiLGRsTYoUOHdixSMzOriaoSv6Q+ZEn/1oi4q0yRZmCfovHhwOquh2dmZrVWzV09Aq4DVkbE5RWKzQWmp7t7xgMbI2JNDeM0M7MaqeaunknAGcBTkhanaRcC+wJExNXAvcCngVXAm8AXax+qmZnVQruJPyIeoXwbfnGZAL5Sq6DMzKx+/OSumVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOtJv4JV0v6RVJyyrMP1rSRkmL0+ei2odpZma10ruKMjcCPwBubqPMwxFxYk0iMjOzumr3jD8i5gOvdkMsZmbWDWrVxj9B0hJJ90n6eKVCkmZKapLUtHbt2hqt2szMOqIWiX8RsF9EHAL8C3B3pYIRcU1EjI2IsUOHDq3Bqs3MrKO6nPgj4vWI2JSG7wX6SBrS5cjMzKwuupz4JX1IktLwuLTM9V1drpmZ1Ue7d/VIug04GhgiqRn4O6APQERcDUwFvixpK/AH4JSIiLpFbGZmXdJu4o+IU9uZ/wOy2z3NzGwH4Cd3zcxyxonfzCxnnPjNzHLGid/MLGec+M3McsaJ38wsZ5z4zcxyxonfzCxnnPjNzHLGid/MLGec+M3McsaJ38wsZ5z4zcxyxonfzCxnnPjNzHLGid/MLGec+M3McsaJ38wsZ5z4zcxyxonfzCxnnPjNzHLGid/MLGec+M3McsaJ38wsZ5z4zcxyxonfzCxn2k38kq6X9IqkZRXmS9JVklZJWirpsNqHaWZmtVLNGf+NwHFtzD8eODh9ZgI/7HpYZmZWL+0m/oiYD7zaRpGTgJsj8xiwm6S9ahWgmZnVVu8aLGNv4KWi8eY0bU1pQUkzyb4VsO+++9Zg1Z0klZ8e8f5ddyO3uSsqxQ2I8rHXbJMqrLvu6220OhwrXV3kjnr49lS1uLhb7ldS9tcREddExNiIGDt06NAarNrMzDqqFom/GdinaHw4sLoGyzUzszqoReKfC0xPd/eMBzZGRKtmHjMz6xnabeOXdBtwNDBEUjPwd0AfgIi4GrgX+DSwCngT+GK9gjUzs65rN/FHxKntzA/gKzWLyMzM6spP7pqZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5YwTv5lZzjjxm5nljBO/mVnOOPGbmeWME7+ZWc448ZuZ5UxViV/ScZKekbRK0jfKzD9T0lpJi9PnrNqHamZmtdC7vQKSegH/CvwJ0Aw8IWluRKwoKTonIs6pQ4xmZlZD1ZzxjwNWRcRzEfE2cDtwUn3DMjOzeqkm8e8NvFQ03pymlfozSUsl3Slpn3ILkjRTUpOkprVr13YiXDMz66pqEr/KTIuS8V8CIyJiNPCfwE3lFhQR10TE2IgYO3To0I5FamZmNVFN4m8Gis/ghwOriwtExPqIeCuNXgscXpvwzMys1qpJ/E8AB0vaX1Jf4BRgbnEBSXsVjX4WWFm7EM3MrJbavasnIrZKOge4H+gFXB8RyyVdAjRFxFzgXEmfBbYCrwJn1jFmMzPrAkWUNtd3j7Fjx0ZTU1PnKqvcZYekmu2pVL879kWj1t3V9fa0uAG1utSU2S6krsRdoW5V692R1eF33cjD7/325y7pyYgY2/kl+MldM7PcceI3M8sZJ34zs5xx4jczyxknfjOznHHiNzPLGSd+M7OcceI3M8sZJ34zs5xx4jczyxknfjOznHHiNzPLGSd+M7OcceI3M8sZJ34zs5xx4jczyxknfjOznHHiNzPLGSd+M7OcceI3M8sZJ34zs5xx4jczyxknfjOznHHiNzPLGSd+M7OcceI3M8uZqhK/pOMkPSNplaRvlJnfT9KcNP9xSSNqHaiZmdVGu4lfUi/gX4HjgY8Bp0r6WEmxLwGvRcRBwBXApbUO1MzMaqOaM/5xwKqIeC4i3gZuB04qKXMScFMavhOYIkm1C9PMzGqldxVl9gZeKhpvBj5RqUxEbJW0EdgDWFdcSNJMYGYa3STpmc4EXcaQlnV15f9N9/2vei/e7l/39qpfb+uYO1a/Dsqvuyik8jGXFKrDejurcrw9QfkN7FLMXd1nnajfEm8jD90Orrt0H+/X1fVXk/jLhRidKENEXANcU8U6O0RSU0SMrfVy62VHixccc3fY0eKFHS/mHS1eqE/M1TT1NAP7FI0PB1ZXKiOpN7Ar8GotAjQzs9qqJvE/ARwsaX9JfYFTgLklZeYCM9LwVOC/I6LVGb+ZmTVeu009qc3+HOB+oBdwfUQsl3QJ0BQRc4HrgFskrSI70z+lnkGXUfPmozrb0eIFx9wddrR4YceLeUeLF+rRPO4TczOzfPGTu2ZmOePEb2aWMz0u8Uv6qqTlkpZJuk1Sf0k3Snpe0uL0GVOm3hhJC1LdpZKmFc1rt34jYk51txWVmVs0ff/U/cWzqTuMvo2OV9IxRfMXS9oi6XNpXiP2sST9o6TfSFop6dwKdWek/fispBlF0w+X9FTqauSqWj902NmYG3Usd3Efd/tx3JWYe+Cx/HDR+lZLurtC3docyxHRYz5kD4I9D+ycxu8AzgRuBKa2U/fDwMFpeBiwBtgtjbdbvxExp/KbKky/AzglDV8NfLknxFu0nMFkF/I/0MB9/EXgZmCnNP2DFeJ8Lv3cPQ3vnuYtBCaQPYdyH3B8D4m524/lrsTbiOO4FjH3pGO5pMzPgOn1PJZ73Bk/2Z1GOyt7HuADtH5moKyI+E1EPJuGVwOvAEPrFuX2OhVzJem/9bFk3V9A1h3G57oU4fZqEe9U4L6IeLOGcbWlXMxfBi6JiHcBIuKVMvU+BTwQEa9GxGvAA8BxkvYCdomIBZH95dxMbfdxp2Nu4LHc2X1cVjccx1CbmHvCsQyApEFk+6zcGX/NjuUelfgj4nfAZcCLZGc5GyPiV2n2P6avvVdI6tfWciSNA/oC/1s0uer63Rxzf0lNkh4rfNUk6+5iQ0RsTePNZGcKPSHeglOA20qmdfc+PhCYlvbffZIOLlO9XJcje6dPc5npPSHmFt11LNcg3m49jmsUc0FPOJYLTgb+KyJeL1O9Zsdyj0r8knYn6/Btf7KvuAMknQ58E/gj4Aiyrzl/28Yy9gJuAb5Y+I/fkfoNiHnfyB7HPg34vqQDqbILjAbFW9jHo8ie7ShoxD7uB2xJ++9a4Ppy1ctMizam10QXYy4so9uO5RrE263HcY1i7knHcsGptP4n1FK9zLROHcs9KvEDnwSej4i1EfEOcBcwMSLWROYt4AayHkNbkbQL8B/AtyLiscL0aus3Iub0VZ6IeA6YBxxK1iHTbumrIJTvJqMh8SZfAH6e6he2o9v3MdmZzc9SmZ8Do8vUrdTlSHMaLp3eE2JuxLHcpXgbcBx3OeakpxzLSNojres/KtSt2bHc0xL/i8B4SR9I7YNTgJXpv3KhzfBzwLLSisruFvg5cHNE/LRkXrv1GxTz7oWvkZKGAJOAFamd7kGytkfIusP4RaPjLdLqrKQR+5isHfTYVGYy8Jsyde8H/jTt692BPwXuj4g1wBuSxqdlTqd2+7hLMTfoWO5KvI04jrsUc5GeciwDfB64JyK2VKhbu2M56nDluisf4GLgabKdfQvZ17b/Bp5K034MDExlxwI/SsOnA+8Ai4s+Y9K8svV7QMwTU5kl6eeXipZ5ANmV+lXAT4F+jY43jY8Afke6Y6JoeiP28W5kZ0dPAQuAQyrE/BdpP64iazahqNwysvbzH5CeZG90zI06lrsQb0OO4xocFz3mWE7T5wHHlZSty7HsLhvMzHKmpzX1mJlZnTnxm5nljBO/mVnOOPGbmeWME7+ZWc448VuXSJon6VMl086T9G/t1NuUfg6TdGeFMvMktfmS6bSuDxSN3ytpt+q3oH4kfVdZL4zflXS2pOlp+o2SprZXv2g5syWdX79ILW/affWiWTtuI+vrpPiR91OAr1dTObInPqtOgmWcR3av9ZtpeZ/uwrI6RFLveK8fmnL+Ehga2ROgZj2Gz/itq+4ETix6cnMEWR8kj0gaKOm/JC1S1lf4SaWVJY2QtCwN7yzp9tQ51hxg56JyP1TW6dZySRenaeemdT0o6cE07YX09CiSvqasz/Nlks4rWt9KSdemZf1K0s4lYRXOyq9W1k/6bySdmKafKemnkn4J/EqZ76Z1PKXUd76yPukHAI9LmlbprF1ZP+oPSXpS0v2Fp0YrUdZX/2NpH/08PcGJpHMlrUjTb0/TJuu9Pt5/raznR7Oe9+SuPzveh+wpyZPS8DeA76bh3mTdxQIMIXvasPDQ4Kb0cwSwLA1/Dbg+DY8GtgJj0/jg9LMX2ROOo9P4C8CQolheSOs6nOzpywHAQGA5Wf8xI9JyC0/C3gGcXmabbgT+H9nJ0cFk/aH0J+vvvbkonj8j6x63F7An2SP5exVvYxqeDZxftOypQB/gUbJvBQDTCttfEktx3aXA5DR8CfD9NLya954ALfTd/0tgUhoeCPRu9LHiT8/4+IzfaqHQ3APbd3Er4BAg3r4AAAJpSURBVJ8kLQX+k6yr2D3bWM5RZM02RMRSsiRX8AVJi4BfAx8HPtZOTEeSdb61OSI2kXWG9cdp3vMRsTgNP0n2z6CcOyLi3cj6xn+OrMdGSH2iF63ntojYFhEvAw+R9epYjY8AI4EHJC0GvsX2nW1tR9KuZEn9oTTpJrJ9Btm+ulVZT4+F5qf/AS5P34x2i7abpSxHnPitFu4Gpkg6jOzNQovS9D8ne4HI4RExBniZ7Ky5La36EJG0P3A+MCUiRpN9w2hvOW29eq64zX0bla91lcZSGN9c5XraI2B5RIxJn1ER8aedXNYJwL+SfdN5Ml1/+A5wFlmT2WOS/qitBVh+OPFbl6Uz6nlk/Z4X93S4K/BKRLwj6Rhgv3YWNZ/snwWSRvJed7q7kCXbjZL2BI4vqvMGUK7tej7wudQL4gCyF1w83JHtAj4vaSdlfcsfADxTYT3TJPWSNJTsDHxhlct/BhgqaQKApD6SPl6pcERsBF6TVPjmcgbwkKSdgH0i4kHgArJOygZKOjAinoqIS4Em3vvGYjnnu3qsVm4ja045pWjarcAvJTWR9TD5dDvL+CFwQ2oaWkxKoBGxRNKvydrpnyNrwii4BrhP0pqIOKYwMSIWSbqR95LwjyLi1+nic7WeIWu62RM4OyK2qPU7rH9O9q7TJWTfCC6IiN9Xs/CIeDvd1nlVasbpDXw/bWclM4Cr0y2sz5G9X7YX8OO0DAFXRMQGSX+f/uFuA1aQvYvVzL1zmpWT/mncExFlnzEw25G5qcfMLGd8xm9mljM+4zczyxknfjOznHHiNzPLGSd+M7OcceI3M8uZ/w8UDd68FMWduAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "plt.title(\"Histogram of validation profile loss\")\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
