{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the (one-indexed) number of the epch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_validation_profile_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best profile loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_prof_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.2f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"train_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.2f\" % i for i in np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.4f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 4\n",
      "\tBest epoch in run: 10\n",
      "\tAssociated value: 179.99440738677978\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 10: 180.97\n",
      "\tRun 2, epoch 8: 180.73\n",
      "\tRun 3, epoch 7: 180.46\n",
      "\tRun 4, epoch 10: 179.99\n",
      "\tRun 5, epoch 10: 180.05\n",
      "\tRun 6, epoch 9: 180.83\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t200.36 194.65 193.33 192.59 192.15 191.83 191.66 191.52 191.40 191.25\n",
      "\t184.69 182.63 181.86 181.45 181.32 181.37 181.15 181.13 181.09 180.97\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "2\n",
      "\t199.41 194.43 193.20 192.45 192.04 191.73 191.50 191.35 191.22 191.14\n",
      "\t184.60 182.48 181.70 181.30 180.91 180.93 180.88 180.73 180.80 180.90\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "3\n",
      "\t198.70 193.95 192.49 191.83 191.42 191.19 191.02 190.83 190.75 190.59\n",
      "\t183.93 181.95 181.27 180.81 180.70 180.67 180.46 180.62 180.62 180.83\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "4\n",
      "\t199.55 194.49 193.11 192.40 192.02 191.76 191.54 191.44 191.29 191.18\n",
      "\t183.14 181.38 180.75 180.37 180.14 180.17 180.15 180.13 180.12 179.99\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "5\n",
      "\t199.47 194.07 192.71 192.03 191.62 191.33 191.18 190.99 190.88 190.73\n",
      "\t183.18 181.80 180.95 180.63 180.28 180.08 180.09 180.15 180.07 180.05\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n",
      "6\n",
      "\t199.59 194.77 193.26 192.50 192.12 191.79 191.54 191.40 191.29\n",
      "\t185.22 182.60 181.85 181.65 181.38 181.22 180.95 180.89 180.83\n",
      "\t0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_profile_and_prior_losses(\"HepG2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best profile loss overall:\n",
      "\tBest run: 3\n",
      "\tBest epoch in run: 7\n",
      "\tAssociated value: 181.0568891398112\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 9: 181.77\n",
      "\tRun 2, epoch 9: 181.86\n",
      "\tRun 3, epoch 7: 181.06\n",
      "\tRun 4, epoch 10: 181.13\n",
      "\tRun 5, epoch 7: 181.82\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t199.37 194.95 194.05 193.47 193.14 192.86 192.67 192.56 192.45 192.38\n",
      "\t184.64 183.46 182.80 182.50 182.20 182.05 182.10 182.17 181.77 182.11\n",
      "\t0.1124 0.0946 0.0776 0.0777 0.0766 0.0789 0.0812 0.0818 0.0797 0.0826\n",
      "2\n",
      "\t199.63 195.21 194.34 193.89 193.69 193.50 193.34 193.15 193.09 192.99\n",
      "\t184.50 183.09 183.09 182.21 182.38 182.05 182.14 182.13 181.86 181.90\n",
      "\t0.1013 0.0823 0.0744 0.0758 0.0762 0.0742 0.0755 0.0810 0.0776 0.0781\n",
      "3\n",
      "\t199.85 195.26 194.04 193.39 193.00 192.85 192.69 192.56 192.47 192.40\n",
      "\t184.08 182.69 182.14 181.53 181.53 181.49 181.06 181.22 181.28 181.22\n",
      "\t0.0980 0.0929 0.0930 0.0843 0.0784 0.0846 0.0813 0.0852 0.0845 0.0822\n",
      "4\n",
      "\t199.82 195.20 194.24 193.80 193.51 193.30 193.10 192.91 192.76 192.65\n",
      "\t183.96 182.62 182.11 181.85 181.70 181.50 181.43 181.22 181.40 181.13\n",
      "\t0.1094 0.0794 0.0788 0.0754 0.0791 0.0749 0.0783 0.0785 0.0795 0.0749\n",
      "5\n",
      "\t199.23 195.12 193.82 193.27 192.83 192.68 192.59\n",
      "\t184.73 183.14 182.46 181.99 182.13 181.96 181.82\n",
      "\t0.0954 0.0972 0.0842 0.0771 0.0815 0.0758 0.0910\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_profile_and_prior_losses(\"HepG2_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_and_prior_losses(condition):\n",
    "    models_path = \"/users/amtseng/att_priors/models/trained_models/binary_models/%s/\" % condition\n",
    "    \n",
    "    print(\"Best validation loss overall:\")\n",
    "    best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "        models_path,\n",
    "        \"val_corr_losses\",\n",
    "        lambda values: np.mean(values),\n",
    "        lambda x, y: x < y\n",
    "    )\n",
    "    print(\"\\tBest run: %s\" % best_run)\n",
    "    print(\"\\tBest epoch in run: %d\" % best_epoch)\n",
    "    print(\"\\tAssociated value: %s\" % best_val)\n",
    "    \n",
    "    print(\"Best epoch in each run:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(\"\\tRun %s, epoch %d: %6.3f\" % (key[0], key[1], all_vals[key]))\n",
    "        \n",
    "    print(\"All validation profile and prior losses:\")\n",
    "    for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "        print(key[0])\n",
    "        metrics = import_metrics_json(models_path, key[0])\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"train_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_corr_losses\"][\"values\"], axis=1)]))\n",
    "        print(\"\\t\" + \" \".join([\"%6.3f\" % i for i in np.mean(metrics[\"val_att_losses\"][\"values\"], axis=1)]))\n",
    "    return all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 7\n",
      "\tBest epoch in run: 1\n",
      "\tAssociated value: 0.23903661345442137\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 1:  0.260\n",
      "\tRun 2, epoch 1:  0.261\n",
      "\tRun 3, epoch 1:  0.242\n",
      "\tRun 4, epoch 1:  0.252\n",
      "\tRun 5, epoch 1:  0.243\n",
      "\tRun 6, epoch 1:  0.244\n",
      "\tRun 7, epoch 1:  0.239\n",
      "\tRun 8, epoch 1:  0.241\n",
      "\tRun 9, epoch 1:  0.244\n",
      "\tRun 10, epoch 1:  0.239\n",
      "\tRun 11, epoch 1:  0.254\n",
      "\tRun 12, epoch 1:  0.247\n",
      "\tRun 13, epoch 1:  0.242\n",
      "\tRun 14, epoch 1:  0.240\n",
      "\tRun 15, epoch 1:  0.243\n",
      "\tRun 16, epoch 1:  0.245\n",
      "\tRun 17, epoch 1:  0.254\n",
      "\tRun 18, epoch 1:  0.241\n",
      "\tRun 19, epoch 1:  0.242\n",
      "\tRun 20, epoch 1:  0.242\n",
      "\tRun 21, epoch 1:  0.255\n",
      "\tRun 22, epoch 1:  0.249\n",
      "\tRun 23, epoch 1:  0.242\n",
      "\tRun 24, epoch 1:  0.245\n",
      "\tRun 25, epoch 1:  0.247\n",
      "\tRun 26, epoch 1:  0.257\n",
      "\tRun 27, epoch 1:  0.255\n",
      "\tRun 28, epoch 1:  0.253\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.249  0.194  0.166  0.146  0.129  0.115  0.105  0.098  0.092  0.088\n",
      "\t 0.260  0.274  0.300  0.341  0.356  0.375  0.403  0.432  0.435  0.464\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "2\n",
      "\t 0.250  0.195  0.167  0.148  0.134  0.123  0.115  0.107  0.099  0.091\n",
      "\t 0.261  0.269  0.293  0.332  0.348  0.378  0.389  0.436  0.434  0.458\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "3\n",
      "\t 0.239  0.180  0.151  0.131  0.116  0.105  0.097  0.090  0.085  0.080\n",
      "\t 0.242  0.259  0.289  0.326  0.340  0.380  0.401  0.442  0.459  0.488\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "4\n",
      "\t 0.249  0.194  0.165  0.147  0.132  0.121  0.113  0.106  0.101  0.096\n",
      "\t 0.252  0.275  0.296  0.324  0.358  0.402  0.426  0.432  0.450  0.483\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "5\n",
      "\t 0.240  0.180  0.151  0.131  0.116  0.106  0.097  0.090  0.085  0.081\n",
      "\t 0.243  0.259  0.283  0.316  0.352  0.371  0.402  0.443  0.450  0.474\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "6\n",
      "\t 0.239  0.179  0.149  0.129  0.114  0.103  0.095  0.088  0.083  0.078\n",
      "\t 0.244  0.259  0.281  0.307  0.343  0.371  0.406  0.421  0.436  0.461\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "7\n",
      "\t 0.243  0.183  0.153  0.132  0.117  0.105  0.097  0.091  0.085  0.080\n",
      "\t 0.239  0.255  0.291  0.324  0.353  0.361  0.384  0.429  0.440  0.467\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "8\n",
      "\t 0.238  0.178  0.149  0.129  0.114  0.103  0.094  0.088  0.083  0.078\n",
      "\t 0.241  0.259  0.285  0.314  0.343  0.355  0.378  0.404  0.424  0.455\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "9\n",
      "\t 0.241  0.181  0.151  0.130  0.115  0.104  0.095  0.089  0.084  0.079\n",
      "\t 0.244  0.256  0.285  0.317  0.339  0.367  0.381  0.424  0.453  0.450\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "10\n",
      "\t 0.238  0.179  0.150  0.129  0.114  0.104  0.095  0.089  0.083  0.079\n",
      "\t 0.239  0.251  0.286  0.317  0.341  0.365  0.393  0.412  0.426  0.449\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "11\n",
      "\t 0.245  0.193  0.165  0.145  0.132  0.118  0.107  0.099  0.092  0.087\n",
      "\t 0.254  0.267  0.297  0.319  0.357  0.362  0.393  0.439  0.445  0.457\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "12\n",
      "\t 0.239  0.181  0.152  0.131  0.117  0.106  0.097  0.090  0.084  0.081\n",
      "\t 0.247  0.260  0.280  0.324  0.343  0.366  0.397  0.412  0.427  0.464\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "13\n",
      "\t 0.236  0.179  0.149  0.128  0.113  0.103  0.095  0.088  0.083  0.078\n",
      "\t 0.242  0.257  0.283  0.310  0.342  0.362  0.395  0.407  0.440  0.471\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "14\n",
      "\t 0.238  0.180  0.149  0.129  0.115  0.103  0.095  0.089  0.083  0.079\n",
      "\t 0.240  0.254  0.295  0.312  0.342  0.379  0.405  0.420  0.440  0.480\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "15\n",
      "\t 0.236  0.180  0.152  0.131  0.116  0.105  0.096  0.090  0.084  0.080\n",
      "\t 0.243  0.254  0.281  0.303  0.348  0.369  0.411  0.424  0.438  0.450\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "16\n",
      "\t 0.238  0.180  0.152  0.133  0.118  0.107  0.099  0.092  0.087  0.082\n",
      "\t 0.245  0.258  0.277  0.311  0.341  0.376  0.394  0.418  0.437\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "17\n",
      "\t 0.248  0.188  0.156  0.135  0.120  0.108  0.099  0.093  0.087  0.082\n",
      "\t 0.254  0.261  0.289  0.320  0.350  0.372  0.395  0.427  0.448  0.454\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "18\n",
      "\t 0.236  0.177  0.147  0.127  0.113  0.102  0.094  0.088  0.082  0.078\n",
      "\t 0.241  0.261  0.283  0.323  0.341  0.373  0.413  0.412  0.439  0.453\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "19\n",
      "\t 0.240  0.180  0.151  0.130  0.115  0.104  0.096  0.089  0.084  0.079\n",
      "\t 0.242  0.254  0.288  0.311  0.339  0.361  0.387  0.425  0.427  0.455\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "20\n",
      "\t 0.239  0.180  0.151  0.132  0.117  0.106  0.097  0.090  0.085  0.080\n",
      "\t 0.242  0.257  0.287  0.313  0.340  0.366  0.385  0.426  0.433  0.460\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "21\n",
      "\t 0.248  0.193  0.157  0.136  0.121  0.109  0.102  0.094  0.088  0.083\n",
      "\t 0.255  0.267  0.295  0.318  0.353  0.389  0.402  0.441  0.447  0.482\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "22\n",
      "\t 0.237  0.178  0.149  0.129  0.115  0.104  0.096  0.089  0.083  0.079\n",
      "\t 0.249  0.259  0.289  0.316  0.345  0.366  0.394  0.415  0.446  0.472\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "23\n",
      "\t 0.242  0.180  0.150  0.130  0.115  0.104  0.096  0.089  0.084  0.079\n",
      "\t 0.242  0.255  0.280  0.315  0.336  0.359  0.401  0.416  0.460  0.473\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "24\n",
      "\t 0.243  0.183  0.154  0.132  0.117  0.106  0.097  0.090  0.085  0.080\n",
      "\t 0.245  0.265  0.283  0.332  0.348  0.389  0.407  0.440  0.449  0.465\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "25\n",
      "\t 0.239  0.181  0.152  0.131  0.117  0.106  0.097\n",
      "\t 0.247  0.251  0.290  0.313  0.349  0.373  0.393\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "26\n",
      "\t 0.248  0.194  0.159  0.138  0.124  0.113  0.104  0.097  0.091  0.086\n",
      "\t 0.257  0.271  0.297  0.319  0.342  0.370  0.396  0.412  0.429  0.438\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "27\n",
      "\t 0.246  0.186  0.155  0.135  0.121  0.110  0.102  0.094  0.089  0.084\n",
      "\t 0.255  0.261  0.290  0.317  0.347  0.373  0.406  0.418  0.438  0.459\n",
      "\t 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n",
      "28\n",
      "\t 0.247  0.194  0.165\n",
      "\t 0.253  0.270  0.301\n",
      "\t 0.000  0.000  0.000\n"
     ]
    }
   ],
   "source": [
    "noprior_vals = print_validation_and_prior_losses(\"HepG2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss overall:\n",
      "\tBest run: 5\n",
      "\tBest epoch in run: 5\n",
      "\tAssociated value: 0.2355518709279989\n",
      "Best epoch in each run:\n",
      "\tRun 1, epoch 5:  0.248\n",
      "\tRun 2, epoch 8:  0.251\n",
      "\tRun 3, epoch 9:  0.251\n",
      "\tRun 4, epoch 8:  0.248\n",
      "\tRun 5, epoch 5:  0.236\n",
      "\tRun 6, epoch 2:  0.252\n",
      "\tRun 7, epoch 7:  0.267\n",
      "\tRun 8, epoch 5:  0.251\n",
      "\tRun 9, epoch 4:  0.242\n",
      "\tRun 10, epoch 3:  0.310\n",
      "All validation profile and prior losses:\n",
      "1\n",
      "\t 0.329  0.288  0.272  0.247  0.219  0.201  0.188  0.175  0.175  0.178\n",
      "\t 0.322  0.293  0.302  0.262  0.248  0.254  0.261  0.268  0.268  0.272\n",
      "\t 0.079  0.068  0.081  0.041  0.035  0.035  0.032  0.030  0.074  0.044\n",
      "2\n",
      "\t 0.336  0.331  0.300  0.274  0.247  0.226  0.210  0.198  0.190  0.182\n",
      "\t 0.317  0.391  0.305  0.284  0.281  0.256  0.252  0.251  0.257  0.271\n",
      "\t 0.066  0.082  0.061  0.052  0.051  0.046  0.044  0.045  0.045  0.076\n",
      "3\n",
      "\t 0.330  0.289  0.275  0.269  0.268  0.249  0.241  0.224  0.211  0.198\n",
      "\t 0.328  0.289  0.275  0.306  0.278  0.277  0.258  0.256  0.251  0.251\n",
      "\t 0.067  0.055  0.062  0.090  0.066  0.067  0.053  0.045  0.040  0.053\n",
      "4\n",
      "\t 0.334  0.277  0.255  0.233  0.246  0.239  0.218  0.207  0.192  0.178\n",
      "\t 0.303  0.317  0.267  0.282  0.273  0.252  0.263  0.248  0.259  0.254\n",
      "\t 0.068  0.107  0.046  0.074  0.081  0.059  0.045  0.041  0.032  0.046\n",
      "5\n",
      "\t 0.336  0.279  0.263  0.237  0.208  0.190  0.173  0.158  0.145  0.135\n",
      "\t 0.308  0.353  0.318  0.246  0.236  0.242  0.255  0.278  0.294  0.319\n",
      "\t 0.065  0.095  0.074  0.035  0.034  0.032  0.029  0.029  0.029  0.033\n",
      "6\n",
      "\t 0.305  0.239  0.211  0.198  0.179  0.160  0.204  0.246  0.242  0.243\n",
      "\t 0.289  0.252  0.252  0.258  0.268  0.292  0.292  0.280  0.311  0.272\n",
      "\t 0.039  0.038  0.038  0.038  0.039  0.033  0.096  0.101  0.102  0.109\n",
      "7\n",
      "\t 0.351  0.311  0.299  0.291  0.281  0.261  0.251  0.243  0.240  0.241\n",
      "\t 0.331  0.335  0.321  0.301  0.297  0.268  0.267  0.274  0.284  0.296\n",
      "\t 0.074  0.086  0.088  0.069  0.071  0.055  0.059  0.068  0.113  0.067\n",
      "8\n",
      "\t 0.318  0.249  0.225  0.211  0.197  0.188  0.176  0.165  0.155  0.146\n",
      "\t 0.296  0.279  0.324  0.252  0.251  0.259  0.264  0.268  0.285  0.289\n",
      "\t 0.054  0.043  0.068  0.038  0.036  0.036  0.037  0.036  0.035  0.034\n",
      "9\n",
      "\t 0.335  0.271  0.223  0.198  0.179  0.168  0.161  0.147  0.138  0.136\n",
      "\t 0.321  0.265  0.244  0.242  0.254  0.253  0.267  0.287  0.283  0.290\n",
      "\t 0.089  0.036  0.029  0.028  0.029  0.043  0.031  0.033  0.032  0.062\n",
      "10\n",
      "\t 0.329  0.319  0.292\n",
      "\t 0.340  0.328  0.310\n",
      "\t 0.109  0.117  0.084\n"
     ]
    }
   ],
   "source": [
    "prior_vals = print_validation_and_prior_losses(\"HepG2_prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f600128b910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_num = 20\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    [np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))],\n",
    "    bin_num, histtype=\"bar\",\n",
    "    label=[\"No prior\", \"With prior\"], color=[\"red\", \"blue\"])\n",
    "title = \"Histogram of validation loss\"\n",
    "title += \"\\nTraining on only 1% of positive bins\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Validation profile loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.2640587687389155\n",
      "5.566017270666034e-05\n"
     ]
    }
   ],
   "source": [
    "np_vals, p_vals = np.array(list(noprior_vals.values())), np.array(list(prior_vals.values()))\n",
    "t, p = scipy.stats.ttest_ind(np_vals, p_vals)\n",
    "print(t)\n",
    "print(p / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
