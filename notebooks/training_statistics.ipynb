{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value to use for\n",
    "    comparison. The best metric value is determined by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a dict of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = reduce_func(metrics[run_num][metric_name][\"values\"])\n",
    "            all_vals[run_num] = val\n",
    "            if best_val is None or compare_func(val, best_val):\n",
    "                best_val, best_run = val, run_num\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_at_best_epoch(models_path, metric_name, reduce_func, compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    for the given `metric_name`. For each run, the function `reduce_func` must take\n",
    "    the array of all values for that metric and return a (scalar) value FOR EACH\n",
    "    SUBARRAY/VALUE in the value array to use for comparison. The best metric value\n",
    "    is determined by `metric_compare_func`, which must take in two arguments, and\n",
    "    return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the (one-indexed) number of the epch, the value\n",
    "    associated with that run and epoch, and a dict of all the values used for\n",
    "    comparison (mapping pair of run number and epoch number to value).\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_epoch, best_val, all_vals = None, None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            # Find the best epoch within that run\n",
    "            best_epoch_in_run, best_val_in_run = None, None\n",
    "            for i, subarr in enumerate(metrics[run_num][metric_name][\"values\"]):\n",
    "                val = reduce_func(subarr)\n",
    "                if best_val_in_run is None or compare_func(val, best_val_in_run):\n",
    "                    best_epoch_in_run, best_val_in_run = i + 1, val\n",
    "            all_vals[(run_num, best_epoch_in_run)] = best_val_in_run\n",
    "            \n",
    "            # If the best value in the best epoch of the run is best so far, update\n",
    "            if best_val is None or compare_func(best_val_in_run, best_val):\n",
    "                best_run, best_epoch, best_val = run_num, best_epoch_in_run, best_val_in_run\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to compute values for run %s\" % run_num)\n",
    "            continue\n",
    "    return best_run, best_epoch, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run: 2\n",
      "Best epoch in run: 5\n",
      "Associated value: 88.49771720484684\n",
      "('1', 4) 88.78697790710712\n",
      "('2', 5) 88.49771720484684\n",
      "('3', 5) 89.79084527069976\n",
      "('4', 3) 89.58271916570776\n",
      "('5', 9) 89.26148537739263\n",
      "('6', 4) 88.99320590944218\n",
      "('7', 10) 89.59271423798755\n",
      "('8', 10) 91.59313578462395\n",
      "('9', 10) 91.41927015116286\n",
      "('10', 9) 91.56127111023146\n",
      "('11', 4) 89.8861976590791\n",
      "('12', 4) 89.50611362539136\n",
      "('13', 4) 89.67429481146162\n",
      "('14', 5) 89.1128744049564\n",
      "('15', 10) 89.60704803466797\n",
      "('16', 7) 88.88628069832602\n",
      "('17', 9) 90.23008309271103\n",
      "('18', 5) 89.66416141989407\n",
      "('19', 5) 88.89251274043328\n",
      "('20', 9) 89.35180024250494\n",
      "('21', 6) 90.39302847211454\n",
      "('22', 6) 88.93682920740703\n",
      "('24', 10) 88.89174473643814\n",
      "('25', 10) 89.76119854165967\n",
      "('26', 10) 89.36809022436837\n"
     ]
    }
   ],
   "source": [
    "models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/SPI1/\"\n",
    "best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "    models_path,\n",
    "    \"val_prof_corr_losses\",\n",
    "    lambda values: np.mean(values),\n",
    "    lambda x, y: x < y\n",
    ")\n",
    "print(\"Best run: %s\" % best_run)\n",
    "print(\"Best epoch in run: %d\" % best_epoch)\n",
    "print(\"Associated value: %s\" % best_val)\n",
    "for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "    print(key, all_vals[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run: 12\n",
      "Best epoch in run: 10\n",
      "Associated value: 89.15911796602603\n",
      "('1', 10) 89.16402611491759\n",
      "('2', 10) 96.63805765137637\n",
      "('3', 6) 98.06763605759043\n",
      "('4', 10) 91.85942960092439\n",
      "('5', 2) 127.20769795430064\n",
      "('6', 6) 127.9285302126318\n",
      "('7', 10) 97.56137843802972\n",
      "('8', 10) 89.45429661809182\n",
      "('9', 1) 128.03340769275047\n",
      "('10', 10) 90.12213013482017\n",
      "('11', 3) 126.74649020808596\n",
      "('12', 10) 89.15911796602603\n",
      "('13', 1) 127.17924965798022\n",
      "('14', 6) 94.32266445139425\n",
      "('15', 1) 127.49588408915749\n"
     ]
    }
   ],
   "source": [
    "models_path = \"/users/amtseng/att_priors/models/trained_models/profile_models/SPI1_prior_hypertune/\"\n",
    "best_run, best_epoch, best_val, all_vals = get_best_metric_at_best_epoch(\n",
    "    models_path,\n",
    "    \"val_prof_corr_losses\",\n",
    "    lambda values: np.mean(values),\n",
    "    lambda x, y: x < y\n",
    ")\n",
    "print(\"Best run: %s\" % best_run)\n",
    "print(\"Best epoch in run: %d\" % best_epoch)\n",
    "print(\"Associated value: %s\" % best_val)\n",
    "for key in sorted(all_vals.keys(), key=lambda p: int(p[0])):\n",
    "    print(key, all_vals[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127.09752059 127.0005582  127.06322442 127.02281215 107.26342508\n",
      " 101.189943    99.29842195  98.51022036  97.56920601  96.63805765]\n",
      "[0.10230981 0.04003526 0.02961809 0.03232238 0.17663985 0.16446357\n",
      " 0.14681025 0.17855237 0.17623507 0.15631514]\n"
     ]
    }
   ],
   "source": [
    "metrics = import_metrics_json(models_path, \"2\")\n",
    "print(np.mean(metrics[\"val_prof_corr_losses\"][\"values\"], axis=1))\n",
    "print(np.mean(metrics[\"val_pos_att_losses\"][\"values\"], axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
