{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src/\"))\n",
    "import extract.data_loading as data_loading\n",
    "import extract.compute_predictions as compute_predictions\n",
    "import extract.compute_shap as compute_shap\n",
    "import extract.compute_ism as compute_ism\n",
    "import model.util as model_util\n",
    "import model.profile_models as profile_models\n",
    "import model.binary_models as binary_models\n",
    "import plot.viz_sequence as viz_sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import json\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()  # It is necessary to call this before the tqdm.notebook submodule is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for the model and data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared paths/constants\n",
    "reference_fasta = \"/users/amtseng/genomes/hg38.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/hg38.canon.chrom.sizes\"\n",
    "data_base_path = \"/users/amtseng/att_priors/data/processed/\"\n",
    "model_base_path = \"/users/amtseng/att_priors/models/trained_models/%s/\" % model_type\n",
    "chrom_set = [\"chr1\"]\n",
    "input_length = 1346 if model_type == \"profile\" else 1000\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPI1\n",
    "condition_name = \"SPI1\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_TFChIP/%s/config/SPI1/SPI1_training_paths.json\" % model_type)\n",
    "num_tasks = 4\n",
    "num_strands = 2\n",
    "controls = \"matched\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithMatchedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "# noprior_model_path = os.path.join(model_base_path, \"SPI1/27/model_ckpt_epoch_17.pt\")\n",
    "# prior_model_path = os.path.join(model_base_path, \"SPI1_prior/14/model_ckpt_epoch_18.pt\")\n",
    "# noprior_model_path = os.path.join(model_base_path, \"SPI1/4/model_ckpt_epoch_2.pt\")\n",
    "# prior_model_path = os.path.join(model_base_path, \"SPI1_prior/16/model_ckpt_epoch_6.pt\")\n",
    "\n",
    "# noprior_model_path = os.path.join(model_base_path, \"SPI1_l2/1/model_ckpt_epoch_19.pt\")\n",
    "\n",
    "# noprior_model_path = os.path.join(model_base_path, \"SPI1_smoothprior_w2e4/12/model_ckpt_epoch_6.pt\")\n",
    "# noprior_model_path = os.path.join(model_base_path, \"SPI1_smoothprior_priorweighttune/5/model_ckpt_epoch_3.pt\")\n",
    "\n",
    "# noprior_model_path = os.path.join(model_base_path, \"SPI1_sparseprior_w0.01/15/model_ckpt_epoch_2.pt\")\n",
    "noprior_model_path = os.path.join(model_base_path, \"SPI1_sparseprior_priorweighttune/11/model_ckpt_epoch_2.pt\")\n",
    "\n",
    "prior_model_path = os.path.join(model_base_path, \"SPI1_prior/16/model_ckpt_epoch_6.pt\")\n",
    "\n",
    "peak_retention = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GATA2\n",
    "condition_name = \"GATA2\"\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_TFChIP/%s/config/GATA2/GATA2_training_paths.json\" % model_type)\n",
    "num_tasks = 3\n",
    "num_strands = 2\n",
    "controls = \"matched\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithMatchedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "# noprior_model_path = os.path.join(model_base_path, \"GATA2/6/model_ckpt_epoch_18.pt\")\n",
    "# prior_model_path = os.path.join(model_base_path, \"GATA2_prior/12/model_ckpt_epoch_19.pt\")\n",
    "noprior_model_path = os.path.join(model_base_path, \"GATA2/8/model_ckpt_epoch_1.pt\")\n",
    "prior_model_path = os.path.join(model_base_path, \"GATA2_prior/21/model_ckpt_epoch_6.pt\")\n",
    "peak_retention = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K562\n",
    "files_spec_path = os.path.join(data_base_path, \"ENCODE_DNase/%s/config/K562/K562_training_paths.json\" % model_type)\n",
    "condition_name = \"K562\"\n",
    "num_tasks = 1\n",
    "num_strands = 1\n",
    "controls = \"shared\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithSharedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "# noprior_model_path = os.path.join(model_base_path, \"K562/19/model_ckpt_epoch_19.pt\")\n",
    "# prior_model_path = os.path.join(model_base_path, \"K562_prior/16/model_ckpt_epoch_20.pt\")\n",
    "noprior_model_path = os.path.join(model_base_path, \"K562/18/model_ckpt_epoch_1.pt\")\n",
    "prior_model_path = os.path.join(model_base_path, \"K562_prior/12/model_ckpt_epoch_6.pt\")\n",
    "peak_retention = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPNet\n",
    "reference_fasta = \"/users/amtseng/genomes/mm10.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/mm10.canon.chrom.sizes\"\n",
    "files_spec_path = os.path.join(data_base_path, \"BPNet_ChIPseq/%s/config/BPNet_training_paths.json\" % model_type)\n",
    "condition_name = \"BPNet\"\n",
    "num_tasks = 3\n",
    "num_strands = 2\n",
    "controls = \"shared\"\n",
    "if model_type == \"profile\":\n",
    "    model_class = profile_models.ProfilePredictorWithSharedControls\n",
    "else:\n",
    "    model_class = binary_models.BinaryPredictor\n",
    "# noprior_model_path = os.path.join(model_base_path, \"BPNet/20/model_ckpt_epoch_18.pt\")\n",
    "# prior_model_path = os.path.join(model_base_path, \"BPNet_prior/25/model_ckpt_epoch_17.pt\")\n",
    "noprior_model_path = os.path.join(model_base_path, \"BPNet/22/model_ckpt_epoch_1.pt\")\n",
    "prior_model_path = os.path.join(model_base_path, \"BPNet_prior/27/model_ckpt_epoch_11.pt\")\n",
    "peak_retention = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(model_path):\n",
    "    model = model_util.restore_model(model_class, model_path)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model without priors\n",
    "noprior_model = restore_model(noprior_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model with priors\n",
    "prior_model = restore_model(prior_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Create an input data loader, that maps coordinates or bin indices to data needed for the model. We'll also need to be able to determine the location of the summit for each coordinate. This is a little more tricky for binary models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the set of peak BEDs; this is hacky way to do it\n",
    "profile_files_spec_path = files_spec_path.replace(\"binary\", \"profile\")\n",
    "with open(profile_files_spec_path, \"r\") as f:\n",
    "    spec = json.load(f)\n",
    "    peak_bed_paths = spec[\"peak_beds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the peak BEDs; in the meantime, add a column for the actual\n",
    "# summit position, not just the offset with respect to the peak\n",
    "peak_beds = []\n",
    "for peak_bed_path in peak_bed_paths:\n",
    "    table = pd.read_csv(\n",
    "        peak_bed_path, sep=\"\\t\", header=None,  # Infer compression\n",
    "        names=[\n",
    "            \"chrom\", \"peak_start\", \"peak_end\", \"name\", \"score\",\n",
    "            \"strand\", \"signal\", \"pval\", \"qval\", \"summit_offset\"\n",
    "        ]\n",
    "    )\n",
    "    table = table[table[\"chrom\"].isin(chrom_set)]\n",
    "    table[\"summit_pos\"] = table[\"peak_start\"] + table[\"summit_offset\"]\n",
    "    peak_beds.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"profile\":\n",
    "    input_func = data_loading.get_profile_input_func(\n",
    "        files_spec_path, input_length, profile_length, reference_fasta\n",
    "    )\n",
    "    pos_examples = data_loading.get_positive_profile_coords(\n",
    "        files_spec_path, chrom_set=chrom_set\n",
    "    )\n",
    "else:\n",
    "    input_func = data_loading.get_binary_input_func(\n",
    "        files_spec_path, input_length, reference_fasta\n",
    "    )\n",
    "    pos_examples = data_loading.get_positive_binary_bins(\n",
    "        files_spec_path, chrom_set=chrom_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample of 1000 random coordinates/bins\n",
    "num_samples = 1000\n",
    "rng = np.random.RandomState(20200318)\n",
    "sample = pos_examples[rng.choice(len(pos_examples), size=num_samples, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates for the sample\n",
    "if model_type == \"profile\":\n",
    "    # For profile models, add a random jitter to avoid center-bias\n",
    "    jitters = np.random.randint(-128, 128 + 1, size=len(sample))\n",
    "    sample[:, 1] = sample[:, 1] + jitters\n",
    "    sample[:, 2] = sample[:, 2] + jitters\n",
    "    sample_coords = sample\n",
    "else:\n",
    "    sample_coords = input_func(sample)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, sample):\n",
    "    \"\"\"\n",
    "    Given an array of N coordinates or bins, computes the input gradients\n",
    "    for the model, returning an N x I x 4 array of gradient values and an\n",
    "    N x I x 4 array of one-hot encoded sequence.\n",
    "    \"\"\"\n",
    "    if model_type == \"profile\":\n",
    "        results = compute_predictions.get_profile_model_predictions(                                              \n",
    "            model, sample, num_tasks, input_func, controls=controls,                        \n",
    "            return_losses=False, return_gradients=True, show_progress=True                                         \n",
    "        )\n",
    "    else:\n",
    "        results = compute_predictions.get_binary_model_predictions(                                              \n",
    "            model, sample, input_func,                      \n",
    "            return_losses=False, return_gradients=True, show_progress=True                                         \n",
    "        )\n",
    "    return results[\"input_grads\"], results[\"input_seqs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_scores(model, sample, batch_size=128):\n",
    "    \"\"\"\n",
    "    Given an array of N coordinates or bins, computes the SHAP scores\n",
    "    for the model, returning an N x I x 4 array of SHAP scores and an\n",
    "    N x I x 4 array of one-hot encoded sequence.\n",
    "    \"\"\"\n",
    "    num_samples = len(sample)\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    \n",
    "    all_shap_scores = np.empty((num_samples, input_length, 4))\n",
    "    all_one_hot_seqs = np.empty((num_samples, input_length, 4))\n",
    "        \n",
    "    if model_type == \"profile\":\n",
    "        shap_explainer = compute_shap.create_profile_explainer(\n",
    "            model, input_length, profile_length, num_tasks, num_strands, controls\n",
    "        )\n",
    "    else:\n",
    "        shap_explainer = compute_shap.create_binary_explainer(\n",
    "            model, input_length\n",
    "        )\n",
    "\n",
    "    for i in tqdm.notebook.trange(num_batches):\n",
    "        batch_slice = slice(i * batch_size, (i + 1) * batch_size)\n",
    "        batch = sample[batch_slice]\n",
    "\n",
    "        if model_type == \"profile\":\n",
    "            input_seqs, profiles = input_func(batch)\n",
    "            shap_scores = shap_explainer(\n",
    "                input_seqs, cont_profs=profiles[:, num_tasks:], hide_shap_output=True\n",
    "            )\n",
    "        else:\n",
    "            input_seqs, _, _ = input_func(batch)\n",
    "            shap_scores = shap_explainer(\n",
    "                input_seqs, hide_shap_output=True\n",
    "            )\n",
    "\n",
    "        all_shap_scores[batch_slice] = shap_scores\n",
    "        all_one_hot_seqs[batch_slice] = input_seqs\n",
    "    return all_shap_scores, all_one_hot_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the importance scores and 1-hot seqs\n",
    "imp_type = (\"DeepSHAP scores\", \"input gradients\")[0]\n",
    "imp_func = compute_shap_scores if imp_type == \"DeepSHAP scores\" else compute_gradients\n",
    "noprior_scores, _ = imp_func(noprior_model, sample)\n",
    "prior_scores, one_hot_seqs = imp_func(prior_model, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the coordinates on both sides symmetrically to make them `input_length` \n",
    "centers = (sample_coords[:, 1] + sample_coords[:, 2]) // 2\n",
    "starts = centers - (input_length // 2)\n",
    "ends = starts + input_length\n",
    "sample_coords[:, 1] = starts\n",
    "sample_coords[:, 2] = ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot out a few examples\n",
    "center_slice = slice(400, 600)\n",
    "for i in np.random.choice(num_samples, size=3, replace=False):\n",
    "    print(sample[i])\n",
    "    print(\"=========================\")\n",
    "    print(\"Without priors:\")\n",
    "    viz_sequence.plot_weights(noprior_scores[i, center_slice] * one_hot_seqs[i, center_slice], subticks_frequency=10)\n",
    "    print(\"With priors:\")\n",
    "    viz_sequence.plot_weights(prior_scores[i, center_slice] * one_hot_seqs[i, center_slice], subticks_frequency=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlate importance to distance from summit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summit_location(chrom, start, end):\n",
    "    \"\"\"\n",
    "    Gets the locations of all summits that overlap this interval,\n",
    "    relative to the given start and end.\n",
    "    \"\"\"\n",
    "    summits = []\n",
    "    for peak_bed in peak_beds:\n",
    "        rows = peak_bed[\n",
    "            (peak_bed[\"chrom\"] == chrom) & \\\n",
    "            (peak_bed[\"summit_pos\"] >= start) & \\\n",
    "            (peak_bed[\"summit_pos\"] < end)\n",
    "        ]\n",
    "        if not len(rows):\n",
    "            continue\n",
    "        summits.extend(list(rows[\"summit_pos\"] - start))\n",
    "    return summits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summits(coords):\n",
    "    \"\"\"\n",
    "    Given an N x 3 object array of coordinates, extracts the set of summit\n",
    "    as offsets to the sample coordinates. Returns a list of N sublists, with\n",
    "    each sublist being the set of overlapping summit offsets.\n",
    "    \"\"\"\n",
    "    summits = []\n",
    "    for coord in tqdm.notebook.tqdm(coords):\n",
    "        summits.append(get_summit_location(coord[0], coord[1], coord[2]))\n",
    "    return summits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_windowed_distance_corr(scores, one_hot_seqs, summit_offsets, window_size=1):\n",
    "    \"\"\"\n",
    "    Windowing by `window_size`, computes the total importance magnitude for the\n",
    "    actual importance in each window, and correlates that to the distance from\n",
    "    the summit. If there are multiple summits, then the closest one is used.\n",
    "    Returns a NumPy array of correlation values, one for each sequence (where\n",
    "    each correlation is done over the windows of the sequence).\n",
    "    \"\"\"\n",
    "    num_samples = len(scores)\n",
    "    corr_vals = np.empty(num_samples)\n",
    "    for i in tqdm.notebook.trange(num_samples):\n",
    "        score_track = np.sum(np.abs(scores[i] * one_hot_seqs[i]), axis=1)  # Actual importance\n",
    "        score_track[score_track < 0] = 0\n",
    "        \n",
    "        offsets = summit_offsets[i]\n",
    "        if not offsets:\n",
    "            print(\"Warning: found a sequence with no captured summit\")\n",
    "            corr_vals[i] = 0\n",
    "            continue\n",
    "        num_windows = len(score_track) - window_size + 1\n",
    "        \n",
    "        # Compute distances to summits\n",
    "        distances = np.empty((len(offsets), num_windows))\n",
    "        window_locs = np.arange(num_windows) + ((len(score_track) - num_windows) / 2)\n",
    "        for j, offset in enumerate(offsets):\n",
    "            distances[j] = np.abs(window_locs - offset)\n",
    "        closest_distances = np.amin(distances, axis=0)\n",
    "        \n",
    "        # Compute sum of importance in each window\n",
    "        importances = np.empty(num_windows)\n",
    "        for j in range(num_windows):\n",
    "            importances[j] = np.sum(score_track[j : j + window_size])\n",
    "        \n",
    "        corr_vals[i] = scipy.stats.spearmanr(importances, closest_distances)[0]\n",
    "    return corr_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summit_offsets = compute_summits(sample_coords)\n",
    "noprior_corrs = compute_windowed_distance_corr(noprior_scores, one_hot_seqs, summit_offsets, window_size=window_size)\n",
    "prior_corrs = compute_windowed_distance_corr(prior_scores, one_hot_seqs, summit_offsets, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = 30\n",
    "all_vals = np.concatenate([noprior_corrs, prior_corrs])\n",
    "bins = np.linspace(np.min(all_vals), np.max(all_vals), bin_num)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.hist(noprior_corrs, bins=bins, color=\"coral\", label=\"No prior\", alpha=0.7)\n",
    "ax.hist(prior_corrs, bins=bins, color=\"slateblue\", label=\"With Fourier prior\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    (\"Histogram of Spearman correlation of %s to distance from peak summit\" % imp_type) +\n",
    "    (\"\\n%s %s models\" % (condition_name, model_type)) +\n",
    "    (\"\\nComputed on %d randomly drawn test peaks\" % num_samples)\n",
    ")\n",
    "plt.xlabel(\"Spearman correlation\")\n",
    "\n",
    "print(\"Average correlation without priors: %f\" % np.mean(noprior_corrs))\n",
    "print(\"Average correlation with priors: %f\" % np.mean(prior_corrs))\n",
    "print(\"Standard error without priors: %f\" % scipy.stats.sem(noprior_corrs))\n",
    "print(\"Standard error with priors: %f\" % scipy.stats.sem(prior_corrs))\n",
    "w, p = scipy.stats.wilcoxon(noprior_corrs, prior_corrs, alternative=\"greater\")\n",
    "print(\"One-sided Wilcoxon test: w = %f, p = %f\" % (w, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(noprior_corrs, prior_corrs, color=\"mediumorchid\", alpha=0.5)\n",
    "plt.title(\n",
    "    (\"Pairwise comparison of Spearman correlation of %s to distance from peak summit\" % imp_type) +\n",
    "    (\"\\n%s %s models\" % (condition_name, model_type)) +\n",
    "    (\"\\nComputed on %d randomly drawn test peaks\" % num_samples)\n",
    ")\n",
    "limits = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "]\n",
    "ax.plot(limits, limits, \"--\", alpha=0.5, color=\"black\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "plt.xlabel(\"Spearman correlation without prior\")\n",
    "plt.ylabel(\"Spearman correlation with Fourier prior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranked enrichment of peak overlap and summit closeness with importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normed_rank_enrichment(ordered_mask):\n",
    "    \"\"\"\n",
    "    From a binary mask array (in order from best to worst thresholds), computes\n",
    "    the rank enrichment at each threshold. Specifically, this computes a\n",
    "    normalized CDF of how many 1s are seen in the first k instances.\n",
    "    \"\"\"\n",
    "    cdf = np.cumsum(ordered_mask)\n",
    "    # To normalize, divide by the expectation if there all 1s were spread\n",
    "    # out evenly across the instances\n",
    "    expectation = np.sum(ordered_mask) / len(ordered_mask) * np.arange(1, len(cdf) + 1)\n",
    "    return cdf / expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_peak_overlap_mask(chrom, start, end):\n",
    "    \"\"\"\n",
    "    Given a coordinate, returns a boolean mask for that coordinate\n",
    "    for which bases lie within a peak.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(end - start, dtype=bool)\n",
    "    for peak_bed in peak_beds:\n",
    "        rows = peak_bed[\n",
    "            (peak_bed[\"chrom\"] == chrom) & \\\n",
    "            (peak_bed[\"peak_start\"] <= end) & \\\n",
    "            (start <= peak_bed[\"peak_end\"])\n",
    "        ]\n",
    "        intervals = rows[[\"peak_start\", \"peak_end\"]].values - start\n",
    "        for interval in intervals:\n",
    "            mask[interval[0]:interval[1]] = True\n",
    "    return mask\n",
    "\n",
    "def compute_peak_overlap_mask(coords):\n",
    "    \"\"\"\n",
    "    Given an N x 3 object array of coordinates, extracts a boolean mask\n",
    "    denoting which locations in each sample overlap a peak.\n",
    "    Returns an N x I boolean array, where each subarray is a boolean mask for\n",
    "    which bases in that coordinate lie within a peak.\n",
    "    \"\"\"\n",
    "    masks = []\n",
    "    for coord in tqdm.notebook.tqdm(coords):\n",
    "        masks.append(compute_single_peak_overlap_mask(coord[0], coord[1], coord[2]))\n",
    "    return np.stack(masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_windowed_peak_overlap_mask(scores, one_hot_seqs, seq_peak_masks, window_size=10):\n",
    "    \"\"\"\n",
    "    Windowing by `window_size`, computes the total importance magnitude for the\n",
    "    actual importance in each window, and identifies which windows overlap a peak.\n",
    "    Returns a boolean NumPy array denoting which windows overlap a peak, sorted in\n",
    "    descending order of window importance, and the indices of each window.\n",
    "    This requires a mask of whether or not each base in each input sequence overlaps\n",
    "    a peak, computed by `compute_peak_overlap_mask`.\n",
    "    \"\"\"\n",
    "    num_samples = len(scores)\n",
    "    all_window_imps, all_peak_masks = [], []\n",
    "    all_window_inds = []\n",
    "    for i in tqdm.notebook.trange(num_samples):\n",
    "        score_track = np.sum(np.abs(scores[i] * one_hot_seqs[i]), axis=1)  # Actual importance\n",
    "        num_windows = len(score_track) - window_size + 1\n",
    "        \n",
    "        # Compute windowed peak mask\n",
    "        window_locs = np.arange(num_windows) + ((len(score_track) - num_windows) / 2)\n",
    "        windowed_peak_mask = seq_peak_masks[i][window_locs.astype(int)]\n",
    "        \n",
    "        # Compute sum of importance in each window\n",
    "        importances = np.empty(num_windows)\n",
    "        for j in range(num_windows):\n",
    "            importances[j] = np.sum(score_track[j : j + window_size])\n",
    "            all_window_inds.append([i, j])\n",
    "        \n",
    "        all_window_imps.append(importances)\n",
    "        all_peak_masks.append(windowed_peak_mask)\n",
    "    \n",
    "    all_window_imps, all_peak_masks = np.concatenate(all_window_imps), np.concatenate(all_peak_masks)\n",
    "    all_window_inds = np.stack(all_window_inds)\n",
    "    sorted_inds = np.flip(np.argsort(all_window_imps))\n",
    "    return all_peak_masks[sorted_inds], all_window_inds[sorted_inds], all_window_imps[sorted_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_masks = compute_peak_overlap_mask(sample_coords)\n",
    "noprior_peak_mask, noprior_windows, noprior_window_imps = compute_windowed_peak_overlap_mask(noprior_scores, one_hot_seqs, peak_masks, window_size=window_size)\n",
    "prior_peak_mask, prior_windows, prior_window_imps = compute_windowed_peak_overlap_mask(prior_scores, one_hot_seqs, peak_masks, window_size=window_size)\n",
    "noprior_peak_cdf = normed_rank_enrichment(noprior_peak_mask)\n",
    "prior_peak_cdf = normed_rank_enrichment(prior_peak_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show(indexes):\n",
    "#     i, j = indexes\n",
    "#     print(j)\n",
    "#     print(sample_coords[i])\n",
    "#     plt.figure(figsize=(20, 3))\n",
    "#     plt.plot(np.sum(prior_scores[i] * one_hot_seqs[i], axis=1))\n",
    "#     plt.show()\n",
    "#     viz_sequence.plot_weights(prior_scores[i][j - 10: j + 10])\n",
    "#     viz_sequence.plot_weights((prior_scores[i] * one_hot_seqs[i])[j - 10: j + 10])\n",
    "    \n",
    "#     plt.figure(figsize=(20, 3))\n",
    "#     plt.plot(np.sum(noprior_scores[i] * one_hot_seqs[i], axis=1))\n",
    "#     plt.show()\n",
    "#     viz_sequence.plot_weights(noprior_scores[i][j - 10: j + 10])\n",
    "#     viz_sequence.plot_weights((noprior_scores[i] * one_hot_seqs[i])[j - 10: j + 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = np.where(~prior_peak_mask[:1000])[0]\n",
    "# print(inds)\n",
    "# show(prior_windows[inds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10), sharex=True)\n",
    "title = \"Cumulative peak overlap over bases ranked by %s\" % imp_type\n",
    "title += \"\\n%s %s models\" % (condition_name, model_type)\n",
    "title += \"\\nComputed on %d randomly drawn test peaks\" % num_samples\n",
    "fig.suptitle(title)\n",
    "ax[0].plot(np.cumsum(noprior_peak_mask), label=\"No prior\", color=\"coral\")\n",
    "ax[0].plot(np.cumsum(prior_peak_mask), label=\"With Fourier prior\", color=\"slateblue\")\n",
    "ax[0].set_ylabel(\"Number of bases in peaks (x1000)\")\n",
    "ax[0].set_yticklabels((ax[0].get_yticks() / 1000).astype(int))\n",
    "ax[1].plot(noprior_peak_cdf, label=\"No prior\", color=\"coral\")\n",
    "ax[1].plot(prior_peak_cdf, label=\"With Fourier prior\", color=\"slateblue\")\n",
    "ax[1].set_ylabel(\"Enrichment of number of bases\")\n",
    "fig.text(0.45, 0.05, \"Top k bases by importance\", fontsize=18)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noprior_prs = sklearn.metrics.precision_recall_curve(\n",
    "    np.flip(noprior_peak_mask.astype(int)),\n",
    "    np.flip(noprior_window_imps / np.max(noprior_window_imps))\n",
    ")\n",
    "prior_prs = sklearn.metrics.precision_recall_curve(\n",
    "    np.flip(prior_peak_mask.astype(int)),\n",
    "    np.flip(prior_window_imps / np.max(prior_window_imps))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "title = \"Precision - Recall of %s peak overlap in top bases by %s\" % (condition_name, imp_type)\n",
    "title += \"\\n%s %s models\" % (condition_name, model_type)\n",
    "title += \"\\nComputed on %d randomly drawn test peaks\" % num_samples\n",
    "plt.title(title)\n",
    "plt.plot(noprior_prs[1][:-1], noprior_prs[0][:-1], color=\"coral\", label=\"No prior\")\n",
    "plt.plot(prior_prs[1][:-1], prior_prs[0][:-1], color=\"slateblue\", label=\"With Fourier prior\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"auPRC without priors: %f\" % (sklearn.metrics.auc(noprior_prs[1][:-1], noprior_prs[0][:-1])))\n",
    "print(\"auPRC with priors: %f\" % sklearn.metrics.auc(prior_prs[1][:-1], prior_prs[0][:-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
