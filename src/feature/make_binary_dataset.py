import torch
import numpy as np
import pandas as pd
import sacred
from datetime import datetime
import h5py
import feature.util as util
import tqdm

dataset_ex = sacred.Experiment("dataset")

@dataset_ex.config
def config():
    # Path to reference genome FASTA
    reference_fasta = "/users/amtseng/genomes/hg38.fasta"

    # Path to chromosome sizes
    chrom_sizes_tsv = "/users/amtseng/genomes/hg38.canon.chrom.sizes"

    # For each input sequence in the raw data, center it and pad to this length 
    input_length = 1000

    # One-hot encoding has this depth
    input_depth = 4

    # Whether or not to perform reverse complement augmentation
    revcomp = True

    # Batch size; will be multiplied by two if reverse complementation is done
    batch_size = 128

    # Sample X negatives randomly for every positive example
    negative_ratio = 1

    # Amount of dataset for each task to keep; can be a set number of peaks, or
    # a fraction (if < 1); set to None to keep everything
    peak_retention = None

    # Number of workers for the data loader
    num_workers = 10
    
    # Negative seed (for selecting negatives)
    negative_seed = None

    # Shuffle seed (for shuffling data points)
    shuffle_seed = None


class BinsToVals():
    """
    From an HDF5 file that maps genomic coordinates to profiles, this creates an
    object that maps a list of bind indices to a NumPy array of coordinates and
    output values.
    Arguments:
        `label_hdf5`: path to HDF5 containing labels; this HDF5 must be a single
            dataset created by `create_ENCODE_binary_bins.py`, with 4 datasets:
            "chrom" (an N-array of chromosomes), "start" and "end" (each an
            N-array of coordinate boundaries), and "values", an N x T array of
            binary values for each task T, containing 0, 1, or -1
    """
    def __init__(self, label_hdf5):
        self.label_hdf5 = label_hdf5

    def _get_ndarray(self, bin_inds):
        """
        From a list of bin indices, returns a B x 3 array of coordinates and a
        B x T array of corresponding label values.
        """
        # Sort the bin indices to fetch from HDF5
        inds = np.argsort(bin_inds)
        bin_inds = bin_inds[inds]
        with h5py.File(self.label_hdf5, "r") as f:
            coords = np.empty((len(bin_inds), 3), dtype=object)
            coords[:, 0] = f["chrom"][bin_inds].astype(str)
            coords[:, 1] = f["start"][bin_inds]
            coords[:, 2] = f["end"][bin_inds]
            vals = f["values"][bin_inds].astype(int)
        # Return to original order
        inds = np.expand_dims(inds, axis=1)
        coords_ord = np.empty_like(coords)
        vals_ord = np.empty_like(vals)
        np.put_along_axis(coords_ord, inds, coords, axis=0)
        np.put_along_axis(vals_ord, inds, vals, axis=0)
        return coords_ord, vals_ord

    def __call__(self, bin_inds):
        return self._get_ndarray(bin_inds)


class SamplingBinsBatcher(torch.utils.data.sampler.Sampler):
    """
    Creates a batch producer that batches bin indices for positive bins and
    negative bins. Each batch will have some positives and negatives according
    to `neg_ratio`.
    Arguments:
        `bin_labels_array`: an N x 2 array of binary labels and chromosomes,
            generated by `create_ENCODE_binary_bins.py`
        `batch_size`: number of samples per batch
        `neg_ratio`: number of negatives to select for each positive example
        `chroms_keep`: if specified, only considers this set of chromosomes from
            the coordinate BEDs
        `peak_retention`: if specified, keeps only a fraction of the positive
            bins that are underpinned by this amount of peaks (taking most
            confident peaks preferentially); can be a fraction of the total set
            of peaks (if value is < 1), or a number of peaks (if value is >= 1)
        `peak_signals_array`: an N x T x 2 array of signal strengths and peak
            ranks by signal values for peaks underlying bins, generated by
            `create_ENCODE_binary_bins.py`; only needed if `peak_retention` is
            used; specifies the signal strength and rank (small rank is high
            signal strength) of each peak in each bin
        `shuffle_before_epoch`: whether or not to shuffle all examples before
            each epoch
        `negative_seed`: seed for picking negatives
        `shuffle_seed`: seed for shuffling
    """
    def __init__(
        self, bin_labels_array, batch_size, neg_ratio, chroms_keep=None,
        peak_retention=None, peak_signals_array=None,
        shuffle_before_epoch=False, negative_seed=None, shuffle_seed=None
    ):
        self.batch_size = batch_size
        self.shuffle_before_epoch = shuffle_before_epoch
    
        pos_mask = (bin_labels_array[:, 1] == 1)
        neg_mask = (bin_labels_array[:, 1] == 0)
        if chroms_keep:
            chroms_keep = np.array(chroms_keep)
            chrom_mask = np.isin(bin_labels_array[:, 0], chroms_keep)
            pos_mask = pos_mask & chrom_mask
            neg_mask = neg_mask & chrom_mask

        # If specified, keep only a subset 
        if peak_retention is not None:
            assert peak_signals_array is not None, \
                "If peak retention is on, an array of signal values and peak ranks must be given"
           
            # Start keeping nothing
            keep_mask = np.zeros(len(pos_mask), dtype=bool)
            num_tasks = peak_signals_array.shape[1]
            for i in range(num_tasks):
                ranks = peak_signals_array[:, i, 1]
                max_rank = np.max(ranks)
                keep_num = int(max_rank * peak_retention) if \
                    peak_retention < 1 else peak_retention
                # Update mask: keep any bin that passes the rank threshold
                keep_mask = keep_mask | ((ranks <= keep_num) & (ranks > 0))
            pos_mask = pos_mask & keep_mask

        self.pos_inds = np.where(pos_mask)[0]
        self.neg_inds = np.where(neg_mask)[0]

        self.neg_per_batch = int(batch_size * neg_ratio / (neg_ratio + 1))
        self.pos_per_batch = batch_size - self.neg_per_batch

        self.negative_rng = np.random.RandomState(negative_seed)
        if shuffle_before_epoch:
            self.shuffle_rng = np.random.RandomState(shuffle_seed)

    def __getitem__(self, index):
        """
        Fetches a full batch of positive and negative bin indices. Returns
        a B-array of bin indices, and a B-array of statuses. The status is
        either 1 or 0: 1 if that bin has a positive binding event for some task,
        and 0 if that bin has no binding events over all tasks.
        """
        pos_inds = self.pos_inds[
            index * self.pos_per_batch : (index + 1) * self.pos_per_batch
        ]
        neg_inds = self.neg_inds[
            index * self.neg_per_batch : (index + 1) * self.neg_per_batch
        ]
        bin_inds = np.concatenate([pos_inds, neg_inds])
        status = np.concatenate([
            np.ones(len(pos_inds), dtype=int),
            np.zeros(len(neg_inds), dtype=int)
        ])
        return bin_inds, status

    def __len__(self):
        return int(np.ceil(len(self.pos_inds) / self.pos_per_batch))
   
    def on_epoch_start(self):
        if (self.shuffle_before_epoch):
            self.shuffle_rng.shuffle(self.pos_inds)
        self.negative_rng.shuffle(self.neg_inds)  # Always shuffle negatives


class BinDataset(torch.utils.data.IterableDataset):
    """
    Generates single samples of a one-hot encoded sequence and value.
    Arguments:
        `bin_batcher (SamplingBinsBatcher): maps indices to batches of
            bin indices
        `coords_to_seq (CoordsToSeq)`: maps coordinates to 1-hot encoded
            sequences
        `bins_to_vals (BinsToVals)`: maps bin indices to values to predict
        `revcomp`: whether or not to perform revcomp to the batch; this will
            double the batch size implicitly
        `return_coords`: if True, each batch returns the set of coordinates for
            that batch along with the 1-hot encoded sequences and values
    """
    def __init__(
        self, bins_batcher, coords_to_seq, bins_to_vals, revcomp=False,
        return_coords=False
    ):
        self.bins_batcher = bins_batcher
        self.coords_to_seq = coords_to_seq
        self.bins_to_vals = bins_to_vals
        self.revcomp = revcomp
        self.return_coords = return_coords

    def get_batch(self, index):
        """
        Returns a batch, which consists of an B x L x 4 NumPy array of 1-hot
        encoded sequence, the associated B x T values, and a 1D length-B NumPy
        array of statuses. The coordinates may also be returned as a B x 3
        array.
        """
        # Get batch of bin indices for this index
        bin_inds_batch, status = self.bins_batcher[index]

        # Map this batch of bin indices to coordinates and output values
        coords, vals = self.bins_to_vals(bin_inds_batch)

        # Map the batch of coordinates to 1-hot encoded sequences
        seqs = self.coords_to_seq(coords, revcomp=self.revcomp)

        if self.revcomp:
            vals = np.concatenate([vals, vals])
            status = np.concatenate([status, status])

        if self.return_coords:
            if self.revcomp:
                coords = np.concatenate([coords, coords])
            return seqs, vals, status, coords
        else:
            return seqs, vals, status

    def __iter__(self):
        """
        Returns an iterator over the batches. If the dataset iterator is called
        from multiple workers, each worker will be give a shard of the full
        range.
        """
        worker_info = torch.utils.data.get_worker_info()
        num_batches = len(self.bins_batcher)
        if worker_info is None:
            # In single-processing mode
            start, end = 0, num_batches
        else:
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            shard_size = int(np.ceil(num_batches / num_workers))
            start = shard_size * worker_id
            end = min(start + shard_size, num_batches)
        return (self.get_batch(i) for i in range(start, end))

    def __len__(self):
        return len(self.bins_batcher)
    
    def on_epoch_start(self):
        """
        This should be called manually before the beginning of every epoch (i.e.
        before the iteration begins).
        """
        self.bins_batcher.on_epoch_start()


@dataset_ex.capture
def create_data_loader(
    labels_hdf5_path, bin_labels_npy_or_array, batch_size, reference_fasta,
    input_length, negative_ratio, peak_retention, num_workers, revcomp,
    negative_seed, shuffle_seed, peak_signals_npy_or_array=None, chrom_set=None,
    shuffle=True, return_coords=False
):
    """
    Creates an IterableDataset object, which iterates through batches of
    bins and returns values for the bins.
    Arguments:
        `labels_hdf5_path`: path to HDF5 containing labels; this HDF5 must be a
            single dataset created by `generate_ENCODE_TFChIP_binary_labels.sh`;
            each row must be: (index, values, end, start, chrom), where the
            values is a T-array of values, for each task T, containing 0, 1, or
            nan
        `bin_labels_npy_or_array`: either the path to a pickled N x 2 object
            array, or the array already imported; this array must be generated
            by `create_ENCODE_binary_bins.py`
        `peak_signals_npy_or_array`: either the path to an N x T array, or the
            array already imported; this array must be generated by
            `create_ENCODE_binary_bins.py`; this is only required if
            `peak_retention` is used
        `chrom_set`: a list of chromosomes to restrict to for the positives and
            negatives; defaults to all coordinates in HDF5
        `shuffle`: if specified, shuffle the coordinates before each epoch
        `return_coords`: if specified, also return the underlying coordinates
            along with the values in each batch
    """
    # Maps set of bin indices to coordinates and values 
    bins_to_vals = BinsToVals(labels_hdf5_path)

    # Yields batches of positive and negative bin indices
    if type(bin_labels_npy_or_array) is str:
        bin_labels_array = np.load(bin_labels_npy_or_array, allow_pickle=True)
    else:
        bin_labels_array = bin_labels_npy_or_array
    if type(peak_signals_npy_or_array) is str:
        peak_signals_array = np.load(peak_signals_npy_or_array)
    else:
        peak_signals_array = peak_signals_npy_or_array  # Could be None
    bins_batcher = SamplingBinsBatcher(
        bin_labels_array, batch_size, negative_ratio, chroms_keep=chrom_set,
        peak_retention=peak_retention, peak_signals_array=peak_signals_array,
        shuffle_before_epoch=shuffle, shuffle_seed=shuffle_seed
    )

    print("Total class counts:")
    num_pos, num_neg = len(bins_batcher.pos_inds), len(bins_batcher.neg_inds)
    print("\tPos: %d, Neg: %d" % (num_pos, num_neg))
    if num_pos:
        print("\tNeg/Pos = %f" % (num_neg / num_pos))

    # Maps set of coordinates to 1-hot encoding, padded
    coords_to_seq = util.CoordsToSeq(
        reference_fasta, center_size_to_use=input_length
    )
    
    # Dataset
    dataset = BinDataset(
        bins_batcher, coords_to_seq, bins_to_vals, revcomp=revcomp,
        return_coords=return_coords
    )

    # Dataset loader: dataset is iterable and already returns batches
    loader = torch.utils.data.DataLoader(
        dataset, batch_size=None, num_workers=num_workers,
        collate_fn=lambda x: x
    )

    return loader


data = None
loader = None
@dataset_ex.automain
def main():
    global data, loader
    import os
    import tqdm
    import json

    paths_json_path = "/users/amtseng/att_priors/data/processed/ENCODE_TFChIP/binary/config/SPI1/SPI1_training_paths.json"
    
    with open(paths_json_path, "r") as f:
        paths_json = json.load(f)
    labels_hdf5 = paths_json["labels_hdf5"]
    bin_labels_npy = paths_json["bin_labels_npy"]
    peak_signals_npy = paths_json["peak_signals_npy"]

    splits_json_path = "/users/amtseng/att_priors/data/processed/chrom_splits.json"
    with open(splits_json_path, "r") as f:
        splits_json = json.load(f)
    train_chroms, val_chroms, test_chroms = \
        splits_json["1"]["train"], splits_json["1"]["val"], \
        splits_json["1"]["test"]

    bin_labels_array = np.load(bin_labels_npy, allow_pickle=True)
    peak_signals_array = np.load(peak_signals_npy, allow_pickle=True)
    loader = create_data_loader(
        labels_hdf5, bin_labels_array, return_coords=True,
        chrom_set=train_chroms, peak_retention=0.01,
        peak_signals_npy_or_array=peak_signals_array, shuffle_seed=123
    )
    loader.dataset.on_epoch_start()

    start_time = datetime.now()
    for batch in tqdm.tqdm(loader, total=len(loader.dataset)):
        data = batch
    end_time = datetime.now()
    print("Time: %ds" % (end_time - start_time).seconds)

    k = 2
    rc_k = int(len(data[0]) / 2) + k

    seqs, vals, statuses, coords = data
    
    seq, val, status, coord = seqs[k], vals[k], statuses[k], coords[k]
    rc_seq, rc_val, rc_status, rc_coord = \
        seqs[rc_k], vals[rc_k], statuses[rc_k], coords[rc_k]

    def print_one_hot_seq(one_hot):
        s = util.one_hot_to_seq(one_hot)
        print(s[:20] + "..." + s[-20:])
   
    print_one_hot_seq(seq)
    print_one_hot_seq(rc_seq)

    print(np.sum(val), np.sum(rc_val))
    print(status, rc_status)
    print(coord, rc_coord)
